<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://example.com">
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/main">主页</a></li>
	        
				<li><a href="/tags/%E9%9A%8F%E7%AC%94/">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
		        
					<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
			        
						<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/main">主页</a></li>
		        
					<li style="width: 50%"><a href="/tags/%E9%9A%8F%E7%AC%94/">随笔</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-大模型初步-lora微调技术" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-lora%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/">大模型初步-lora微调技术</a>
    </h1>
  

        
        <a href="/2024/05/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-lora%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/" class="archive-article-date">
  	<time datetime="2024-05-17T12:19:22.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-17</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>lora的技术讲解在之前llama2的文章中已经涉及到，这里不赘述，直接看microsoft的源码实现(未来可能考虑将llama2的文章拆解到各个部分)<br>代码地址: <a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA/tree/main">https://github.com/microsoft/LoRA/tree/main</a></p>
<p>细节: 在这篇<a target="_blank" rel="noopener" href="https://my.oschina.net/HuggingFace/blog/10091976">文章</a>提到了</p>
<blockquote>
<p>（严格讲，对不同算子采用不同的 A 初始化方式，例如，Linear 算子采用 kaiming_uniform，对于 Embedding 算子采用 normal 高斯分布）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoRALayer</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        lora_dropout: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        self.r = r</span><br><span class="line">        self.lora_alpha = lora_alpha</span><br><span class="line">        <span class="comment"># Optional dropout </span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;这里lambda中输出等于输入，相当于不执行丢弃神经元操作，将dropout当成了一个可选项&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> lora_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">            self.lora_dropout = nn.Dropout(p=lora_dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.lora_dropout = <span class="keyword">lambda</span> x: x</span><br><span class="line">        <span class="comment"># Mark the weight as unmerged</span></span><br><span class="line">        self.merged = <span class="literal">False</span></span><br><span class="line">        self.merge_weights = merge_weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embedding</span>(nn.Embedding, LoRALayer):</span><br><span class="line">    <span class="comment"># LoRA implemented in a dense layer</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        num_embeddings: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        embedding_dim: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span> = <span class="number">0</span>,  <span class="comment"># 表示低秩矩阵的秩</span></span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        **kwargs</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    这里使用多重继承的意义:</span></span><br><span class="line"><span class="string">    这样做的目的是将 nn.Embedding 和 LoRALayer 的功能结合起来，创建一个同时具有嵌入层和LoRA层功能的类</span></span><br><span class="line"><span class="string">    通过继承这两个类，Embedding 类可以同时拥有 nn.Embedding 的嵌入功能和 LoRALayer 的低秩自适应功能，使得嵌入层可以通过LoRA技术进行高效的微调</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">        nn.Embedding.__init__(self, num_embeddings, embedding_dim, **kwargs)</span><br><span class="line">        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=<span class="number">0</span>,</span><br><span class="line">                           merge_weights=merge_weights)</span><br><span class="line">        <span class="comment"># Actual trainable parameters</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        这里self.weight是从nn.Embedding继承来的属性，代表了嵌入层的权重矩阵</span></span><br><span class="line"><span class="string">        new_zeros 方法是 PyTorch 张量的一种方法，用于创建一个与原始张量具有相同数据类型和设备（如 CPU 或 GPU）的新张量，并且这个新张量的所有元素都被初始化为 0</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> r &gt; <span class="number">0</span>:</span><br><span class="line">            self.lora_A = nn.Parameter(self.weight.new_zeros((r, num_embeddings)))</span><br><span class="line">            self.lora_B = nn.Parameter(self.weight.new_zeros((embedding_dim, r)))</span><br><span class="line">            self.scaling = self.lora_alpha / self.r  <span class="comment"># 缩放因子，用于调整 LoRA 更新的大小</span></span><br><span class="line">            <span class="comment"># Freezing the pre-trained weight matrix</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;这里对于原来的矩阵不变化，不更新梯度&#x27;&#x27;&#x27;</span></span><br><span class="line">            self.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        nn.Embedding.reset_parameters(self)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;lora_A&#x27;</span>):</span><br><span class="line">            <span class="comment"># initialize A the same way as the default for nn.Linear and B to zero</span></span><br><span class="line">            nn.init.zeros_(self.lora_A)</span><br><span class="line">            nn.init.normal_(self.lora_B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, mode: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">        nn.Embedding.train(self, mode)</span><br><span class="line">        <span class="keyword">if</span> mode:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> self.merged:</span><br><span class="line">                <span class="comment"># Make sure that the weights are not merged</span></span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span>:</span><br><span class="line">                    self.weight.data -= (self.lora_B @ self.lora_A).transpose(<span class="number">0</span>, <span class="number">1</span>) * self.scaling</span><br><span class="line">                self.merged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> <span class="keyword">not</span> self.merged:</span><br><span class="line">                <span class="comment"># Merge the weights and mark it</span></span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span>:</span><br><span class="line">                    self.weight.data += (self.lora_B @ self.lora_A).transpose(<span class="number">0</span>, <span class="number">1</span>) * self.scaling</span><br><span class="line">                self.merged = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">if</span> self.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.merged:</span><br><span class="line">            result = nn.Embedding.forward(self, x)</span><br><span class="line">            after_A = F.embedding(</span><br><span class="line">                x, self.lora_A.transpose(<span class="number">0</span>, <span class="number">1</span>), self.padding_idx, self.max_norm,</span><br><span class="line">                self.norm_type, self.scale_grad_by_freq, self.sparse</span><br><span class="line">            )</span><br><span class="line">            result += (after_A @ self.lora_B.transpose(<span class="number">0</span>, <span class="number">1</span>)) * self.scaling</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Embedding.forward(self, x)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Linear, LoRALayer):</span><br><span class="line">    <span class="comment"># LoRA implemented in a dense layer</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        in_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        out_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span> = <span class="number">0</span>, </span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span> = <span class="number">1</span>, </span></span><br><span class="line"><span class="params">        lora_dropout: <span class="built_in">float</span> = <span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        fan_in_fan_out: <span class="built_in">bool</span> = <span class="literal">False</span>, <span class="comment"># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span></span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        **kwargs</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        nn.Linear.__init__(self, in_features, out_features, **kwargs)</span><br><span class="line">        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,</span><br><span class="line">                           merge_weights=merge_weights)</span><br><span class="line"></span><br><span class="line">        self.fan_in_fan_out = fan_in_fan_out</span><br><span class="line">        <span class="comment"># Actual trainable parameters</span></span><br><span class="line">        <span class="keyword">if</span> r &gt; <span class="number">0</span>:</span><br><span class="line">            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))</span><br><span class="line">            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))</span><br><span class="line">            self.scaling = self.lora_alpha / self.r</span><br><span class="line">            <span class="comment"># Freezing the pre-trained weight matrix</span></span><br><span class="line">            self.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.reset_parameters()</span><br><span class="line">        <span class="keyword">if</span> fan_in_fan_out:</span><br><span class="line">            self.weight.data = self.weight.data.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        nn.Linear.reset_parameters(self)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;lora_A&#x27;</span>):</span><br><span class="line">            <span class="comment"># initialize B the same way as the default for nn.Linear and A to zero</span></span><br><span class="line">            <span class="comment"># this is different than what is described in the paper but should not affect performance</span></span><br><span class="line">            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">            nn.init.zeros_(self.lora_B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, mode: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">T</span>(<span class="params">w</span>):</span><br><span class="line">            <span class="keyword">return</span> w.transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="keyword">if</span> self.fan_in_fan_out <span class="keyword">else</span> w</span><br><span class="line">        nn.Linear.train(self, mode)</span><br><span class="line">        <span class="keyword">if</span> mode:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> self.merged:</span><br><span class="line">                <span class="comment"># Make sure that the weights are not merged</span></span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span>:</span><br><span class="line">                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling</span><br><span class="line">                self.merged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> <span class="keyword">not</span> self.merged:</span><br><span class="line">                <span class="comment"># Merge the weights and mark it</span></span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span>:</span><br><span class="line">                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling</span><br><span class="line">                self.merged = <span class="literal">True</span>       </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">T</span>(<span class="params">w</span>):</span><br><span class="line">            <span class="keyword">return</span> w.transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="keyword">if</span> self.fan_in_fan_out <span class="keyword">else</span> w</span><br><span class="line">        <span class="keyword">if</span> self.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.merged:</span><br><span class="line">            result = F.linear(x, T(self.weight), bias=self.bias)            </span><br><span class="line">            result += (self.lora_dropout(x) @ self.lora_A.transpose(<span class="number">0</span>, <span class="number">1</span>) @ self.lora_B.transpose(<span class="number">0</span>, <span class="number">1</span>)) * self.scaling</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> F.linear(x, T(self.weight), bias=self.bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MergedLinear</span>(nn.Linear, LoRALayer):</span><br><span class="line">    <span class="comment"># LoRA implemented in a dense layer</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        in_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        out_features: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span> = <span class="number">0</span>, </span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span> = <span class="number">1</span>, </span></span><br><span class="line"><span class="params">        lora_dropout: <span class="built_in">float</span> = <span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        enable_lora: <span class="type">List</span>[<span class="built_in">bool</span>] = [<span class="literal">False</span>],</span></span><br><span class="line"><span class="params">        fan_in_fan_out: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        **kwargs</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        nn.Linear.__init__(self, in_features, out_features, **kwargs)</span><br><span class="line">        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,</span><br><span class="line">                           merge_weights=merge_weights)</span><br><span class="line">        <span class="keyword">assert</span> out_features % <span class="built_in">len</span>(enable_lora) == <span class="number">0</span>, \</span><br><span class="line">            <span class="string">&#x27;The length of enable_lora must divide out_features&#x27;</span></span><br><span class="line">        self.enable_lora = enable_lora</span><br><span class="line">        self.fan_in_fan_out = fan_in_fan_out</span><br><span class="line">        <span class="comment"># Actual trainable parameters</span></span><br><span class="line">        <span class="keyword">if</span> r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">any</span>(enable_lora):</span><br><span class="line">            self.lora_A = nn.Parameter(</span><br><span class="line">                self.weight.new_zeros((r * <span class="built_in">sum</span>(enable_lora), in_features)))</span><br><span class="line">            self.lora_B = nn.Parameter(</span><br><span class="line">                self.weight.new_zeros((out_features // <span class="built_in">len</span>(enable_lora) * <span class="built_in">sum</span>(enable_lora), r))</span><br><span class="line">            ) <span class="comment"># weights for Conv1D with groups=sum(enable_lora)</span></span><br><span class="line">            self.scaling = self.lora_alpha / self.r</span><br><span class="line">            <span class="comment"># Freezing the pre-trained weight matrix</span></span><br><span class="line">            self.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">            <span class="comment"># Compute the indices</span></span><br><span class="line">            self.lora_ind = self.weight.new_zeros(</span><br><span class="line">                (out_features, ), dtype=torch.<span class="built_in">bool</span></span><br><span class="line">            ).view(<span class="built_in">len</span>(enable_lora), -<span class="number">1</span>)</span><br><span class="line">            self.lora_ind[enable_lora, :] = <span class="literal">True</span></span><br><span class="line">            self.lora_ind = self.lora_ind.view(-<span class="number">1</span>)</span><br><span class="line">        self.reset_parameters()</span><br><span class="line">        <span class="keyword">if</span> fan_in_fan_out:</span><br><span class="line">            self.weight.data = self.weight.data.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        nn.Linear.reset_parameters(self)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;lora_A&#x27;</span>):</span><br><span class="line">            <span class="comment"># initialize A the same way as the default for nn.Linear and B to zero</span></span><br><span class="line">            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">            nn.init.zeros_(self.lora_B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">zero_pad</span>(<span class="params">self, x</span>):</span><br><span class="line">        result = x.new_zeros((<span class="built_in">len</span>(self.lora_ind), *x.shape[<span class="number">1</span>:]))</span><br><span class="line">        result[self.lora_ind] = x</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">merge_AB</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">T</span>(<span class="params">w</span>):</span><br><span class="line">            <span class="keyword">return</span> w.transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="keyword">if</span> self.fan_in_fan_out <span class="keyword">else</span> w</span><br><span class="line">        delta_w = F.conv1d(</span><br><span class="line">            self.lora_A.unsqueeze(<span class="number">0</span>), </span><br><span class="line">            self.lora_B.unsqueeze(-<span class="number">1</span>), </span><br><span class="line">            groups=<span class="built_in">sum</span>(self.enable_lora)</span><br><span class="line">        ).squeeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> T(self.zero_pad(delta_w))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, mode: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">T</span>(<span class="params">w</span>):</span><br><span class="line">            <span class="keyword">return</span> w.transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="keyword">if</span> self.fan_in_fan_out <span class="keyword">else</span> w</span><br><span class="line">        nn.Linear.train(self, mode)</span><br><span class="line">        <span class="keyword">if</span> mode:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> self.merged:</span><br><span class="line">                <span class="comment"># Make sure that the weights are not merged</span></span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">any</span>(self.enable_lora):</span><br><span class="line">                    self.weight.data -= self.merge_AB() * self.scaling</span><br><span class="line">                self.merged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> <span class="keyword">not</span> self.merged:</span><br><span class="line">                <span class="comment"># Merge the weights and mark it</span></span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">any</span>(self.enable_lora):</span><br><span class="line">                    self.weight.data += self.merge_AB() * self.scaling</span><br><span class="line">                self.merged = <span class="literal">True</span>        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">T</span>(<span class="params">w</span>):</span><br><span class="line">            <span class="keyword">return</span> w.transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="keyword">if</span> self.fan_in_fan_out <span class="keyword">else</span> w</span><br><span class="line">        <span class="keyword">if</span> self.merged:</span><br><span class="line">            <span class="keyword">return</span> F.linear(x, T(self.weight), bias=self.bias)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result = F.linear(x, T(self.weight), bias=self.bias)</span><br><span class="line">            <span class="keyword">if</span> self.r &gt; <span class="number">0</span>:</span><br><span class="line">                result += self.lora_dropout(x) @ T(self.merge_AB().T) * self.scaling</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvLoRA</span>(nn.Module, LoRALayer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, conv_module, in_channels, out_channels, kernel_size, r=<span class="number">0</span>, lora_alpha=<span class="number">1</span>, lora_dropout=<span class="number">0.</span>, merge_weights=<span class="literal">True</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvLoRA, self).__init__()</span><br><span class="line">        self.conv = conv_module(in_channels, out_channels, kernel_size, **kwargs)</span><br><span class="line">        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(kernel_size, <span class="built_in">int</span>)</span><br><span class="line">        <span class="comment"># Actual trainable parameters</span></span><br><span class="line">        <span class="keyword">if</span> r &gt; <span class="number">0</span>:</span><br><span class="line">            self.lora_A = nn.Parameter(</span><br><span class="line">                self.conv.weight.new_zeros((r * kernel_size, in_channels * kernel_size))</span><br><span class="line">            )</span><br><span class="line">            self.lora_B = nn.Parameter(</span><br><span class="line">              self.conv.weight.new_zeros((out_channels//self.conv.groups*kernel_size, r*kernel_size))</span><br><span class="line">            )</span><br><span class="line">            self.scaling = self.lora_alpha / self.r</span><br><span class="line">            <span class="comment"># Freezing the pre-trained weight matrix</span></span><br><span class="line">            self.conv.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.reset_parameters()</span><br><span class="line">        self.merged = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        self.conv.reset_parameters()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;lora_A&#x27;</span>):</span><br><span class="line">            <span class="comment"># initialize A the same way as the default for nn.Linear and B to zero</span></span><br><span class="line">            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">            nn.init.zeros_(self.lora_B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, mode=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvLoRA, self).train(mode)</span><br><span class="line">        <span class="keyword">if</span> mode:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> self.merged:</span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># Make sure that the weights are not merged</span></span><br><span class="line">                    self.conv.weight.data -= (self.lora_B @ self.lora_A).view(self.conv.weight.shape) * self.scaling</span><br><span class="line">                self.merged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.merge_weights <span class="keyword">and</span> <span class="keyword">not</span> self.merged:</span><br><span class="line">                <span class="keyword">if</span> self.r &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># Merge the weights and mark it</span></span><br><span class="line">                    self.conv.weight.data += (self.lora_B @ self.lora_A).view(self.conv.weight.shape) * self.scaling</span><br><span class="line">                self.merged = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.merged:</span><br><span class="line">            <span class="keyword">return</span> self.conv._conv_forward(</span><br><span class="line">                x, </span><br><span class="line">                self.conv.weight + (self.lora_B @ self.lora_A).view(self.conv.weight.shape) * self.scaling,</span><br><span class="line">                self.conv.bias</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2d</span>(<span class="title class_ inherited__">ConvLoRA</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv2d, self).__init__(nn.Conv2d, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv1d</span>(<span class="title class_ inherited__">ConvLoRA</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv1d, self).__init__(nn.Conv1d, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Can Extend to other ones like this</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3d</span>(<span class="title class_ inherited__">ConvLoRA</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv3d, self).__init__(nn.Conv3d, *args, **kwargs)</span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-lora%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-分布式训练代码解读" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/16/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/">大模型初步-分布式训练代码解读</a>
    </h1>
  

        
        <a href="/2024/05/16/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" class="archive-article-date">
  	<time datetime="2024-05-16T15:37:52.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-16</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这里参考的文章有: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/668646528">https://zhuanlan.zhihu.com/p/668646528</a><br>考虑到参考的本身就是一个教程性质的东西，那这里就对里面没有涉及到的一些细节进行解析</p>
<p>1 Accelerate 分布式训练简介<br>使用pytorch原生API训练比较麻烦，因此推荐accelerate库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> torch</span><br><span class="line">  <span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">  <span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">+ <span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;自动初始化，不用指明具体使用的硬件类型&#x27;&#x27;&#x27;</span></span><br><span class="line">+ accelerator = Accelerator()  </span><br><span class="line">  model = torch.nn.Transformer()</span><br><span class="line">  optim = torch.optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">  dataset = load_dataset(<span class="string">&#x27;my_dataset&#x27;</span>)</span><br><span class="line">  data = torch.utils.data.DataLoader(dataset, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">+ model, optim, data = accelerator.prepare(model, optim, data)</span><br><span class="line"></span><br><span class="line">  model.train()</span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">      <span class="keyword">for</span> source, targets <span class="keyword">in</span> data:</span><br><span class="line">          optimizer.zero_grad()</span><br><span class="line">          </span><br><span class="line">          output = model(source)</span><br><span class="line">          loss = F.cross_entropy(output, targets)</span><br><span class="line">+         accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">+         optimizer.step()</span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/16/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-llama2代码解读3" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/15/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB3/">大模型初步-llama2代码解读3</a>
    </h1>
  

        
        <a href="/2024/05/15/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB3/" class="archive-article-date">
  	<time datetime="2024-05-15T05:25:50.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-15</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>只有一个model肯定是不行的，我们需要选择合适的方式训练</p>
<h3 id="pretrain-py"><a href="#pretrain-py" class="headerlink" title="pretrain.py"></a>pretrain.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> nullcontext</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Transformer, ModelArgs</span><br><span class="line"><span class="keyword">from</span> torch.distributed <span class="keyword">import</span> destroy_process_group, init_process_group</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> PretrainDataset</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0&#x27;</span></span><br><span class="line"><span class="comment"># To run with DDP on 4 gpus on 1 node, example:</span></span><br><span class="line"><span class="comment"># torchrun --standalone --nproc_per_node=4 pretrain.py</span></span><br><span class="line"><span class="comment"># OR python -m torch.distributed.launch --nproc_per_node=4 pretrain.py</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;接受三个参数：filename（文件名），verbosity（详细程度，默认为1），name（日志记录器的名称，默认为None）&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_logger</span>(<span class="params">filename, verbosity=<span class="number">1</span>, name=<span class="literal">None</span></span>):</span><br><span class="line">    level_dict = &#123;<span class="number">0</span>: logging.DEBUG, <span class="number">1</span>: logging.INFO, <span class="number">2</span>: logging.WARNING&#125;  <span class="comment"># 定义了一个字典level_dict，将整数级别映射到相应的日志级别</span></span><br><span class="line">    formatter = logging.Formatter(</span><br><span class="line">        <span class="string">&quot;[%(asctime)s][%(filename)s][%(levelname)s] %(message)s&quot;</span></span><br><span class="line">    )  <span class="comment"># 定义了一个日志格式化器formatter，用于设置日志消息的格式。格式中包含时间戳、文件名、日志级别和消息内容。</span></span><br><span class="line">    logger = logging.getLogger(name)  <span class="comment"># 创建一个日志记录器对象logger，使用给定的名称（name参数）</span></span><br><span class="line">    logger.setLevel(level_dict[verbosity])  <span class="comment"># 设置日志记录器的日志级别，根据传入的详细程度参数（verbosity）从level_dict字典中获取对应的日志级别</span></span><br><span class="line"></span><br><span class="line">    fh = logging.FileHandler(filename, <span class="string">&quot;w&quot;</span>)</span><br><span class="line">    fh.setFormatter(formatter)</span><br><span class="line">    logger.addHandler(fh)</span><br><span class="line"></span><br><span class="line">    sh = logging.StreamHandler()</span><br><span class="line">    sh.setFormatter(formatter)</span><br><span class="line">    logger.addHandler(sh)</span><br><span class="line">    <span class="keyword">return</span> logger</span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr</span>(<span class="params">it</span>):</span><br><span class="line">    <span class="comment"># 1) linear warmup for warmup_iters steps</span></span><br><span class="line">    <span class="keyword">if</span> it &lt; warmup_iters:  <span class="comment"># 线性预热</span></span><br><span class="line">        <span class="keyword">return</span> learning_rate * it / warmup_iters</span><br><span class="line">    <span class="comment"># 2) if it &gt; lr_decay_iters, return min learning rate</span></span><br><span class="line">    <span class="keyword">if</span> it &gt; lr_decay_iters:  <span class="comment"># 学习率衰减</span></span><br><span class="line">        <span class="keyword">return</span> min_lr</span><br><span class="line">    <span class="comment"># 3) in between, use cosine decay down to min learning rate</span></span><br><span class="line">    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= decay_ratio &lt;= <span class="number">1</span></span><br><span class="line">    coeff = <span class="number">0.5</span> * (<span class="number">1.0</span> + math.cos(math.pi * decay_ratio))  <span class="comment"># coeff ranges 0..1</span></span><br><span class="line">    <span class="keyword">return</span> min_lr + coeff * (learning_rate - min_lr)</span><br></pre></td></tr></table></figure>
<p>这里有一个关于AMP混合精度的用法，可以参考这篇文章的说明: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/533096307">https://zhuanlan.zhihu.com/p/533096307</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">epoch</span>):</span><br><span class="line">    start_time = time.time()  <span class="comment"># 记录当前时间作为训练开始的时间点</span></span><br><span class="line">    <span class="keyword">for</span> step, (X, Y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):  <span class="comment"># train_loader是一个数据加载器，step是它们在train_loader中的索引，输入数据（X）和对应的标签（Y）</span></span><br><span class="line">        X = X.to(device)</span><br><span class="line">        Y = Y.to(device)</span><br><span class="line">        lr = get_lr(epoch*iter_per_epoch+step) <span class="keyword">if</span> decay_lr <span class="keyword">else</span> learning_rate  <span class="comment"># 如果decay_lr为真，将调用get_lr</span></span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:  <span class="comment"># 更新优化器中的学习率，使其在训练过程中保持一致</span></span><br><span class="line">            param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line">        <span class="comment"># and using the GradScaler if data type is float16</span></span><br><span class="line">        <span class="comment"># for micro_step in range(gradient_accumulation_steps):</span></span><br><span class="line">        <span class="keyword">if</span> ddp:</span><br><span class="line">            <span class="comment"># in DDP training we only need to sync gradients at the last micro step.</span></span><br><span class="line">            <span class="comment"># the official way to do this is with model.no_sync() context manager, but</span></span><br><span class="line">            <span class="comment"># I really dislike that this bloats the code and forces us to repeat code</span></span><br><span class="line">            <span class="comment"># looking at the source of that context manager, it just toggles this variable</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            当gradient_accumulation_steps为1时，也就是只有一个微步时，不需要同步梯度；而当gradient_accumulation_steps大于1时，需要在最后一个微步同步梯度</span></span><br><span class="line"><span class="string">            假设gradient_accumulation_steps的值为N，那么在执行N个微步后才会执行一次反向传播。这样做的目的是减少显存的需求，特别是在显存受限的情况下</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            model.require_backward_grad_sync = <span class="number">0</span> == gradient_accumulation_steps - <span class="number">1</span></span><br><span class="line">        <span class="keyword">with</span> ctx:  <span class="comment"># 使用上下文管理器（ctx）来执行以下操作</span></span><br><span class="line">            logits = model(X, Y)  <span class="comment"># 通过模型对输入数据X和标签Y进行前向传播，得到预测结果</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;这里last_loss属性还未知&#x27;&#x27;&#x27;</span></span><br><span class="line">            loss = raw_model.last_loss</span><br><span class="line">            loss = loss / gradient_accumulation_steps  <span class="comment"># 计算平均损失</span></span><br><span class="line">        <span class="comment"># immediately async prefetch next batch while model is doing the forward pass on the GPU</span></span><br><span class="line">        <span class="comment"># backward pass, with gradient scaling if training in fp16</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;这行代码使用了 PyTorch 的混合精度训练技术（Automatic Mixed Precision, AMP）。scaler 是一个 torch.cuda.amp.GradScaler 对象，用于缩放梯度值&#x27;&#x27;&#x27;</span></span><br><span class="line">        scaler.scale(loss).backward()   <span class="comment"># scaler.scale(loss) 用于对损失函数值进行缩放</span></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % gradient_accumulation_steps == <span class="number">0</span>:  <span class="comment"># 梯度累积的步数</span></span><br><span class="line">            <span class="comment"># clip the gradient  即是否需要进行梯度裁剪</span></span><br><span class="line">            <span class="keyword">if</span> grad_clip != <span class="number">0.0</span>:</span><br><span class="line">                scaler.unscale_(optimizer)  <span class="comment"># 对优化器的梯度进行反缩放操作</span></span><br><span class="line">                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  <span class="comment"># 对模型的梯度进行裁剪，以防止梯度爆炸的问题</span></span><br><span class="line">            <span class="comment"># step the optimizer and scaler if training in fp16</span></span><br><span class="line">            scaler.step(optimizer)  <span class="comment"># 执行优化器的参数更新步骤</span></span><br><span class="line">            scaler.update()  <span class="comment"># 更新 GradScaler 的缩放因子。这是为了在下一次进行梯度计算之前，根据当前的梯度情况，更新缩放因子。</span></span><br><span class="line">            <span class="comment"># flush the gradients as soon as we can, no need for this memory anymore</span></span><br><span class="line">            optimizer.zero_grad(set_to_none=<span class="literal">True</span>)  <span class="comment"># 将模型的梯度置零</span></span><br><span class="line">        <span class="comment"># 打印日志</span></span><br><span class="line">        <span class="keyword">if</span> step % log_interval == <span class="number">0</span>:</span><br><span class="line">            spend_time = time.time()-start_time</span><br><span class="line">            logger.info(                                    <span class="comment"># 其中使用了 &#123;&#125; 占位符来填充不同的值</span></span><br><span class="line">                    <span class="string">&#x27;Epoch:[&#123;&#125;/&#123;&#125;](&#123;&#125;/&#123;&#125;) loss:&#123;:.3f&#125; lr:&#123;:.7f&#125; epoch_Time:&#123;&#125;min:&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                        epoch,</span><br><span class="line">                        max_epoch, </span><br><span class="line">                        step, </span><br><span class="line">                        iter_per_epoch,  <span class="comment"># 每个 epoch 的总步数</span></span><br><span class="line">                        loss.item(),   <span class="comment"># 填充损失函数的值</span></span><br><span class="line">                        optimizer.param_groups[-<span class="number">1</span>][<span class="string">&#x27;lr&#x27;</span>],</span><br><span class="line">                        spend_time / (step+<span class="number">1</span>) * iter_per_epoch // <span class="number">60</span> - spend_time // <span class="number">60</span>))   <span class="comment"># 填充当前 epoch 的时间消耗（单位为分钟）</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % save_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> ddp:</span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                只有在分布式训练中，进程的 rank 为 0 的进程才执行下面的代码块。这个条件通常用于确保只有一个进程保存模型</span></span><br><span class="line"><span class="string">                这里再保存模型前先设置为评估模式，保存完再设置为训练模式。主要针对batchnorm和dropout layer进行操作</span></span><br><span class="line"><span class="string">                我们保存矩阵参数不想得到丢弃神经元以后得结果，也希望得到整个训练过程累积的平均值和方差，而不是一个batch下的</span></span><br><span class="line"><span class="string">                注意eval和train模式下的区别</span></span><br><span class="line"><span class="string">                &#x27;&#x27;&#x27;</span></span><br><span class="line">                <span class="keyword">if</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">                    model.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式，即不进行梯度计算</span></span><br><span class="line">                    torch.save(model.module.state_dict(), <span class="string">&#x27;&#123;&#125;/iter_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(save_dir, <span class="built_in">int</span>(step+epoch*iter_per_epoch)))</span><br><span class="line">                    model.train()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.<span class="built_in">eval</span>()</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">&#x27;&#123;&#125;/iter_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(save_dir, <span class="built_in">int</span>(step+epoch*iter_per_epoch)))</span><br><span class="line">                model.train()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_model</span>():</span><br><span class="line">    <span class="comment"># 定义一个字典 model_args，这些参数包括维度 (dim)、层数 (n_layers)、头数 (n_heads)、键值头数 (n_kv_heads)、词汇表大小 (vocab_size)</span></span><br><span class="line">    <span class="comment"># 倍数 (multiple_of)、最大序列长度 (max_seq_len) 和丢弃率 (dropout)</span></span><br><span class="line">    model_args = <span class="built_in">dict</span>(</span><br><span class="line">        dim=dim,</span><br><span class="line">        n_layers=n_layers,</span><br><span class="line">        n_heads=n_heads,</span><br><span class="line">        n_kv_heads=n_heads,</span><br><span class="line">        vocab_size=<span class="number">64793</span>,</span><br><span class="line">        multiple_of=multiple_of,</span><br><span class="line">        max_seq_len=max_seq_len,</span><br><span class="line">        dropout=dropout,</span><br><span class="line">    )  <span class="comment"># start with model_args from command line</span></span><br><span class="line">    <span class="keyword">if</span> init_from == <span class="string">&quot;scratch&quot;</span>:  <span class="comment"># 表示从头开始初始化一个新的模型</span></span><br><span class="line">        <span class="comment"># init a new model from scratch</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Initializing a new model from scratch&quot;</span>)</span><br><span class="line">        gptconf = ModelArgs(**model_args)  <span class="comment"># 使用 model_args 字典创建一个 ModelArgs 对象 gptconf，用于配置模型的参数，用**解包字典</span></span><br><span class="line">        model = Transformer(gptconf)  <span class="comment"># 使用 gptconf 创建一个 Transformer 模型对象 model，这里已经是一个对象，不需要**的解包操作</span></span><br><span class="line">    <span class="keyword">elif</span> init_from == <span class="string">&quot;resume&quot;</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Resuming training from <span class="subst">&#123;out_dir&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="comment"># resume training from a checkpoint.</span></span><br><span class="line">        ckpt_path = os.path.join(out_dir, <span class="string">&quot;ckpt.pt&quot;</span>)  <span class="comment"># 构建检查点文件路径，将输出目录 out_dir 和文件名 &quot;ckpt.pt&quot; 连接起来</span></span><br><span class="line">        checkpoint = torch.load(ckpt_path, map_location=device)  <span class="comment"># 加载检查点文件，将其存储在checkpoint变量中，map_location用于指定加载的模型在哪个设备上运行</span></span><br><span class="line">        checkpoint_model_args = checkpoint[<span class="string">&quot;model_args&quot;</span>]  <span class="comment"># 从检查点中获取模型参数 model_args</span></span><br><span class="line">        <span class="comment"># force these config attributes to be equal otherwise we can&#x27;t even resume training</span></span><br><span class="line">        <span class="comment"># the rest of the attributes (e.g. dropout) can stay as desired from command line</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="string">&quot;dim&quot;</span>, <span class="string">&quot;n_layers&quot;</span>, <span class="string">&quot;n_heads&quot;</span>, <span class="string">&quot;n_kv_heads&quot;</span>, <span class="string">&quot;vocab_size&quot;</span>, <span class="string">&quot;multiple_of&quot;</span>, <span class="string">&quot;max_seq_len&quot;</span>]:</span><br><span class="line">            model_args[k] = checkpoint_model_args[k]  <span class="comment"># 遍历需要强制相等的模型参数</span></span><br><span class="line">        <span class="comment"># create the model</span></span><br><span class="line">        gptconf = ModelArgs(**model_args)</span><br><span class="line">        model = Transformer(gptconf)</span><br><span class="line">        state_dict = checkpoint[<span class="string">&quot;model&quot;</span>]  <span class="comment"># 从检查点中获取模型的状态字典，获取矩阵参数</span></span><br><span class="line">        <span class="comment"># fix the keys of the state dictionary </span></span><br><span class="line">        <span class="comment"># honestly no idea how checkpoints sometimes get this prefix, have to debug more</span></span><br><span class="line">        unwanted_prefix = <span class="string">&quot;_orig_mod.&quot;</span>  <span class="comment"># 这段代码的作用是处理加载的状态字典中的键，去除指定的前缀 unwanted_prefix</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        这里的用法很巧妙，对于items()，需要把迭代器转换为list才能修改键值</span></span><br><span class="line"><span class="string">        对于有这些前缀开头的，state_dict.pop(k)相当于把这些键给删除掉，同时返回这个键的value值，不过这个value值赋给了新的去掉前缀的键state_dict[k[len(unwanted_prefix):]]</span></span><br><span class="line"><span class="string">        这个小问题在注释中可见，发生是未知的，待进一步学习再解释</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">list</span>(state_dict.items()):  <span class="comment"># 遍历状态字典中的键值对</span></span><br><span class="line">            <span class="keyword">if</span> k.startswith(unwanted_prefix):  <span class="comment"># 检查键是否以指定的前缀 unwanted_prefix 开头</span></span><br><span class="line">                state_dict[k[<span class="built_in">len</span>(unwanted_prefix):]] = state_dict.pop(k)</span><br><span class="line">        model.load_state_dict(state_dict)  <span class="comment"># 将更新后的状态字典加载到模型 model 中，恢复模型的参数</span></span><br><span class="line">        iter_num = checkpoint[<span class="string">&quot;iter_num&quot;</span>]</span><br><span class="line">        best_val_loss = checkpoint[<span class="string">&quot;best_val_loss&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>下面是关于字典用法的一些补充代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># input</span></span><br><span class="line">state_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;a&#x27;</span>: <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;b&#x27;</span>: <span class="string">&#x27;dog&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;c&#x27;</span>: <span class="string">&#x27;fish&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">list</span>(state_dict.items()):</span><br><span class="line">    <span class="built_in">print</span>(k, v)</span><br><span class="line"><span class="built_in">print</span>(state_dict.items())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(state_dict.items()))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(state_dict))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">list</span>(state_dict.items()):  <span class="comment"># 如果不加list会报错</span></span><br><span class="line">    state_dict[k + <span class="string">&#x27;_1&#x27;</span>] = v + <span class="string">&#x27;_1&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(state_dict.items()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">a cat</span><br><span class="line">b dog</span><br><span class="line">c fish</span><br><span class="line">dict_items([(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;fish&#x27;</span>)])</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;fish&#x27;</span>)]</span><br><span class="line">[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;fish&#x27;</span>), (<span class="string">&#x27;a_1&#x27;</span>, <span class="string">&#x27;cat_1&#x27;</span>), (<span class="string">&#x27;b_1&#x27;</span>, <span class="string">&#x27;dog_1&#x27;</span>), (<span class="string">&#x27;c_1&#x27;</span>, <span class="string">&#x27;fish_1&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># I/O</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    out_dir = <span class="string">&#x27;out&#x27;</span></span><br><span class="line">    max_epoch = <span class="number">1</span></span><br><span class="line">    eval_interval = <span class="number">1</span></span><br><span class="line">    log_interval = <span class="number">100</span></span><br><span class="line">    save_interval = <span class="number">10000</span></span><br><span class="line">    eval_iters = <span class="number">200</span></span><br><span class="line">    eval_only = <span class="literal">False</span>  <span class="comment"># if True, script exits right after the first eval</span></span><br><span class="line">    always_save_checkpoint = <span class="literal">True</span>  <span class="comment"># if True, always save a checkpoint after each eval</span></span><br><span class="line">    init_from = <span class="string">&#x27;scratch&#x27;</span>  <span class="comment"># &#x27;scratch&#x27; or &#x27;resume&#x27; or &#x27;gpt2*&#x27;</span></span><br><span class="line">    gradient_accumulation_steps = <span class="number">1</span>  <span class="comment"># used to simulate larger batch sizes</span></span><br><span class="line">    batch_size = <span class="number">32</span>  <span class="comment"># if gradient_accumulation_steps &gt; 1, this is the micro-batch size</span></span><br><span class="line">    <span class="comment"># model 根据需要更改 </span></span><br><span class="line">    max_seq_len = <span class="number">512</span></span><br><span class="line">    dim = <span class="number">512</span></span><br><span class="line">    n_layers = <span class="number">8</span></span><br><span class="line">    n_heads = <span class="number">8</span></span><br><span class="line">    multiple_of = <span class="number">32</span></span><br><span class="line">    dropout = <span class="number">0.0</span>  <span class="comment"># for pretraining 0 is good, for finetuning try 0.1+</span></span><br><span class="line">    bias = <span class="literal">False</span>  <span class="comment"># do we use bias inside LayerNorm and Linear layers?</span></span><br><span class="line">    <span class="comment"># adamw optimizer</span></span><br><span class="line">    learning_rate = <span class="number">3e-4</span>  <span class="comment"># max learning rate</span></span><br><span class="line">    weight_decay = <span class="number">1e-1</span></span><br><span class="line">    beta1 = <span class="number">0.9</span></span><br><span class="line">    beta2 = <span class="number">0.95</span></span><br><span class="line">    grad_clip = <span class="number">1.0</span>  <span class="comment"># clip gradients at this value, or disable if == 0.0</span></span><br><span class="line">    <span class="comment"># learning rate decay settings</span></span><br><span class="line">    decay_lr = <span class="literal">True</span>  <span class="comment"># whether to decay the learning rate</span></span><br><span class="line">    warmup_iters = <span class="number">1000</span>  <span class="comment"># how many steps to warm up for</span></span><br><span class="line">    lr_decay_iters = <span class="number">80000</span>  <span class="comment"># should be ~= max_iters per Chinchilla</span></span><br><span class="line">    min_lr = <span class="number">1e-5</span>  <span class="comment"># minimum learning rate, should be ~= learning_rate/10 per Chinchilla</span></span><br><span class="line">    <span class="comment"># DDP settings</span></span><br><span class="line">    backend = <span class="string">&#x27;nccl&#x27;</span>  <span class="comment"># &#x27;nccl&#x27;, &#x27;gloo&#x27;, etc.</span></span><br><span class="line">    <span class="comment"># system</span></span><br><span class="line">    device = <span class="string">&#x27;cuda&#x27;</span>  <span class="comment"># examples: &#x27;cpu&#x27;, &#x27;cuda&#x27;, &#x27;cuda:0&#x27;, &#x27;cuda:1&#x27; etc., or try &#x27;mps&#x27; on macbooks</span></span><br><span class="line">    dtype = <span class="string">&#x27;float16&#x27;</span>  <span class="comment"># &#x27;float32&#x27;, &#x27;bfloat16&#x27;, or &#x27;float16&#x27;, the latter will auto implement a GradScaler</span></span><br><span class="line">    <span class="built_in">compile</span> = <span class="literal">False</span>  <span class="comment"># use PyTorch 2.0 to compile the model to be faster</span></span><br><span class="line">    <span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line">    config_keys = [</span><br><span class="line">        k  <span class="comment"># 下面两行是列表推导式 使用globals().items()获取了当前作用域中的全局变量字典，并通过迭代字典的键值对来访问每个全局变量的名称（键）</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">globals</span>().items()  <span class="comment"># 获取的全局变量字典的键值对</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> k.startswith(<span class="string">&quot;_&quot;</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(v, (<span class="built_in">int</span>, <span class="built_in">float</span>, <span class="built_in">bool</span>, <span class="built_in">str</span>))  <span class="comment"># 条件判断语句只是一个简单的过滤条件，无需进一步的代码块，因此没有缩进</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># exec(open(&quot;configurator.py&quot;).read())  # overrides from command line or config file</span></span><br><span class="line">    <span class="comment"># config = &#123;k: globals()[k] for k in config_keys&#125;  # will be useful for logging</span></span><br><span class="line">    <span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    save_dir = os.path.join(out_dir, <span class="string">&#x27;pretrain&#x27;</span>)  <span class="comment"># 将out_dir和字符串&#x27;pretrain&#x27;拼接成一个路径，并将结果赋值给变量save_dir</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_dir):</span><br><span class="line">        os.makedirs(save_dir)</span><br><span class="line">    logger = get_logger(os.path.join(save_dir, <span class="string">&#x27;log.log&#x27;</span>))  <span class="comment"># get_logger()函数返回一个日志记录器对象，并将其赋值给变量logger</span></span><br><span class="line">    <span class="comment"># various inits, derived attributes, I/O setup</span></span><br><span class="line">    <span class="comment"># various inits, derived attributes, I/O setup</span></span><br><span class="line">    <span class="comment"># 从环境变量RANK中获取值，并通过os.environ.get()函数获取该值。如果RANK环境变量不存在，则返回默认值-1。然后，将获取的值转换为整数类型，并与-1进行比较</span></span><br><span class="line">    <span class="comment"># 如果两者不相等，则将ddp变量设置为True，表示代码正在分布式数据并行（DDP）运行环境中。否则，将ddp变量设置为False。</span></span><br><span class="line">    ddp = <span class="built_in">int</span>(os.environ.get(<span class="string">&quot;RANK&quot;</span>, -<span class="number">1</span>)) != -<span class="number">1</span>  <span class="comment"># is this a ddp run?</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> ddp:</span><br><span class="line">        <span class="comment"># Check if the operating system is Windows</span></span><br><span class="line">        <span class="keyword">if</span> os.name == <span class="string">&#x27;nt&#x27;</span>:</span><br><span class="line">            <span class="comment"># Diff between backends: https://pytorch.org/docs/stable/distributed.html</span></span><br><span class="line">            init_process_group(backend=<span class="string">&quot;gloo&quot;</span>)  <span class="comment"># 目前只支持使用&quot;gloo&quot;后端进行分布式训练。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># If the operating system is Linux based, os.name == &#x27;posix&#x27; （os.name == &#x27;posix&#x27;）</span></span><br><span class="line">            init_process_group(backend=<span class="string">&quot;nccl&quot;</span>)</span><br><span class="line">        <span class="comment"># rank用于全局标识，用于进程间的全局通信和同步操作。</span></span><br><span class="line">        <span class="comment"># local_rank用于在节点内标识，反映进程在节点内的本地性能排名和资源需求</span></span><br><span class="line">        ddp_rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;RANK&quot;</span>])</span><br><span class="line">        ddp_local_rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;LOCAL_RANK&quot;</span>])</span><br><span class="line">        ddp_world_size = <span class="built_in">int</span>(os.environ[<span class="string">&quot;WORLD_SIZE&quot;</span>])</span><br><span class="line">        device = <span class="string">f&quot;cuda:<span class="subst">&#123;ddp_local_rank&#125;</span>&quot;</span>  <span class="comment"># 当前进程使用的GPU设备为当前进程的本地排名。</span></span><br><span class="line">        torch.cuda.set_device(device)  <span class="comment"># 这行代码设置当前进程使用的CUDA设备，确保每个进程使用不同的CUDA设备 并且使得device中包含&quot;cuda“</span></span><br><span class="line">        master_process = ddp_rank == <span class="number">0</span>  <span class="comment"># this process will do logging, checkpointing etc. 判断当前进程是否为主进程</span></span><br><span class="line">        seed_offset = ddp_rank  <span class="comment"># each process gets a different seed 将随机种子的偏移量(seed_offset)设置为当前进程的排名(ddp_rank)</span></span><br><span class="line">        <span class="comment"># world_size number of processes will be training simultaneously, so we can scale</span></span><br><span class="line">        <span class="comment"># down the desired gradient accumulation iterations per process proportionally</span></span><br><span class="line">        <span class="comment"># assert gradient_accumulation_steps % ddp_world_size == 0</span></span><br><span class="line">        <span class="comment"># gradient_accumulation_steps //= ddp_world_size</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># if not ddp, we are running on a single gpu, and one process</span></span><br><span class="line">        master_process = <span class="literal">True</span></span><br><span class="line">        seed_offset = <span class="number">0</span></span><br><span class="line">        ddp_world_size = <span class="number">1</span></span><br><span class="line">    tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * max_seq_len  <span class="comment"># 计算每次迭代处理的标记数</span></span><br><span class="line">    <span class="keyword">if</span> master_process:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;tokens per iteration will be: <span class="subst">&#123;tokens_per_iter:,&#125;</span>&quot;</span>)  <span class="comment"># 打印每次迭代处理的标记数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;breaks down as: <span class="subst">&#123;gradient_accumulation_steps&#125;</span> grad accum steps * <span class="subst">&#123;ddp_world_size&#125;</span> processes * <span class="subst">&#123;batch_size&#125;</span> batch size * <span class="subst">&#123;max_seq_len&#125;</span> max seq len&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> master_process:</span><br><span class="line">        os.makedirs(out_dir, exist_ok=<span class="literal">True</span>)  <span class="comment"># 如果是主进程，创建一个输出目录out_dir，如果目录已存在则忽略</span></span><br><span class="line">    torch.manual_seed(<span class="number">1337</span> + seed_offset)  <span class="comment"># 设置随机种子，用于在训练过程中产生随机性</span></span><br><span class="line">    torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span>  <span class="comment"># allow tf32 on matmul 允许在CUDA上使用TensorFloat-32 (TF32)加速矩阵乘法运算</span></span><br><span class="line">    torch.backends.cudnn.allow_tf32 = <span class="literal">True</span>  <span class="comment"># allow tf32 on cudnn 允许在CUDA上使用TensorFloat-32 (TF32)加速CuDNN运算</span></span><br><span class="line">    device_type = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> <span class="string">&quot;cuda&quot;</span> <span class="keyword">in</span> device <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>  <span class="comment"># for later use in torch.autocast 根据device的名称判断设备类型</span></span><br><span class="line">    <span class="comment"># note: float16 data type will automatically use a GradScaler</span></span><br><span class="line">    ptdtype = &#123;<span class="string">&quot;float32&quot;</span>: torch.float32, <span class="string">&quot;bfloat16&quot;</span>: torch.bfloat16, <span class="string">&quot;float16&quot;</span>: torch.float16&#125;[dtype]</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    nullcontext() 是Python 3.7 引入的一个上下文管理器，它是一个空的上下文管理器，不进行任何操作，只是作为一个占位符。当使用 nullcontext() </span></span><br><span class="line"><span class="string">    作为上下文管理器时，代码块内的操作不会受到上下文管理器的影响，相当于没有上下文管理器存在。</span></span><br><span class="line"><span class="string">    如果当前设备是 CPU，则 ctx 是一个空的上下文管理器；如果当前设备是 GPU，则 ctx 是一个启用了自动混合精度的上下文管理器。</span></span><br><span class="line"><span class="string">    这样，在后续的代码中使用 with ctx: 就可以根据设备类型自动选择相应的上下文管理器，从而实现相应的操作或优化。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    ctx = (</span><br><span class="line">        nullcontext()</span><br><span class="line">        <span class="keyword">if</span> device_type == <span class="string">&quot;cpu&quot;</span></span><br><span class="line">        <span class="keyword">else</span> torch.cuda.amp.autocast()</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    best_val_loss = <span class="number">1e9</span>  <span class="comment"># 将最佳验证损失初始化为一个较大的值</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># -----init dataloader------</span></span><br><span class="line">    data_path_list = [  <span class="comment"># 定义数据路径列表，包含用于预训练的数据文件路径</span></span><br><span class="line">        <span class="string">&#x27;./data/pretrain_data.bin&#x27;</span></span><br><span class="line">        <span class="comment"># &#x27;./data/baidubaike_563w.bin&#x27;,</span></span><br><span class="line">        <span class="comment"># &#x27;./data/medical_book.bin&#x27;,</span></span><br><span class="line">        <span class="comment"># &#x27;./data/medical_encyclopedia.bin&#x27;,</span></span><br><span class="line">        <span class="comment"># &#x27;./data/medical_qa.bin&#x27;,</span></span><br><span class="line">        <span class="comment"># &#x27;./data/wiki.bin&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 创建预训练数据集对象 train_ds，使用指定的数据路径列表、最大序列长度和 memmap=True 参数</span></span><br><span class="line">    train_ds = PretrainDataset(data_path_list, max_length=max_seq_len, memmap=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 创建分布式采样器 train_sampler，用于在分布式环境中对训练数据集进行采样</span></span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(train_ds)</span><br><span class="line">    train_loader = torch.utils.data.DataLoader(  <span class="comment"># 用于按批次加载训练数据</span></span><br><span class="line">        train_ds,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        pin_memory=<span class="literal">False</span>,</span><br><span class="line">        drop_last=<span class="literal">False</span>,</span><br><span class="line">        shuffle=<span class="literal">False</span>,        </span><br><span class="line">        num_workers=<span class="number">0</span> <span class="keyword">if</span> os.name == <span class="string">&#x27;nt&#x27;</span> <span class="keyword">else</span> <span class="number">4</span>,</span><br><span class="line">        sampler=train_sampler</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># val_ds = PretrainDataset(data_path_list, max_length=256)</span></span><br><span class="line">    <span class="comment"># val_loader = torch.utils.data.DataLoader(</span></span><br><span class="line">    <span class="comment">#     val_ds,</span></span><br><span class="line">    <span class="comment">#     batch_size=batch_size,</span></span><br><span class="line">    <span class="comment">#     pin_memory=False,</span></span><br><span class="line">    <span class="comment">#     drop_last=False,</span></span><br><span class="line">    <span class="comment">#     shuffle=False,        </span></span><br><span class="line">    <span class="comment">#     num_workers=0,</span></span><br><span class="line">    <span class="comment"># )</span></span><br><span class="line">    <span class="comment"># init model</span></span><br><span class="line">    model = init_model()</span><br><span class="line">    model.to(device)  <span class="comment"># 将模型移动到指定的设备，确保在指定设备上进行计算</span></span><br><span class="line">    <span class="comment"># initialize a GradScaler. If enabled=False scaler is a no-op 初始化一个梯度缩放器（GradScaler）</span></span><br><span class="line">    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == <span class="string">&#x27;float16&#x27;</span>))  <span class="comment"># 如果dtype为&#x27;float16&#x27;，则启用梯度缩放器，否则禁用</span></span><br><span class="line">    <span class="comment"># optimizer</span></span><br><span class="line">    optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)</span><br><span class="line">    <span class="comment"># compile the model  如果用的pytorch2.0版本以上可以设置compile为True！！！</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">compile</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;compiling the model... (takes a ~minute)&quot;</span>)</span><br><span class="line">        unoptimized_model = model</span><br><span class="line">        model = torch.<span class="built_in">compile</span>(model)  <span class="comment"># requires PyTorch 2.0</span></span><br><span class="line">    <span class="comment"># wrap model into DDP container</span></span><br><span class="line">    <span class="keyword">if</span> ddp:</span><br><span class="line">        <span class="comment"># Ignore the `freqs_cis` buffer so that DDP does not broadcast it at</span></span><br><span class="line">        <span class="comment"># construction time since NCCL does not support `ComplexFloat`</span></span><br><span class="line">        prefix = <span class="string">&quot;_orig_mod.&quot;</span> <span class="keyword">if</span> <span class="built_in">compile</span> <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        model._ddp_params_and_buffers_to_ignore = &#123;prefix + <span class="string">&quot;freqs_cis&quot;</span>&#125;  <span class="comment"># 这个用法没有看懂？？？</span></span><br><span class="line">        model = DDP(model, device_ids=[ddp_local_rank])</span><br><span class="line">        <span class="comment"># 使用分布式数据并行（DDP）将模型封装为DDP容器。device_ids参数指定使用的设备ID列表，</span></span><br><span class="line">    raw_model = model.module <span class="keyword">if</span> ddp <span class="keyword">else</span> model  <span class="comment"># unwrap DDP container if needed</span></span><br><span class="line">    <span class="comment"># training loop</span></span><br><span class="line">    iter_per_epoch = <span class="built_in">len</span>(train_loader)  <span class="comment"># 计算每个训练轮次中迭代的次数，train_loader是训练数据的数据加载器</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        train_epoch(epoch)</span><br><span class="line">        <span class="comment"># val_loss=valid_epoch(epoch)</span></span><br><span class="line">        <span class="keyword">if</span> ddp:</span><br><span class="line">            <span class="keyword">if</span> torch.distributed.get_rank() == <span class="number">0</span>:  <span class="comment"># 一般用0，当然，可以选任意的rank保存。</span></span><br><span class="line">                torch.save(raw_model.state_dict(), <span class="string">&#x27;&#123;&#125;/epoch_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(save_dir, epoch))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            torch.save(raw_model.state_dict(), <span class="string">&#x27;&#123;&#125;/epoch_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(save_dir, epoch))</span><br><span class="line">    <span class="keyword">if</span> ddp:</span><br><span class="line">        destroy_process_group()  <span class="comment"># 销毁进程组，清理DDP相关资源</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/15/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB3/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-P-Tuning代码解析" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-P-Tuning%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/">大模型初步-P-Tuning微调技术</a>
    </h1>
  

        
        <a href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-P-Tuning%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" class="archive-article-date">
  	<time datetime="2024-05-11T15:13:59.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-11</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这里参考的是苏剑林对于P-Tuning的讲解，不过苏神是用keras实现的，这个框架我不是很熟悉，所以还是到peft的github官网上找相关微调的代码，看这些开源的代码，可以从中学到很多规范和代码实现思路。<br>网站地址:<a target="_blank" rel="noopener" href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/p_tuning/model.py">https://github.com/huggingface/peft/blob/main/src/peft/tuners/p_tuning/model.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .config <span class="keyword">import</span> PromptEncoderConfig, PromptEncoderReparameterizationType</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PromptEncoder</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The prompt encoder network that is used to generate the virtual token embeddings for p-tuning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        config ([`PromptEncoderConfig`]): The configuration of the prompt encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; from peft import PromptEncoder, PromptEncoderConfig</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; config = PromptEncoderConfig(</span></span><br><span class="line"><span class="string">    ...     peft_type=&quot;P_TUNING&quot;,</span></span><br><span class="line"><span class="string">    ...     task_type=&quot;SEQ_2_SEQ_LM&quot;,</span></span><br><span class="line"><span class="string">    ...     num_virtual_tokens=20,</span></span><br><span class="line"><span class="string">    ...     token_dim=768,</span></span><br><span class="line"><span class="string">    ...     num_transformer_submodules=1,</span></span><br><span class="line"><span class="string">    ...     num_attention_heads=12,</span></span><br><span class="line"><span class="string">    ...     num_layers=12,</span></span><br><span class="line"><span class="string">    ...     encoder_reparameterization_type=&quot;MLP&quot;,</span></span><br><span class="line"><span class="string">    ...     encoder_hidden_size=768,</span></span><br><span class="line"><span class="string">    ... )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; prompt_encoder = PromptEncoder(config)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    **Attributes**:</span></span><br><span class="line"><span class="string">        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **mlp_head** (`torch.nn.Sequential`) -- The MLP head of the prompt encoder if `inference_mode=False`.</span></span><br><span class="line"><span class="string">        - **lstm_head** (`torch.nn.LSTM`) -- The LSTM head of the prompt encoder if `inference_mode=False` and</span></span><br><span class="line"><span class="string">        `encoder_reparameterization_type=&quot;LSTM&quot;`.</span></span><br><span class="line"><span class="string">        - **token_dim** (`int`) -- The hidden embedding dimension of the base transformer model.</span></span><br><span class="line"><span class="string">        - **input_size** (`int`) -- The input size of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **output_size** (`int`) -- The output size of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **hidden_size** (`int`) -- The hidden size of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **total_virtual_tokens** (`int`): The total number of virtual tokens of the</span></span><br><span class="line"><span class="string">        prompt encoder.</span></span><br><span class="line"><span class="string">        - **encoder_type** (Union[[`PromptEncoderReparameterizationType`], `str`]): The encoder type of the prompt</span></span><br><span class="line"><span class="string">          encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input shape: (`batch_size`, `total_virtual_tokens`)</span></span><br><span class="line"><span class="string">    Output shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.token_dim = config.token_dim</span><br><span class="line">        self.input_size = self.token_dim</span><br><span class="line">        self.output_size = self.token_dim</span><br><span class="line">        self.hidden_size = config.encoder_hidden_size</span><br><span class="line">        self.total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules</span><br><span class="line">        self.encoder_type = config.encoder_reparameterization_type</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding</span></span><br><span class="line">        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;先判断是不是推理模式&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> config.inference_mode:   </span><br><span class="line">            <span class="keyword">if</span> self.encoder_type == PromptEncoderReparameterizationType.LSTM:</span><br><span class="line">                lstm_dropout = config.encoder_dropout</span><br><span class="line">                num_layers = config.encoder_num_layers</span><br><span class="line">                <span class="comment"># LSTM</span></span><br><span class="line">                self.lstm_head = torch.nn.LSTM(</span><br><span class="line">                    input_size=self.input_size,</span><br><span class="line">                    hidden_size=self.hidden_size,</span><br><span class="line">                    num_layers=num_layers,</span><br><span class="line">                    dropout=lstm_dropout,</span><br><span class="line">                    bidirectional=<span class="literal">True</span>,</span><br><span class="line">                    batch_first=<span class="literal">True</span>,</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                self.mlp_head = torch.nn.Sequential(</span><br><span class="line">                    torch.nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size * <span class="number">2</span>),</span><br><span class="line">                    torch.nn.ReLU(),</span><br><span class="line">                    torch.nn.Linear(self.hidden_size * <span class="number">2</span>, self.output_size),</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> self.encoder_type == PromptEncoderReparameterizationType.MLP:</span><br><span class="line">                encoder_num_layers_default = PromptEncoderConfig.encoder_num_layers</span><br><span class="line">                <span class="keyword">if</span> config.encoder_num_layers != encoder_num_layers_default:</span><br><span class="line">                    warnings.warn(</span><br><span class="line">                        <span class="string">f&quot;for <span class="subst">&#123;self.encoder_type.value&#125;</span>, the argument `encoder_num_layers` is ignored. &quot;</span></span><br><span class="line">                        <span class="string">f&quot;Exactly <span class="subst">&#123;encoder_num_layers_default&#125;</span> MLP layers are used.&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                layers = [</span><br><span class="line">                    torch.nn.Linear(self.input_size, self.hidden_size),</span><br><span class="line">                    torch.nn.ReLU(),</span><br><span class="line">                    torch.nn.Linear(self.hidden_size, self.hidden_size),</span><br><span class="line">                    torch.nn.ReLU(),</span><br><span class="line">                    torch.nn.Linear(self.hidden_size, self.output_size),</span><br><span class="line">                ]</span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;将列表解包为单独的参数传入&#x27;&#x27;&#x27;</span></span><br><span class="line">                self.mlp_head = torch.nn.Sequential(*layers) </span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, indices</span>):</span><br><span class="line">        input_embeds = self.embedding(indices)</span><br><span class="line">        <span class="keyword">if</span> self.encoder_type == PromptEncoderReparameterizationType.LSTM:</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            这里的在经过lstm_head后返回的是一个元组，会包含两个元素：第一个元素是所有时间步的输出（output），第二个元素是最后一个时间步的隐藏状态和细胞状态（hidden）</span></span><br><span class="line"><span class="string">            如果这里使用的是双向lstm，最后一维是768*2，如果是单向lstm，最后一维的大小就是768</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            output_embeds = self.mlp_head(self.lstm_head(input_embeds)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">elif</span> self.encoder_type == PromptEncoderReparameterizationType.MLP:</span><br><span class="line">            output_embeds = self.mlp_head(input_embeds)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_embeds</span><br></pre></td></tr></table></figure>
<!-- ```python
import numpy as np
from bert4keras.backend import keras, K
from bert4keras.layers import Loss, Embedding
from bert4keras.tokenizers import Tokenizer
from bert4keras.models import build_transformer_model, BERT
from bert4keras.optimizers import Adam
from bert4keras.snippets import sequence_padding, DataGenerator
from bert4keras.snippets import open
from keras.layers import Lambda, Dense

maxlen = 128
batch_size = 32
config_path = '/root/kg/bert/chinese_nezha_gpt_L-12_H-768_A-12/config.json'
checkpoint_path = '/root/kg/bert/chinese_nezha_gpt_L-12_H-768_A-12/gpt.ckpt'
dict_path = '/root/kg/bert/chinese_nezha_gpt_L-12_H-768_A-12/vocab.txt'

def load_data(filename):
    D = []
    with open(filename, encoding='utf-8') as f:
        for l in f:
            '''l.strip()去除行首尾的空白字符（如空格、换行符等），split('\t')根据制表符\t将行分割成两部分，进行数值计算'''
            text, label = l.strip().split('\t')
            D.append((text, int(label)))
    return D

train_data = load_data('datasets/sentiment/sentiment.train.data')
valid_data = load_data('datasets/sentiment/sentiment.valid.data')
test_data = load_data('datasets/sentiment/sentiment.test.data')

'''模拟标注和非标注数据，可以模拟一个数据稀缺的场景'''
train_frac = 0.01  # 标注数据的比例
num_labeled = int(len(train_data) * train_frac)
unlabeled_data = [(t, 2) for t, l in train_data[num_labeled:]]
train_data = train_data[:num_labeled]
# train_data = train_data + unlabeled_data

'''应用分词器，转换为小写形式'''
tokenizer = Tokenizer(dict_path, do_lower_case=True)

# 对应的任务描述
'''pos_id表示正面情感，neg_id表示负面情感'''
desc = ['[unused%s]' % i for i in range(1, 9)]
desc_ids = [tokenizer.token_to_id(t) for t in desc]
pos_id = tokenizer.token_to_id(u'很')
neg_id = tokenizer.token_to_id(u'不')
``` -->
<!-- ```python
class data_generator(DataGenerator):
    """数据生成器，注意这里的token_ids组合方式"""
    def __iter__(self, random=False):
        batch_token_ids = []
        for is_end, (text, label) in self.sample(random):
            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)
            token_ids = token_ids[:1] + desc_ids[:4] + token_ids[:-1]
            token_ids = token_ids + desc_ids[4:]
            if label == 0:
                token_ids = token_ids + [neg_id]
            elif label == 1:
                token_ids = token_ids + [pos_id]
            batch_token_ids.append(token_ids)
            if len(batch_token_ids) == self.batch_size or is_end:
                batch_token_ids = sequence_padding(batch_token_ids)
                '''yield是生成器的使用方法，注意yield完要清空列表'''
                yield batch_token_ids, None
                batch_token_ids = []

class CrossEntropy(Loss):
    """交叉熵作为loss，并mask掉padding部分"""
    def compute_loss(self, inputs, mask=None):
        '''将输入的元组拆分为真实标签y_true和预测标签y_pred，这里在inputs为元组或者列表的时候且长度相同的时候可以调用'''
        y_true, y_pred = inputs
        if mask[1] is None:
            y_mask = 1.0
        '''cast函数将整数mask[1]转换为浮点数 -- eg: y = K.cast(x, K.floatx())'''
        else:
            y_mask = K.cast(mask[1], K.floatx())[:, 1:]
        y_true = y_true[:, 1:]  # 目标token_ids
        y_pred = y_pred[:, :-1]  # 预测序列，错开一位
        accuracy = keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
        accuracy = K.sum(accuracy * y_mask) / K.sum(y_mask)
        self.add_metric(accuracy, name='accuracy')
        loss = K.sparse_categorical_crossentropy(y_true, y_pred)
        loss = K.sum(loss * y_mask) / K.sum(y_mask)
        return loss

class PtuningEmbedding(Embedding):
    """新定义Embedding层，只优化部分Token"""
    def call(self, inputs, mode='embedding'):
        embeddings = self.embeddings
        embeddings_sg = K.stop_gradient(embeddings)
        mask = np.zeros((K.int_shape(embeddings)[0], 1))
        mask[1:9] += 1  # 只优化id为1～8的token
        '''非常巧妙的只计算前8个token的梯度值'''
        self.embeddings = embeddings * mask + embeddings_sg * (1 - mask)
        outputs = super(PtuningEmbedding, self).call(inputs, mode)
        self.embeddings = embeddings
        return outputs

class PtuningBERT(BERT):
    """替换原来的Embedding"""
    def apply(self, inputs=None, layer=None, arguments=None, **kwargs):
        if layer is Embedding:
            layer = PtuningEmbedding
        return super(PtuningBERT,
                     self).apply(inputs, layer, arguments, **kwargs)
``` -->
<!-- ```python

model = build_transformer_model(
    config_path=config_path,
    checkpoint_path=checkpoint_path,
    model=PtuningBERT,
    segment_vocab_size=0,  # 去掉segmeng_ids输入
    application='lm',
)  # 建立模型，加载权重

for layer in model.layers:
    if layer.name != 'Embedding-Token':
        layer.trainable = False

output = CrossEntropy(1)([model.input, model.output])

model = keras.models.Model(model.input, output)
model.compile(optimizer=Adam(6e-4))
model.summary()

# 转换数据集
train_generator = data_generator(train_data, batch_size)
valid_generator = data_generator(valid_data, batch_size)
test_generator = data_generator(test_data, batch_size)

class Evaluator(keras.callbacks.Callback):
    def __init__(self):
        self.best_val_acc = 0.

    def on_epoch_end(self, epoch, logs=None):
        val_acc = evaluate(valid_generator)
        if val_acc > self.best_val_acc:
            self.best_val_acc = val_acc
            model.save_weights('best_model_gpt.weights')
        test_acc = evaluate(test_generator)
        print(
            u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\n' %
            (val_acc, self.best_val_acc, test_acc)
        )

def evaluate(data):
    total, right = 0., 0.
    for x_true, _ in data:
        y_pred = model.predict(x_true)
        for x, y in zip(x_true, y_pred):
            x = np.trim_zeros(x)
            y = y[:len(x)][-2, [neg_id, pos_id]].argmax()
            y = [neg_id, pos_id][y]
            if y == x[-1]:
                right += 1
            total += 1
    return right / total

if __name__ == '__main__':

    evaluator = Evaluator()
    model.fit_generator(
        train_generator.forfit(),
        steps_per_epoch=len(train_generator) * 50,
        epochs=1000,
        callbacks=[evaluator]
    )
else:
    model.load_weights('best_model_gpt.weights')
``` -->

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-P-Tuning%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-P-Tuning微调技术" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-P-Tuning%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/">大模型初步-P-Tuning微调技术</a>
    </h1>
  

        
        <a href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-P-Tuning%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/" class="archive-article-date">
  	<time datetime="2024-05-11T15:13:59.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-11</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这里参考的是苏剑林对于P-Tuning的实现，不过这里是用keras实现的，这个框架我不是很熟悉，所以还是到peft的github官网上找相关微调的代码，看这些开源的代码，可以从中学到很多规范和代码实现思路。<br>网站地址:<a target="_blank" rel="noopener" href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/p_tuning/model.py">https://github.com/huggingface/peft/blob/main/src/peft/tuners/p_tuning/model.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .config <span class="keyword">import</span> PromptEncoderConfig, PromptEncoderReparameterizationType</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PromptEncoder</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The prompt encoder network that is used to generate the virtual token embeddings for p-tuning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        config ([`PromptEncoderConfig`]): The configuration of the prompt encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; from peft import PromptEncoder, PromptEncoderConfig</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; config = PromptEncoderConfig(</span></span><br><span class="line"><span class="string">    ...     peft_type=&quot;P_TUNING&quot;,</span></span><br><span class="line"><span class="string">    ...     task_type=&quot;SEQ_2_SEQ_LM&quot;,</span></span><br><span class="line"><span class="string">    ...     num_virtual_tokens=20,</span></span><br><span class="line"><span class="string">    ...     token_dim=768,</span></span><br><span class="line"><span class="string">    ...     num_transformer_submodules=1,</span></span><br><span class="line"><span class="string">    ...     num_attention_heads=12,</span></span><br><span class="line"><span class="string">    ...     num_layers=12,</span></span><br><span class="line"><span class="string">    ...     encoder_reparameterization_type=&quot;MLP&quot;,</span></span><br><span class="line"><span class="string">    ...     encoder_hidden_size=768,</span></span><br><span class="line"><span class="string">    ... )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; prompt_encoder = PromptEncoder(config)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    **Attributes**:</span></span><br><span class="line"><span class="string">        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **mlp_head** (`torch.nn.Sequential`) -- The MLP head of the prompt encoder if `inference_mode=False`.</span></span><br><span class="line"><span class="string">        - **lstm_head** (`torch.nn.LSTM`) -- The LSTM head of the prompt encoder if `inference_mode=False` and</span></span><br><span class="line"><span class="string">        `encoder_reparameterization_type=&quot;LSTM&quot;`.</span></span><br><span class="line"><span class="string">        - **token_dim** (`int`) -- The hidden embedding dimension of the base transformer model.</span></span><br><span class="line"><span class="string">        - **input_size** (`int`) -- The input size of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **output_size** (`int`) -- The output size of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **hidden_size** (`int`) -- The hidden size of the prompt encoder.</span></span><br><span class="line"><span class="string">        - **total_virtual_tokens** (`int`): The total number of virtual tokens of the</span></span><br><span class="line"><span class="string">        prompt encoder.</span></span><br><span class="line"><span class="string">        - **encoder_type** (Union[[`PromptEncoderReparameterizationType`], `str`]): The encoder type of the prompt</span></span><br><span class="line"><span class="string">          encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input shape: (`batch_size`, `total_virtual_tokens`)</span></span><br><span class="line"><span class="string">    Output shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.token_dim = config.token_dim</span><br><span class="line">        self.input_size = self.token_dim</span><br><span class="line">        self.output_size = self.token_dim</span><br><span class="line">        self.hidden_size = config.encoder_hidden_size</span><br><span class="line">        self.total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules</span><br><span class="line">        self.encoder_type = config.encoder_reparameterization_type</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding</span></span><br><span class="line">        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;先判断是不是推理模式&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> config.inference_mode:   </span><br><span class="line">            <span class="keyword">if</span> self.encoder_type == PromptEncoderReparameterizationType.LSTM:</span><br><span class="line">                lstm_dropout = config.encoder_dropout</span><br><span class="line">                num_layers = config.encoder_num_layers</span><br><span class="line">                <span class="comment"># LSTM</span></span><br><span class="line">                self.lstm_head = torch.nn.LSTM(</span><br><span class="line">                    input_size=self.input_size,</span><br><span class="line">                    hidden_size=self.hidden_size,</span><br><span class="line">                    num_layers=num_layers,</span><br><span class="line">                    dropout=lstm_dropout,</span><br><span class="line">                    bidirectional=<span class="literal">True</span>,</span><br><span class="line">                    batch_first=<span class="literal">True</span>,</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                self.mlp_head = torch.nn.Sequential(</span><br><span class="line">                    torch.nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size * <span class="number">2</span>),</span><br><span class="line">                    torch.nn.ReLU(),</span><br><span class="line">                    torch.nn.Linear(self.hidden_size * <span class="number">2</span>, self.output_size),</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> self.encoder_type == PromptEncoderReparameterizationType.MLP:</span><br><span class="line">                encoder_num_layers_default = PromptEncoderConfig.encoder_num_layers</span><br><span class="line">                <span class="keyword">if</span> config.encoder_num_layers != encoder_num_layers_default:</span><br><span class="line">                    warnings.warn(</span><br><span class="line">                        <span class="string">f&quot;for <span class="subst">&#123;self.encoder_type.value&#125;</span>, the argument `encoder_num_layers` is ignored. &quot;</span></span><br><span class="line">                        <span class="string">f&quot;Exactly <span class="subst">&#123;encoder_num_layers_default&#125;</span> MLP layers are used.&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                layers = [</span><br><span class="line">                    torch.nn.Linear(self.input_size, self.hidden_size),</span><br><span class="line">                    torch.nn.ReLU(),</span><br><span class="line">                    torch.nn.Linear(self.hidden_size, self.hidden_size),</span><br><span class="line">                    torch.nn.ReLU(),</span><br><span class="line">                    torch.nn.Linear(self.hidden_size, self.output_size),</span><br><span class="line">                ]</span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;将列表解包为单独的参数传入&#x27;&#x27;&#x27;</span></span><br><span class="line">                self.mlp_head = torch.nn.Sequential(*layers) </span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, indices</span>):</span><br><span class="line">        input_embeds = self.embedding(indices)</span><br><span class="line">        <span class="keyword">if</span> self.encoder_type == PromptEncoderReparameterizationType.LSTM:</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            这里的在经过lstm_head后返回的是一个元组，会包含两个元素：第一个元素是所有时间步的输出（output），第二个元素是最后一个时间步的隐藏状态和细胞状态（hidden）</span></span><br><span class="line"><span class="string">            如果这里使用的是双向lstm，最后一维是768*2，如果是单向lstm，最后一维的大小就是768</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            output_embeds = self.mlp_head(self.lstm_head(input_embeds)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">elif</span> self.encoder_type == PromptEncoderReparameterizationType.MLP:</span><br><span class="line">            output_embeds = self.mlp_head(input_embeds)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_embeds</span><br></pre></td></tr></table></figure>
<!-- ```python
import numpy as np
from bert4keras.backend import keras, K
from bert4keras.layers import Loss, Embedding
from bert4keras.tokenizers import Tokenizer
from bert4keras.models import build_transformer_model, BERT
from bert4keras.optimizers import Adam
from bert4keras.snippets import sequence_padding, DataGenerator
from bert4keras.snippets import open
from keras.layers import Lambda, Dense

maxlen = 128
batch_size = 32
config_path = '/root/kg/bert/chinese_nezha_gpt_L-12_H-768_A-12/config.json'
checkpoint_path = '/root/kg/bert/chinese_nezha_gpt_L-12_H-768_A-12/gpt.ckpt'
dict_path = '/root/kg/bert/chinese_nezha_gpt_L-12_H-768_A-12/vocab.txt'

def load_data(filename):
    D = []
    with open(filename, encoding='utf-8') as f:
        for l in f:
            '''l.strip()去除行首尾的空白字符（如空格、换行符等），split('\t')根据制表符\t将行分割成两部分，进行数值计算'''
            text, label = l.strip().split('\t')
            D.append((text, int(label)))
    return D

train_data = load_data('datasets/sentiment/sentiment.train.data')
valid_data = load_data('datasets/sentiment/sentiment.valid.data')
test_data = load_data('datasets/sentiment/sentiment.test.data')

'''模拟标注和非标注数据，可以模拟一个数据稀缺的场景'''
train_frac = 0.01  # 标注数据的比例
num_labeled = int(len(train_data) * train_frac)
unlabeled_data = [(t, 2) for t, l in train_data[num_labeled:]]
train_data = train_data[:num_labeled]
# train_data = train_data + unlabeled_data

'''应用分词器，转换为小写形式'''
tokenizer = Tokenizer(dict_path, do_lower_case=True)

# 对应的任务描述
'''pos_id表示正面情感，neg_id表示负面情感'''
desc = ['[unused%s]' % i for i in range(1, 9)]
desc_ids = [tokenizer.token_to_id(t) for t in desc]
pos_id = tokenizer.token_to_id(u'很')
neg_id = tokenizer.token_to_id(u'不')
``` -->
<!-- ```python
class data_generator(DataGenerator):
    """数据生成器，注意这里的token_ids组合方式"""
    def __iter__(self, random=False):
        batch_token_ids = []
        for is_end, (text, label) in self.sample(random):
            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)
            token_ids = token_ids[:1] + desc_ids[:4] + token_ids[:-1]
            token_ids = token_ids + desc_ids[4:]
            if label == 0:
                token_ids = token_ids + [neg_id]
            elif label == 1:
                token_ids = token_ids + [pos_id]
            batch_token_ids.append(token_ids)
            if len(batch_token_ids) == self.batch_size or is_end:
                batch_token_ids = sequence_padding(batch_token_ids)
                '''yield是生成器的使用方法，注意yield完要清空列表'''
                yield batch_token_ids, None
                batch_token_ids = []

class CrossEntropy(Loss):
    """交叉熵作为loss，并mask掉padding部分"""
    def compute_loss(self, inputs, mask=None):
        '''将输入的元组拆分为真实标签y_true和预测标签y_pred，这里在inputs为元组或者列表的时候且长度相同的时候可以调用'''
        y_true, y_pred = inputs
        if mask[1] is None:
            y_mask = 1.0
        '''cast函数将整数mask[1]转换为浮点数 -- eg: y = K.cast(x, K.floatx())'''
        else:
            y_mask = K.cast(mask[1], K.floatx())[:, 1:]
        y_true = y_true[:, 1:]  # 目标token_ids
        y_pred = y_pred[:, :-1]  # 预测序列，错开一位
        accuracy = keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
        accuracy = K.sum(accuracy * y_mask) / K.sum(y_mask)
        self.add_metric(accuracy, name='accuracy')
        loss = K.sparse_categorical_crossentropy(y_true, y_pred)
        loss = K.sum(loss * y_mask) / K.sum(y_mask)
        return loss

class PtuningEmbedding(Embedding):
    """新定义Embedding层，只优化部分Token"""
    def call(self, inputs, mode='embedding'):
        embeddings = self.embeddings
        embeddings_sg = K.stop_gradient(embeddings)
        mask = np.zeros((K.int_shape(embeddings)[0], 1))
        mask[1:9] += 1  # 只优化id为1～8的token
        '''非常巧妙的只计算前8个token的梯度值'''
        self.embeddings = embeddings * mask + embeddings_sg * (1 - mask)
        outputs = super(PtuningEmbedding, self).call(inputs, mode)
        self.embeddings = embeddings
        return outputs

class PtuningBERT(BERT):
    """替换原来的Embedding"""
    def apply(self, inputs=None, layer=None, arguments=None, **kwargs):
        if layer is Embedding:
            layer = PtuningEmbedding
        return super(PtuningBERT,
                     self).apply(inputs, layer, arguments, **kwargs)
``` -->
<!-- ```python

model = build_transformer_model(
    config_path=config_path,
    checkpoint_path=checkpoint_path,
    model=PtuningBERT,
    segment_vocab_size=0,  # 去掉segmeng_ids输入
    application='lm',
)  # 建立模型，加载权重

for layer in model.layers:
    if layer.name != 'Embedding-Token':
        layer.trainable = False

output = CrossEntropy(1)([model.input, model.output])

model = keras.models.Model(model.input, output)
model.compile(optimizer=Adam(6e-4))
model.summary()

# 转换数据集
train_generator = data_generator(train_data, batch_size)
valid_generator = data_generator(valid_data, batch_size)
test_generator = data_generator(test_data, batch_size)

class Evaluator(keras.callbacks.Callback):
    def __init__(self):
        self.best_val_acc = 0.

    def on_epoch_end(self, epoch, logs=None):
        val_acc = evaluate(valid_generator)
        if val_acc > self.best_val_acc:
            self.best_val_acc = val_acc
            model.save_weights('best_model_gpt.weights')
        test_acc = evaluate(test_generator)
        print(
            u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\n' %
            (val_acc, self.best_val_acc, test_acc)
        )

def evaluate(data):
    total, right = 0., 0.
    for x_true, _ in data:
        y_pred = model.predict(x_true)
        for x, y in zip(x_true, y_pred):
            x = np.trim_zeros(x)
            y = y[:len(x)][-2, [neg_id, pos_id]].argmax()
            y = [neg_id, pos_id][y]
            if y == x[-1]:
                right += 1
            total += 1
    return right / total

if __name__ == '__main__':

    evaluator = Evaluator()
    model.fit_generator(
        train_generator.forfit(),
        steps_per_epoch=len(train_generator) * 50,
        epochs=1000,
        callbacks=[evaluator]
    )
else:
    model.load_weights('best_model_gpt.weights')
``` -->

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-P-Tuning%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-Prompt-Tuning微调技术" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-Prompt-Tuning%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/">大模型初步-Prompt Tuning微调技术</a>
    </h1>
  

        
        <a href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-Prompt-Tuning%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/" class="archive-article-date">
  	<time datetime="2024-05-11T02:06:37.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-11</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这里开始学习基于hugging face的peft包构成的prompt-tuning微调技术，这里结合代码进行解释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> default_data_collator, get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;指定设备，模型和分词器路径&#x27;&#x27;&#x27;</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line">model_name_or_path = <span class="string">&quot;bigscience/bloomz&quot;</span></span><br><span class="line">tokenizer_name_or_path = <span class="string">&quot;bigscience/bloomz&quot;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">PromptTuningInit是peft库中的一个枚举，用于指定提示调整的初始化方法</span></span><br><span class="line"><span class="string">TaskType是peft库中的一个枚举，用于指定任务的类型。这有助于库理解模型应该用于哪种类型的任务，以便正确地应用提示调整</span></span><br><span class="line"><span class="string">TaskType.CAUSAL_LM：因果语言模型任务，适用于生成文本的任务，如自动完成、文本续写等。</span></span><br><span class="line"><span class="string">TaskType.SEQ_2_SEQ_LM：序列到序列语言模型任务，适用于翻译、摘要等生成任务。</span></span><br><span class="line"><span class="string">TaskType.SEQ_CLS：序列分类任务，适用于文本分类、情感分析等任务。</span></span><br><span class="line"><span class="string">TaskType.MASKED_LM：掩码语言模型任务，适用于预训练阶段的语言模型，如BERT</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">peft_config = PromptTuningConfig(</span><br><span class="line">    task_type=TaskType.CAUSAL_LM,</span><br><span class="line">    prompt_tuning_init=PromptTuningInit.TEXT,</span><br><span class="line">    num_virtual_tokens=<span class="number">8</span>,</span><br><span class="line">    prompt_tuning_init_text=<span class="string">&quot;Classify if the tweet is a complaint or not:&quot;</span>,</span><br><span class="line">    tokenizer_name_or_path=model_name_or_path,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">dataset_name = <span class="string">&quot;twitter_complaints&quot;</span></span><br><span class="line">text_column = <span class="string">&quot;Tweet text&quot;</span></span><br><span class="line">label_column = <span class="string">&quot;text_label&quot;</span></span><br><span class="line">max_length = <span class="number">64</span></span><br><span class="line">lr = <span class="number">3e-2</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">load_dataset函数的用法如下：</span></span><br><span class="line"><span class="string">load_dataset(</span></span><br><span class="line"><span class="string">    path: Optional[str] = None,</span></span><br><span class="line"><span class="string">    name: Optional[str] = None,</span></span><br><span class="line"><span class="string">    data_dir: Optional[str] = None,</span></span><br><span class="line"><span class="string">    data_files: Optional[Union[str, List[str], Dict[str, str]]] = None,</span></span><br><span class="line"><span class="string">    split: Optional[Union[str, List[str], Split]] = None,</span></span><br><span class="line"><span class="string">    cache_dir: Optional[str] = None,</span></span><br><span class="line"><span class="string">    features: Optional[Features] = None,</span></span><br><span class="line"><span class="string">    download_mode: Optional[DownloadMode] = None,</span></span><br><span class="line"><span class="string">    ignore_verifications: Optional[bool] = None,</span></span><br><span class="line"><span class="string">    keep_in_memory: Optional[bool] = None,</span></span><br><span class="line"><span class="string">    save_infos: Optional[bool] = None,</span></span><br><span class="line"><span class="string">    script_version: Optional[str] = None,</span></span><br><span class="line"><span class="string">    use_auth_token: Optional[Union[bool, str]] = None,</span></span><br><span class="line"><span class="string">    **config_kwargs,</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">path可以设置为数据集的路径，在例子中设置的是python脚本路径，以定义数据集的加载逻辑</span></span><br><span class="line"><span class="string">name是数据集的子集名称，例子中是&quot;twitter_complaints&quot;</span></span><br><span class="line"><span class="string">cache_dir是一个字符串，表示缓存数据集的目录。如果指定了这个参数，加载的数据集将被缓存在这个目录下，以便于下次快速加载</span></span><br><span class="line"><span class="string">最后dataset加载出来一个字典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;/home/guodong.li/data/peft/raft/raft.py&quot;</span>, dataset_name, cache_dir=<span class="string">&quot;/home/guodong.li/data/peft/data&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这里打印标签列表&#x27;&#x27;&#x27;</span></span><br><span class="line">classes = [k.replace(<span class="string">&quot;_&quot;</span>, <span class="string">&quot; &quot;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> dataset[<span class="string">&quot;train&quot;</span>].features[<span class="string">&quot;Label&quot;</span>].names]</span><br><span class="line"><span class="built_in">print</span>(classes)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这里lambda是一个匿名函数，接受一个x为输入并返回一个修改后的样本，实现的是将数值标签转换为文本标签的功能&#x27;&#x27;&#x27;</span></span><br><span class="line">dataset = dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;text_label&quot;</span>: [classes[label] <span class="keyword">for</span> label <span class="keyword">in</span> x[<span class="string">&quot;Label&quot;</span>]]&#125;,</span><br><span class="line">    batched=<span class="literal">True</span>,  <span class="comment"># 批次处理</span></span><br><span class="line">    num_proc=<span class="number">1</span>,  <span class="comment"># 使用的进程数</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line">dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data preprocessing</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;如果分词器没有定义pad_token_id，则使用eos_token_id作为填充标记&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">if</span> tokenizer.pad_token_id <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    tokenizer.pad_token_id = tokenizer.eos_token_id  </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;计算所有类别标签的最大长度，以便后续对输入进行填充&#x27;&#x27;&#x27;</span></span><br><span class="line">target_max_length = <span class="built_in">max</span>([<span class="built_in">len</span>(tokenizer(class_label)[<span class="string">&quot;input_ids&quot;</span>]) <span class="keyword">for</span> class_label <span class="keyword">in</span> classes])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;target_max_length:&quot;</span>, target_max_length)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;其实这个函数直接用hugging face中的代码统一批处理就可以了，这里只是展现一下具体的实现，可以跳过不用看&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;批次处理大小&#x27;&#x27;&#x27;</span></span><br><span class="line">    batch_size = <span class="built_in">len</span>(examples[text_column])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch_size:&quot;</span>, batch_size)</span><br><span class="line">    <span class="comment"># 这里的写法存疑？？？text_column不是变量吧---&gt;没有错误 这里text_column在前文声明是一个变量</span></span><br><span class="line">    inputs = [<span class="string">f&quot;<span class="subst">&#123;text_column&#125;</span> : <span class="subst">&#123;x&#125;</span> Label : &quot;</span> <span class="keyword">for</span> x <span class="keyword">in</span> examples[text_column]]  </span><br><span class="line">    targets = [<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> examples[label_column]]</span><br><span class="line">    model_inputs = tokenizer(inputs)</span><br><span class="line">    labels = tokenizer(targets)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    这里tokenzier后的字典有几个键分别为：&#x27;input_ids&#x27;，&#x27;token_type_ids&#x27;，&#x27;attention_mask&#x27;，注意这里(batch_size,sequence_length)是input_ids...下面的张量</span></span><br><span class="line"><span class="string">    注意这里加上pad_token_id是因为最开始的tokenizer没有加上eos等标签，如果用bert-case-based会加上这样的标签，这在之前计算target_max_length就体现出来差别，</span></span><br><span class="line"><span class="string">    本来单词被分成4个token，然后前后都被加了一个token(分别为bos和eos)，变成6个</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        sample_input_ids = model_inputs[<span class="string">&quot;input_ids&quot;</span>][i]</span><br><span class="line">        label_input_ids = labels[<span class="string">&quot;input_ids&quot;</span>][i] + [tokenizer.pad_token_id]  </span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(i, sample_input_ids, label_input_ids)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;这里[-10]*10还是一维列表，只是里面有10个-10&#x27;&#x27;&#x27;</span></span><br><span class="line">        model_inputs[<span class="string">&quot;input_ids&quot;</span>][i] = sample_input_ids + label_input_ids</span><br><span class="line">        labels[<span class="string">&quot;input_ids&quot;</span>][i] = [-<span class="number">100</span>] * <span class="built_in">len</span>(sample_input_ids) + label_input_ids</span><br><span class="line">        model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i] = [<span class="number">1</span>] * <span class="built_in">len</span>(model_inputs[<span class="string">&quot;input_ids&quot;</span>][i])</span><br><span class="line">    <span class="comment"># 疑问：这里对token_type_ids的长度不进行变化吗？这样是不是不够严谨？</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        sample_input_ids = model_inputs[<span class="string">&quot;input_ids&quot;</span>][i]</span><br><span class="line">        label_input_ids = labels[<span class="string">&quot;input_ids&quot;</span>][i]</span><br><span class="line">        </span><br><span class="line">        model_inputs[<span class="string">&quot;input_ids&quot;</span>][i] = [tokenizer.pad_token_id] * (max_length - <span class="built_in">len</span>(sample_input_ids)) + sample_input_ids</span><br><span class="line">        model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i] = [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(sample_input_ids)) + model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i]</span><br><span class="line">        labels[<span class="string">&quot;input_ids&quot;</span>][i] = [-<span class="number">100</span>] * (max_length - <span class="built_in">len</span>(sample_input_ids)) + label_input_ids</span><br><span class="line"></span><br><span class="line">        model_inputs[<span class="string">&quot;input_ids&quot;</span>][i] = torch.tensor(model_inputs[<span class="string">&quot;input_ids&quot;</span>][i][:max_length])</span><br><span class="line">        model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i] = torch.tensor(model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i][:max_length])</span><br><span class="line">        labels[<span class="string">&quot;input_ids&quot;</span>][i] = torch.tensor(labels[<span class="string">&quot;input_ids&quot;</span>][i][:max_length])</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model_inputs input_ids:&quot;</span>, model_inputs[<span class="string">&quot;input_ids&quot;</span>][i])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model_inputs attention_mask:&quot;</span>, model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;labels input_ids:&quot;</span>, labels[<span class="string">&quot;input_ids&quot;</span>][i])</span><br><span class="line"></span><br><span class="line">    model_inputs[<span class="string">&quot;labels&quot;</span>] = labels[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> model_inputs</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;column_names:&quot;</span>, dataset[<span class="string">&quot;train&quot;</span>].column_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始的训练和测试数据同时预处理，然后作为训练和评估数据集</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">指定在处理后应该从数据集中移除的列,这里选择了dataset[&quot;train&quot;]中的所有列</span></span><br><span class="line"><span class="string">load_from_cache_file=False: 指示map函数不要从缓存文件加载数据。这通常用于调试或当您知道数据集没有缓存时。</span></span><br><span class="line"><span class="string">desc=&quot;Running tokenizer on dataset&quot;: 为map函数提供一个描述性字符串。这有助于在处理过程中提供反馈。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">processed_datasets = dataset.<span class="built_in">map</span>(</span><br><span class="line">    preprocess_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    num_proc=<span class="number">1</span>,</span><br><span class="line">    remove_columns=dataset[<span class="string">&quot;train&quot;</span>].column_names,</span><br><span class="line">    load_from_cache_file=<span class="literal">False</span>,</span><br><span class="line">    desc=<span class="string">&quot;Running tokenizer on dataset&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataset = processed_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">eval_dataset = processed_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练与评估使用同一份数据，但是训练数据打乱</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=<span class="literal">True</span>)</span><br><span class="line">eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(eval_dataloader))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    batch_size = <span class="built_in">len</span>(examples[text_column])</span><br><span class="line">    inputs = [<span class="string">f&quot;<span class="subst">&#123;text_column&#125;</span> : <span class="subst">&#123;x&#125;</span> Label : &quot;</span> <span class="keyword">for</span> x <span class="keyword">in</span> examples[text_column]]</span><br><span class="line">    model_inputs = tokenizer(inputs)</span><br><span class="line">    <span class="comment"># print(model_inputs)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        sample_input_ids = model_inputs[<span class="string">&quot;input_ids&quot;</span>][i]</span><br><span class="line">        </span><br><span class="line">        model_inputs[<span class="string">&quot;input_ids&quot;</span>][i] = [tokenizer.pad_token_id] * ( max_length - <span class="built_in">len</span>(sample_input_ids)) + sample_input_ids</span><br><span class="line">        model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i] = [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(sample_input_ids)) + model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i]</span><br><span class="line">        </span><br><span class="line">        model_inputs[<span class="string">&quot;input_ids&quot;</span>][i] = torch.tensor(model_inputs[<span class="string">&quot;input_ids&quot;</span>][i][:max_length])</span><br><span class="line">        model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i] = torch.tensor(model_inputs[<span class="string">&quot;attention_mask&quot;</span>][i][:max_length])</span><br><span class="line">    <span class="keyword">return</span> model_inputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始的测试数据用于测试</span></span><br><span class="line">test_dataset = dataset[<span class="string">&quot;test&quot;</span>].<span class="built_in">map</span>(</span><br><span class="line">    test_preprocess_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    num_proc=<span class="number">1</span>,</span><br><span class="line">    remove_columns=dataset[<span class="string">&quot;train&quot;</span>].column_names,</span><br><span class="line">    load_from_cache_file=<span class="literal">False</span>,</span><br><span class="line">    desc=<span class="string">&quot;Running tokenizer on dataset&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">指定一个数据批处理函数collate function。default_data_collator是PyTorch中用于处理数据集的一种默认批处理方法。它能够处理多种类型的数据，并将它们组合成模型可以接受的格式</span></span><br><span class="line"><span class="string">pin_memory=True: 指示DataLoader在内存中预先分配空间，并将输入数据复制到这个空间中。这可以提高数据传输到GPU的速度，尤其是在使用大型数据集时。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(test_dataloader))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;根据上文这里的peft_config用的是finetuning的方法，在操作完前后model会发生变化&#x27;&#x27;&#x27;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_name_or_path)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">model = get_peft_model(model, peft_config)  </span><br><span class="line">model.print_trainable_parameters()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;这部分代码在解析transformers用法的文章中已经出现&#x27;&#x27;&#x27;</span></span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=lr)</span><br><span class="line">lr_scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=(<span class="built_in">len</span>(train_dataloader) * num_epochs),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader)):</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;而不是直接使用 loss 是为了确保 total_loss 中的损失值与计算图（graph）不相关联，这里用.访问loss的属性&#x27;&#x27;&#x27;</span></span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        total_loss += loss.detach().<span class="built_in">float</span>()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    eval_loss = <span class="number">0</span></span><br><span class="line">    eval_preds = []</span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(eval_dataloader)):</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        eval_loss += loss.detach().<span class="built_in">float</span>()</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        这里torch.argmax第二个参数表示沿着最后一个轴找最大值，将张量移动到cpu并转换为numpy</span></span><br><span class="line"><span class="string">        batch_decode函数解码操作，并且跳过序列中的特殊令牌</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        eval_preds.extend(</span><br><span class="line">            tokenizer.batch_decode(torch.argmax(outputs.logits, -<span class="number">1</span>).detach().cpu().numpy(), skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    eval_epoch_loss = eval_loss / <span class="built_in">len</span>(eval_dataloader)</span><br><span class="line">    eval_ppl = torch.exp(eval_epoch_loss)</span><br><span class="line">    train_epoch_loss = total_loss / <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">    train_ppl = torch.exp(train_epoch_loss)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;这里的print的打印结果就是epoch=epoch变量，这样会比较简洁方便&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;epoch=&#125;</span>: <span class="subst">&#123;train_ppl=&#125;</span> <span class="subst">&#123;train_epoch_loss=&#125;</span> <span class="subst">&#123;eval_ppl=&#125;</span> <span class="subst">&#123;eval_epoch_loss=&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;更具体的模型评估&#x27;&#x27;&#x27;</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">i = <span class="number">33</span></span><br><span class="line">inputs = tokenizer(<span class="string">f&#x27;<span class="subst">&#123;text_column&#125;</span> : <span class="subst">&#123;dataset[<span class="string">&quot;test&quot;</span>][i][<span class="string">&quot;Tweet text&quot;</span>]&#125;</span> Label : &#x27;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;test&quot;</span>][i][<span class="string">&quot;Tweet text&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> inputs.items()&#125;</span><br><span class="line">    outputs = model.generate(</span><br><span class="line">        input_ids=inputs[<span class="string">&quot;input_ids&quot;</span>], attention_mask=inputs[<span class="string">&quot;attention_mask&quot;</span>], max_new_tokens=<span class="number">10</span>, eos_token_id=<span class="number">3</span></span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(outputs)</span><br><span class="line">    <span class="built_in">print</span>(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;保存模型&#x27;&#x27;&#x27;</span></span><br><span class="line">peft_model_id = <span class="string">f&quot;<span class="subst">&#123;model_name_or_path&#125;</span>_<span class="subst">&#123;peft_config.peft_type&#125;</span>_<span class="subst">&#123;peft_config.task_type&#125;</span>&quot;</span></span><br><span class="line">model.save_pretrained(peft_model_id)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">定义了一个字符串变量 ckpt，其值是 peft_model_id 加上 /adapter_model.bin。这通常用于指定一个文件路径</span></span><br><span class="line"><span class="string">du -h 是Unix shell中的一个命令，用于显示文件或目录的磁盘使用情况。-h 选项表示以易读的格式（human-readable）显示大小</span></span><br><span class="line"><span class="string">tree 是一个Unix shell命令，用于以树状图的形式显示目录结构。-h 选项表示以易读的格式显示大小</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">ckpt = <span class="string">f&quot;<span class="subst">&#123;peft_model_id&#125;</span>/adapter_model.bin&quot;</span></span><br><span class="line">!du -h $ckpt</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--------------&quot;</span>)</span><br><span class="line">!tree -h $peft_model_id</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于高效微调好的模型进行预训练。</span></span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel, PeftConfig</span><br><span class="line">peft_model_id = <span class="string">f&quot;<span class="subst">&#123;model_name_or_path&#125;</span>_<span class="subst">&#123;peft_config.peft_type&#125;</span>_<span class="subst">&#123;peft_config.task_type&#125;</span>&quot;</span></span><br><span class="line">config = PeftConfig.from_pretrained(peft_model_id)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model path:&quot;</span>, config.base_model_name_or_path)</span><br><span class="line"><span class="comment"># 加载基础模型</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)</span><br><span class="line"><span class="comment"># 加载LoRA模型</span></span><br><span class="line">model = PeftModel.from_pretrained(model, peft_model_id)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">i = <span class="number">4</span></span><br><span class="line">inputs = tokenizer(<span class="string">f&#x27;<span class="subst">&#123;text_column&#125;</span> : <span class="subst">&#123;dataset[<span class="string">&quot;test&quot;</span>][i][<span class="string">&quot;Tweet text&quot;</span>]&#125;</span> Label : &#x27;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;test&quot;</span>][i][<span class="string">&quot;Tweet text&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> inputs.items()&#125;</span><br><span class="line">    outputs = model.generate(</span><br><span class="line">        input_ids=inputs[<span class="string">&quot;input_ids&quot;</span>], attention_mask=inputs[<span class="string">&quot;attention_mask&quot;</span>], max_new_tokens=<span class="number">10</span>, eos_token_id=<span class="number">3</span></span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(outputs)</span><br><span class="line">    <span class="built_in">print</span>(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-Prompt-Tuning%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-llama2代码解读2" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/">大模型初步-llama2代码解读2</a>
    </h1>
  

        
        <a href="/2024/05/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/" class="archive-article-date">
  	<time datetime="2024-05-10T15:20:52.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-10</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇文章只有对于model.py的解读，这篇文章则是关于数据处理的代码片段</p>
<h3 id="dataset-py"><a href="#dataset-py" class="headerlink" title="dataset.py"></a>dataset.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PretrainDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path_lst, max_length=<span class="number">256</span>, memmap=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;先判断是否使用内存映射的方式加载数据&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> memmap:</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;打开列表的第一个文件，这里假设文件中的数据是以uint16储存的&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(data_path_lst[<span class="number">0</span>], <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                seek函数</span></span><br><span class="line"><span class="string">                offset：表示移动的字符数。如果是正数，表示向文件末尾方向移动；如果是负数，表示向文件开头方向移动。</span></span><br><span class="line"><span class="string">                whence：可选参数，用于指定offset的参考位置。它有三种可能的值：</span></span><br><span class="line"><span class="string">                0：表示从文件开头计算偏移量（默认值）。</span></span><br><span class="line"><span class="string">                1：表示从当前文件位置计算偏移量。</span></span><br><span class="line"><span class="string">                2：表示从文件末尾计算偏移量。</span></span><br><span class="line"><span class="string">                &#x27;&#x27;&#x27;</span></span><br><span class="line">                nbytes = f.seek(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">                flen = f.tell() // np.dtype(<span class="string">&#x27;uint16&#x27;</span>).itemsize  <span class="comment"># 返回uint16类型占用的字节数，这里是2字节</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;np.memmap创建一个内存映射数组，它允许你处理比物理内存更大的数据集，后面的两个参数不要漏&#x27;&#x27;&#x27;</span></span><br><span class="line">            self.data = np.memmap(data_path_lst[<span class="number">0</span>], dtype=np.dtype(<span class="string">&#x27;uint16&#x27;</span>), shape=(flen//max_length, max_length))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data_lst = []</span><br><span class="line">            <span class="keyword">for</span> data_path <span class="keyword">in</span> data_path_lst:</span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;使用二进制读取，下面self.data读取出来的结果是一个uint16的矩阵&#x27;&#x27;&#x27;</span></span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(data_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    data = np.fromfile(f, dtype=np.uint16)  <span class="comment"># np.fromfile(f, dtype=np.uint16)从文件中读取数据，并将其转换为uint16类型的NumPy数组</span></span><br><span class="line">                    data_lst.append(data)</span><br><span class="line">            data = np.concatenate(data_lst)</span><br><span class="line">            data = data[:max_length*<span class="built_in">int</span>(<span class="built_in">len</span>(data)/max_length)]  <span class="comment"># 确保数据可以均匀地被划分成多个长度为max_length的样本</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;以上读取和拼接的时候都是一维吧&#x27;&#x27;&#x27;</span></span><br><span class="line">            self.data = data.reshape(-<span class="number">1</span>, max_length)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;memmap:&#123;&#125; train data.shape:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(memmap, self.data.shape))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;downloading finished.....&quot;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;一般__len__用来返回样本数量&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;对一个样本构造输入和输出样本，第一个词预测第二个词，前两个词预测第三个词...&#x27;&#x27;&#x27;</span></span><br><span class="line">        sample = self.data[index]</span><br><span class="line">        X = np.array(sample[:-<span class="number">1</span>]).astype(np.int64)</span><br><span class="line">        Y = np.array(sample[<span class="number">1</span>:]).astype(np.int64)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.from_numpy(X), torch.from_numpy(Y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>学习测试的代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">text_to_write = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">    text_to_write += <span class="string">f&#x27;<span class="subst">&#123;i%<span class="number">10</span>&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line">file_path1 = <span class="string">&#x27;./dataset_1.txt&#x27;</span></span><br><span class="line">file_path2 = <span class="string">&#x27;./dataset_2.txt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path1, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> file1, <span class="built_in">open</span>(file_path2, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> file2:</span><br><span class="line">    <span class="comment"># 写入文本内容到文件</span></span><br><span class="line">    file1.write(text_to_write)</span><br><span class="line">    file2.write(text_to_write)</span><br><span class="line"></span><br><span class="line">max_length = <span class="number">64</span></span><br><span class="line">data_list = []</span><br><span class="line">data_path_list = [<span class="string">&#x27;./dataset_1.txt&#x27;</span>, <span class="string">&#x27;./dataset_2.txt&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> data_path <span class="keyword">in</span> data_path_list:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_path, <span class="string">&#x27;rb&#x27;</span>):</span><br><span class="line">        data = np.fromfile(data_path, dtype=np.uint8)</span><br><span class="line">        data_list.append(data)</span><br><span class="line">data = np.concatenate(data_list)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(data))</span><br><span class="line">data = data[:max_length*<span class="built_in">int</span>(<span class="built_in">len</span>(data)/max_length)]</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line">data = data.reshape(-<span class="number">1</span>, max_length)</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">我这里给两个文件都写入了256个字符，但是根据fromfile的dtype设置，实际情况会有不同，如果是uint8就不会连接读取，但是如果是uint16,32,64，就会把多个数字拼接起来读取到data里面，这里需要注意一下</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-llama2代码解读1" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/">大模型初步-llama2代码解读1</a>
    </h1>
  

        
        <a href="/2024/05/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/" class="archive-article-date">
  	<time datetime="2024-05-10T13:29:33.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-05-10</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我们在<a target="_blank" rel="noopener" href="https://kevin236-max.github.io/2024/03/19/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2/">之前的文章</a>中总结了llama2涉及到的技术，作为初学者在看完方法以后依然无法实现这些想法，所以我们这里从代码入手，详细解析这些代码涉及到的语法点。</p>
<h3 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArgs</span>:</span><br><span class="line">    dim: <span class="built_in">int</span> = <span class="number">4096</span></span><br><span class="line">    n_layers: <span class="built_in">int</span> = <span class="number">32</span></span><br><span class="line">    n_heads: <span class="built_in">int</span> = <span class="number">32</span></span><br><span class="line">    n_kv_heads: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span><br><span class="line">    vocab_size: <span class="built_in">int</span> = -<span class="number">1</span>  <span class="comment"># defined later by tokenizer</span></span><br><span class="line">    multiple_of: <span class="built_in">int</span> = <span class="number">256</span>  <span class="comment"># make SwiGLU hidden layer size multiple of large power of 2</span></span><br><span class="line">    norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span></span><br><span class="line">    max_seq_len: <span class="built_in">int</span> = <span class="number">2048</span></span><br><span class="line">    dropout: <span class="built_in">float</span> = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;对输入的量进行类型限定很重要&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, eps: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        创建可学习的参数weight，当模型进行训练时，nn.Parameter 对象会被自动优化和更新</span></span><br><span class="line"><span class="string">        初始化所有元素为 1，这个张量被用作 self.weight 的初始值</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.weight = nn.Parameter(torch.ones(dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_norm</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        假设输入张量x的形状为(batch_size, sequence_length, feature_dim)，最后根号内生成的是一个三维张量，形状为(batch_size, sequence_length, 1)，</span></span><br><span class="line"><span class="string">        这里是对每一个feature赋一个权重</span></span><br><span class="line"><span class="string">        这里keepdim=True的操作是使得矩阵在维度等于1的时候不压缩</span></span><br><span class="line"><span class="string">        然后x * torch.rsqrt()的时候会使得三维张量执行广播运算，第三维的1复制feature_dim次变成(batch_size, sequence_length, feature_dim)，然后就可以和x相乘了</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + self.eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;调用_norm方法对输入x进行归一化处理，并将结果转换为和输入x相同的数据类型，这是很重要的做法&#x27;&#x27;&#x27;</span></span><br><span class="line">        output = self._norm(x.<span class="built_in">float</span>()).type_as(x)</span><br><span class="line">        <span class="keyword">return</span> output * self.weight</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    补充知识点：可以执行广播的条件</span></span><br><span class="line"><span class="string">    每个张量至少有一个维度。</span></span><br><span class="line"><span class="string">    从最后一位开始比较两个张量的维度大小：</span></span><br><span class="line"><span class="string">    1.如果维度大小相同，或者其中一个张量在该维度上的大小为 1，则它们是兼容的</span></span><br><span class="line"><span class="string">    2.如果维度大小不同，且没有一个张量在该维度上的大小为 1，则它们不兼容，无法进行广播</span></span><br><span class="line"><span class="string">    A = torch.randn(2, 1, 4)</span></span><br><span class="line"><span class="string">    B = torch.randn(3, 1)</span></span><br><span class="line"><span class="string">    C = A * B  # B 在第一个和第三个维度上扩展为 (2, 3, 4)，然后与 A 元素级相乘</span></span><br><span class="line"><span class="string">    以上的例子可以广播</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span>, theta: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].<span class="built_in">float</span>() / dim))</span><br><span class="line">    t = torch.arange(end, device=freqs.device)  <span class="comment"># type: ignore</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;求外积，对两个一维张量，维度分别为n,m ，得到(n,m)的二维张量 c[i,j]=a[i]*b[j]&#x27;&#x27;&#x27;</span></span><br><span class="line">    freqs = torch.outer(t, freqs).<span class="built_in">float</span>()  <span class="comment"># type: ignore</span></span><br><span class="line">    freqs_cos = torch.cos(freqs)  <span class="comment"># real part</span></span><br><span class="line">    freqs_sin = torch.sin(freqs)  <span class="comment"># imaginary part</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cos, freqs_sin</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reshape_for_broadcast</span>(<span class="params">freqs_cis: torch.Tensor, x: torch.Tensor</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    x.ndim返回维度</span></span><br><span class="line"><span class="string">    enumerate遍历列表，元组，字符串，索引从0开始</span></span><br><span class="line"><span class="string">    将freqs_cis的维度拓展为x的shape形状，用view可以原地变换，节省空间</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    ndim = x.ndim </span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[-<span class="number">1</span>])</span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(x.shape)]</span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params"></span></span><br><span class="line"><span class="params">    xq: torch.Tensor,</span></span><br><span class="line"><span class="params">    xk: torch.Tensor,</span></span><br><span class="line"><span class="params">    freqs_cos: torch.Tensor,</span></span><br><span class="line"><span class="params">    freqs_sin: torch.Tensor</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape xq and xk to match the complex representation</span></span><br><span class="line">    xq_r, xq_i = xq.<span class="built_in">float</span>().reshape(xq.shape[:-<span class="number">1</span>] + (-<span class="number">1</span>, <span class="number">2</span>)).unbind(-<span class="number">1</span>)</span><br><span class="line">    xk_r, xk_i = xk.<span class="built_in">float</span>().reshape(xk.shape[:-<span class="number">1</span>] + (-<span class="number">1</span>, <span class="number">2</span>)).unbind(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape freqs_cos and freqs_sin for broadcasting</span></span><br><span class="line">    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)</span><br><span class="line">    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply rotation using real numbers</span></span><br><span class="line">    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin</span><br><span class="line">    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos</span><br><span class="line">    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin</span><br><span class="line">    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos</span><br><span class="line"></span><br><span class="line">    <span class="comment"># flatten last two dimensions</span></span><br><span class="line">    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-<span class="number">1</span>).flatten(<span class="number">3</span>)</span><br><span class="line">    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-<span class="number">1</span>).flatten(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">repeat_kv</span>(<span class="params">x: torch.Tensor, n_rep: <span class="built_in">int</span></span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;torch.repeat_interleave(x, dim=2, repeats=n_rep)&quot;&quot;&quot;</span></span><br><span class="line">    bs, slen, n_kv_heads, head_dim = x.shape</span><br><span class="line">    <span class="keyword">if</span> n_rep == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        x[:, :, :, <span class="literal">None</span>, :]</span><br><span class="line">        .expand(bs, slen, n_kv_heads, n_rep, head_dim)</span><br><span class="line">        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_kv_heads = args.n_heads <span class="keyword">if</span> args.n_kv_heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> args.n_kv_heads</span><br><span class="line">        model_parallel_size = <span class="number">1</span></span><br><span class="line">        self.n_local_heads = args.n_heads // model_parallel_size</span><br><span class="line">        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size</span><br><span class="line">        self.n_rep = self.n_local_heads // self.n_local_kv_heads</span><br><span class="line">        self.head_dim = args.dim // args.n_heads</span><br><span class="line">        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.attn_dropout = nn.Dropout(args.dropout)</span><br><span class="line">        self.resid_dropout = nn.Dropout(args.dropout)</span><br><span class="line">        self.dropout = args.dropout</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use flash attention or a manual implementation?</span></span><br><span class="line">        self.flash = <span class="built_in">hasattr</span>(torch.nn.functional, <span class="string">&#x27;scaled_dot_product_attention&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.flash:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;</span>)</span><br><span class="line">            mask = torch.full((<span class="number">1</span>, <span class="number">1</span>, args.max_seq_len, args.max_seq_len), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">            mask = torch.triu(mask, diagonal=<span class="number">1</span>)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;mask&quot;</span>, mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        x: torch.Tensor,</span></span><br><span class="line"><span class="params">        freqs_cos: torch.Tensor,</span></span><br><span class="line"><span class="params">        freqs_sin: torch.Tensor,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># QKV</span></span><br><span class="line">        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)</span><br><span class="line">        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)</span><br><span class="line">        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># RoPE relative positional embeddings</span></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># grouped multiquery attention: expand out keys and values</span></span><br><span class="line">        xk = repeat_kv(xk, self.n_rep)  <span class="comment"># (bs, seqlen, n_local_heads, head_dim)</span></span><br><span class="line">        xv = repeat_kv(xv, self.n_rep)  <span class="comment"># (bs, seqlen, n_local_heads, head_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># make heads into a batch dimension</span></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line">        xk = xk.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        xv = xv.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># flash implementation</span></span><br><span class="line">        <span class="keyword">if</span> self.flash:</span><br><span class="line">            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=<span class="literal">None</span>, dropout_p=self.dropout <span class="keyword">if</span> self.training <span class="keyword">else</span> <span class="number">0.0</span>, is_causal=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># manual implementation</span></span><br><span class="line">            scores = torch.matmul(xq, xk.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(self.head_dim)</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;mask&#x27;</span>)</span><br><span class="line">            scores = scores + self.mask[:, :, :seqlen, :seqlen]   <span class="comment"># (bs, n_local_heads, seqlen, cache_len + seqlen)</span></span><br><span class="line">            scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">            scores = self.attn_dropout(scores)</span><br><span class="line">            output = torch.matmul(scores, xv)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># restore time as batch dimension and concat heads</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># final projection into the residual stream</span></span><br><span class="line">        output = self.wo(output)</span><br><span class="line">        output = self.resid_dropout(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, hidden_dim: <span class="built_in">int</span>, multiple_of: <span class="built_in">int</span>, dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        hidden_dim = <span class="built_in">int</span>(<span class="number">2</span> * hidden_dim / <span class="number">3</span>)</span><br><span class="line">        hidden_dim = multiple_of * ((hidden_dim + multiple_of - <span class="number">1</span>) // multiple_of)</span><br><span class="line">        self.w1 = nn.Linear(dim, hidden_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w2 = nn.Linear(hidden_dim, dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w3 = nn.Linear(dim, hidden_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer_id: <span class="built_in">int</span>, args: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = args.n_heads</span><br><span class="line">        self.dim = args.dim</span><br><span class="line">        self.head_dim = args.dim // args.n_heads</span><br><span class="line">        self.attention = Attention(args)</span><br><span class="line">        self.feed_forward = FeedForward(</span><br><span class="line">            dim=args.dim,</span><br><span class="line">            hidden_dim=<span class="number">4</span> * args.dim,</span><br><span class="line">            multiple_of=args.multiple_of,</span><br><span class="line">            dropout=args.dropout,</span><br><span class="line">        )</span><br><span class="line">        self.layer_id = layer_id</span><br><span class="line">        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)</span><br><span class="line">        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, freqs_cos, freqs_sin</span>):</span><br><span class="line">        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)</span><br><span class="line">        out = h + self.feed_forward.forward(self.ffn_norm(h))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    last_loss: <span class="type">Optional</span>[torch.Tensor]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.params = params</span><br><span class="line">        self.vocab_size = params.vocab_size</span><br><span class="line">        self.n_layers = params.n_layers</span><br><span class="line"></span><br><span class="line">        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)</span><br><span class="line">        self.dropout = nn.Dropout(params.dropout)</span><br><span class="line">        self.layers = torch.nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> layer_id <span class="keyword">in</span> <span class="built_in">range</span>(params.n_layers):</span><br><span class="line">            self.layers.append(TransformerBlock(layer_id, params))</span><br><span class="line">        self.norm = RMSNorm(params.dim, eps=params.norm_eps)</span><br><span class="line">        self.output = nn.Linear(params.dim, params.vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># share the unembedding parameters with the embedding parameters</span></span><br><span class="line">        self.tok_embeddings.weight = self.output.weight  <span class="comment"># https://paperswithcode.com/method/weight-tying</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># some useful precompute for the RoPE relative positional embeddings</span></span><br><span class="line">        freqs_cos, freqs_sin = precompute_freqs_cis(self.params.dim // self.params.n_heads, self.params.max_seq_len)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        注册缓冲区（buffer）的操作。在PyTorch中，缓冲区是一种持久化的张量，可以与模型的权重一起保存和加载，但不会被优化器更新</span></span><br><span class="line"><span class="string">        具体来说，这两行代码将名为&quot;freqs_cos&quot;和&quot;freqs_sin&quot;的张量注册为模型的缓冲区。这些张量通常用于存储Transformer模型中的位置编码信息。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;freqs_cos&quot;</span>, freqs_cos, persistent=<span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;freqs_sin&quot;</span>, freqs_sin, persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        调用了self._init_weights函数，并将其应用于模型的所有参数。这个函数用于初始化模型的权重</span></span><br><span class="line"><span class="string">        pn是参数的名称，p是参数的值</span></span><br><span class="line"><span class="string">        这行代码检查参数名称是否以&#x27;w3.weight&#x27;或&#x27;wo.weight&#x27;结尾。这些参数是残差投影的权重，根据GPT-2论文的建议，它们会进行特殊的缩放初始化</span></span><br><span class="line"><span class="string">        使用正态分布初始化参数p的值,这种特殊的缩放初始化是为了避免梯度消失或梯度爆炸问题？？？</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># init all weights</span></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line">        <span class="comment"># apply special scaled init to the residual projections, per GPT-2 paper</span></span><br><span class="line">        <span class="keyword">for</span> pn, p <span class="keyword">in</span> self.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> pn.endswith(<span class="string">&#x27;w3.weight&#x27;</span>) <span class="keyword">or</span> pn.endswith(<span class="string">&#x27;wo.weight&#x27;</span>):</span><br><span class="line">                torch.nn.init.normal_(p, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>/math.sqrt(<span class="number">2</span> * params.n_layers))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize attribute for the loss of the last forward call.</span></span><br><span class="line">        <span class="comment"># This will be set if the forward is called with a targets tensor.</span></span><br><span class="line">        self.last_loss = <span class="literal">None</span>  <span class="comment"># 用于存储最后一次前向传播的损失值，如果在前向传播中使用了目标张量，那么损失值将被计算并存储在这个属性中</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, module</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Linear):  <span class="comment"># 这行代码检查 module 是否属于 nn.Linear 类型，即判断是否是线性层</span></span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">if</span> module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                torch.nn.init.zeros_(module.bias)  <span class="comment"># 如果线性层具有偏置项，则将偏置项 module.bias 初始化为全零</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(module, nn.Embedding):</span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)  <span class="comment"># 使用正态分布初始化嵌入层的权重 module.weight</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens: torch.Tensor, targets: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>) -&gt; torch.Tensor:</span><br><span class="line">        _bsz, seqlen = tokens.shape</span><br><span class="line">        h = self.tok_embeddings(tokens)</span><br><span class="line">        h = self.dropout(h)</span><br><span class="line">        freqs_cos = self.freqs_cos[:seqlen]</span><br><span class="line">        freqs_sin = self.freqs_sin[:seqlen]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            h = layer(h, freqs_cos, freqs_sin)</span><br><span class="line">        h = self.norm(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># 检查是否提供了目标值 targets</span></span><br><span class="line">            <span class="comment"># if we are given some desired targets also calculate the loss</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;如果提供了目标值targets，计算模型的损失值。使用交叉熵损失函数计算预测结果logits和目标值targets之间的损失。ignore_index=-1表示忽略目标值为-1的部分。&#x27;&#x27;&#x27;</span></span><br><span class="line">            logits = self.output(h)  <span class="comment"># logits 形状 [batch_size, seq_len, params.vocab_size]？？？</span></span><br><span class="line">            self.last_loss = F.cross_entropy(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), targets.view(-<span class="number">1</span>), ignore_index=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># inference-time mini-optimization: only forward the output on the very last position</span></span><br><span class="line">            logits = self.output(h[:, [-<span class="number">1</span>], :])  <span class="comment"># note: using list [-1] to preserve the time dim 保留序列的最后一个位置</span></span><br><span class="line">            self.last_loss = <span class="literal">None</span>   <span class="comment"># logits 形状 [batch_size, 1, params.vocab_size]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    self.named_parameters() 是 PyTorch 中的一个方法，用于返回模型中所有具有名称的参数。它返回一个迭代器，其中每个元素是一个元组，包含参数的名称和对应的张量</span></span><br><span class="line"><span class="string">    创建了一个字典 param_dict，其中包含了模型的所有参数</span></span><br><span class="line"><span class="string">    过滤掉那些不需要梯度计算的参数。只有在 p.requires_grad 为 True 的情况下，参数才被包含在 param_dict 中</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    如果参数的维度大于等于2，则会应用权重衰减，否则不会应用权重衰减。通常，2D 参数是权重矩阵（嵌入层和矩阵乘法的权重），而维度小于2的参数是偏置项和层归一化等</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self, weight_decay, learning_rate, betas, device_type</span>):</span><br><span class="line">        <span class="comment"># start with all of the candidate parameters</span></span><br><span class="line">        param_dict = &#123;pn: p <span class="keyword">for</span> pn, p <span class="keyword">in</span> self.named_parameters()&#125;</span><br><span class="line">        <span class="comment"># filter out those that do not require grad</span></span><br><span class="line">        param_dict = &#123;pn: p <span class="keyword">for</span> pn, p <span class="keyword">in</span> param_dict.items() <span class="keyword">if</span> p.requires_grad&#125;</span><br><span class="line">        <span class="comment"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span></span><br><span class="line">        <span class="comment"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don&#x27;t.</span></span><br><span class="line">        decay_params = [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_dict.items() <span class="keyword">if</span> p.dim() &gt;= <span class="number">2</span>]</span><br><span class="line">        nodecay_params = [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_dict.items() <span class="keyword">if</span> p.dim() &lt; <span class="number">2</span>]</span><br><span class="line">        optim_groups = [</span><br><span class="line">            &#123;<span class="string">&#x27;params&#x27;</span>: decay_params, <span class="string">&#x27;weight_decay&#x27;</span>: weight_decay&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;params&#x27;</span>: nodecay_params, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">        num_decay_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> decay_params)  <span class="comment"># 计算参数总和并且在下面打印出来</span></span><br><span class="line">        num_nodecay_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> nodecay_params)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;num decayed parameter tensors: <span class="subst">&#123;<span class="built_in">len</span>(decay_params)&#125;</span>, with <span class="subst">&#123;num_decay_params:,&#125;</span> parameters&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;num non-decayed parameter tensors: <span class="subst">&#123;<span class="built_in">len</span>(nodecay_params)&#125;</span>, with <span class="subst">&#123;num_nodecay_params:,&#125;</span> parameters&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        使用 inspect.signature() 函数获取了 torch.optim.AdamW 的签名信息，并通过 parameters 属性获取了所有参数的名称</span></span><br><span class="line"><span class="string">        然后检查字符串&#x27;fused&#x27;是否在这些参数名称中出现。如果&#x27;fused&#x27;出现在参数名称中，说明AdamW优化器的融合版本可用，将fused_available设置为True，否则设置为False</span></span><br><span class="line"><span class="string">        只有在fused_available和使用cuda的时候used_fused才为true</span></span><br><span class="line"><span class="string">        根据 use_fused 的值构建一个字典 extra_args。如果 use_fused 为 True，则创建一个包含键值对 &#123;&#x27;fused&#x27;: True&#125; 的字典；否则，则创建一个空字典&#123;&#125;</span></span><br><span class="line"><span class="string">        最后根据extra_args的字典值，判断是否需要融合优化</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># Create AdamW optimizer and use the fused version if it is available</span></span><br><span class="line">        fused_available = <span class="string">&#x27;fused&#x27;</span> <span class="keyword">in</span> inspect.signature(torch.optim.AdamW).parameters</span><br><span class="line">        use_fused = fused_available <span class="keyword">and</span> device_type == <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">        extra_args = <span class="built_in">dict</span>(fused=<span class="literal">True</span>) <span class="keyword">if</span> use_fused <span class="keyword">else</span> <span class="built_in">dict</span>()</span><br><span class="line">        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;using fused AdamW: <span class="subst">&#123;use_fused&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> optimizer</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;显卡使用性能评估&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">estimate_mfu</span>(<span class="params">self, fwdbwd_per_iter, dt</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># first estimate the number of flops we do per iteration.</span></span><br><span class="line">        <span class="comment"># see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311</span></span><br><span class="line">        N = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters())</span><br><span class="line">        cfg = self.params</span><br><span class="line">        L, H, Q, T = cfg.n_layers, cfg.n_heads, cfg.dim//cfg.n_heads, cfg.max_seq_len</span><br><span class="line">        flops_per_token = <span class="number">6</span>*N + <span class="number">12</span>*L*H*Q*T</span><br><span class="line">        flops_per_fwdbwd = flops_per_token * T</span><br><span class="line">        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter</span><br><span class="line">        <span class="comment"># express our flops throughput as ratio of A100 bfloat16 peak flops</span></span><br><span class="line">        flops_achieved = flops_per_iter * (<span class="number">1.0</span>/dt)  <span class="comment"># per second</span></span><br><span class="line">        flops_promised = <span class="number">312e12</span>  <span class="comment"># A100 GPU bfloat16 peak flops is 312 TFLOPS</span></span><br><span class="line">        mfu = flops_achieved / flops_promised</span><br><span class="line">        <span class="keyword">return</span> mfu</span><br><span class="line"></span><br><span class="line">    <span class="comment"># @torch.inference_mode()</span></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="comment"># idx：索引序列，是一个形状为 (b,t) 的 LongTensor。</span></span><br><span class="line">    <span class="comment"># eos：表示序列结束的索引。</span></span><br><span class="line">    <span class="comment"># max_new_tokens：要生成的新标记的最大数量。</span></span><br><span class="line">    <span class="comment"># temperature：用于控制生成的多样性的温度参数，默认为 1.0。</span></span><br><span class="line">    <span class="comment"># top_k：用于限制生成选项的最大数量。如果为 None，则不进行限制。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, eos, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span><br><span class="line"><span class="string">        the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span><br><span class="line"><span class="string">        Most likely you&#x27;ll want to make sure to be in model.eval() mode of operation for this.</span></span><br><span class="line"><span class="string">        Also note this is a super inefficient version of sampling with no key/value cache.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">            <span class="comment"># if the sequence context is growing too long we must crop it at block_size</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;这个循环会执行 max_new_tokens 次，每次生成一个新标记。如果序列的长度超过了 max_seq_len（模型参数中的最大序列长度），则会将序列进行裁剪&#x27;&#x27;&#x27;</span></span><br><span class="line">            idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= self.params.max_seq_len <span class="keyword">else</span> idx[:, -self.params.max_seq_len:]</span><br><span class="line">            <span class="comment"># forward the model to get the logits for the index in the sequence</span></span><br><span class="line">            logits = self(idx_cond)</span><br><span class="line">            logits = logits[:, -<span class="number">1</span>, :]  <span class="comment"># crop to just the final time step</span></span><br><span class="line">            <span class="keyword">if</span> temperature == <span class="number">0.0</span>:</span><br><span class="line">                <span class="comment"># &quot;sample&quot; the single most likely index</span></span><br><span class="line">                _, idx_next = torch.topk(logits, k=<span class="number">1</span>, dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># pluck the logits at the final step and scale by desired temperature</span></span><br><span class="line">                logits = logits / temperature</span><br><span class="line">                <span class="comment"># optionally crop the logits to only the top k options</span></span><br><span class="line">                <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    v, _ = torch.topk(logits, <span class="built_in">min</span>(top_k, logits.size(-<span class="number">1</span>)))</span><br><span class="line">                    logits[logits &lt; v[:, [-<span class="number">1</span>]]] = -<span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">                <span class="comment"># apply softmax to convert logits to (normalized) probabilities</span></span><br><span class="line">                probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">                idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># append sampled index to the running sequence and continue</span></span><br><span class="line">            idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> idx_next == eos:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">export</span>(<span class="params">self, filepath=<span class="string">&#x27;model.bin&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;export the model weights in fp32 into .bin file to be read from C&quot;&quot;&quot;</span></span><br><span class="line">        f = <span class="built_in">open</span>(filepath, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">serialize</span>(<span class="params">t</span>):</span><br><span class="line">            d = t.detach().cpu().view(-<span class="number">1</span>).numpy().astype(np.float32)</span><br><span class="line">            b = struct.pack(<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">len</span>(d)&#125;</span>f&#x27;</span>, *d)</span><br><span class="line">            f.write(b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># first write out the header</span></span><br><span class="line">        hidden_dim = self.layers[<span class="number">0</span>].feed_forward.w1.weight.shape[<span class="number">0</span>]</span><br><span class="line">        p = self.params</span><br><span class="line">        n_kv_heads = p.n_heads <span class="keyword">if</span> p.n_kv_heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> p.n_kv_heads</span><br><span class="line">        header = struct.pack(<span class="string">&#x27;iiiiiii&#x27;</span>, p.dim, hidden_dim, p.n_layers, p.n_heads,</span><br><span class="line">                        n_kv_heads, p.vocab_size, p.max_seq_len)</span><br><span class="line">        f.write(header)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># next write out the embedding weights</span></span><br><span class="line">        serialize(self.tok_embeddings.weight)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># now all the layers</span></span><br><span class="line">        <span class="comment"># attention weights</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.attention_norm.weight)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.attention.wq.weight)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.attention.wk.weight)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.attention.wv.weight)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.attention.wo.weight)</span><br><span class="line">        <span class="comment"># ffn weights</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.ffn_norm.weight)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.feed_forward.w1.weight)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.feed_forward.w2.weight)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            serialize(layer.feed_forward.w3.weight)</span><br><span class="line">        <span class="comment"># final rmsnorm</span></span><br><span class="line">        serialize(self.norm.weight)</span><br><span class="line">        <span class="comment"># note: no need to write final classifier weights due to weight sharing</span></span><br><span class="line">        <span class="comment"># freqs_cis</span></span><br><span class="line">        serialize(self.freqs_cos[:p.max_seq_len])</span><br><span class="line">        serialize(self.freqs_sin[:p.max_seq_len])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># write to binary file</span></span><br><span class="line">        f.close()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;wrote <span class="subst">&#123;filepath&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/05/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-llama2%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-TRL代码解读" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-TRL%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/">大模型初步-TRL代码解读</a>
    </h1>
  

        
        <a href="/2024/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-TRL%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" class="archive-article-date">
  	<time datetime="2024-04-30T15:04:58.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-04-30</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作为初学者，TRL的底层代码值得仔细阅读，里面包含平时没怎么见过的语法点，用法，这些内容都需要知晓。<br>代码地址：<a target="_blank" rel="noopener" href="https://github.com/huggingface/trl">https://github.com/huggingface/trl</a><br>我们先从examples/scripts里面的代码开始入手学习</p>
<h3 id="sft-py"><a href="#sft-py" class="headerlink" title="sft.py"></a>sft.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># regular:</span></span><br><span class="line"><span class="string">python examples/scripts/sft.py \</span></span><br><span class="line"><span class="string">    --model_name_or_path=&quot;facebook/opt-350m&quot; \</span></span><br><span class="line"><span class="string">    --report_to=&quot;wandb&quot; \</span></span><br><span class="line"><span class="string">    --learning_rate=1.41e-5 \</span></span><br><span class="line"><span class="string">    --per_device_train_batch_size=64 \</span></span><br><span class="line"><span class="string">    --gradient_accumulation_steps=16 \</span></span><br><span class="line"><span class="string">    --output_dir=&quot;sft_openassistant-guanaco&quot; \</span></span><br><span class="line"><span class="string">    --logging_steps=1 \</span></span><br><span class="line"><span class="string">    --num_train_epochs=3 \</span></span><br><span class="line"><span class="string">    --max_steps=-1 \</span></span><br><span class="line"><span class="string">    --push_to_hub \</span></span><br><span class="line"><span class="string">    --gradient_checkpointing</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># peft:</span></span><br><span class="line"><span class="string">python examples/scripts/sft.py \</span></span><br><span class="line"><span class="string">    --model_name_or_path=&quot;facebook/opt-350m&quot; \</span></span><br><span class="line"><span class="string">    --report_to=&quot;wandb&quot; \</span></span><br><span class="line"><span class="string">    --learning_rate=1.41e-5 \</span></span><br><span class="line"><span class="string">    --per_device_train_batch_size=64 \</span></span><br><span class="line"><span class="string">    --gradient_accumulation_steps=16 \</span></span><br><span class="line"><span class="string">    --output_dir=&quot;sft_openassistant-guanaco&quot; \</span></span><br><span class="line"><span class="string">    --logging_steps=1 \</span></span><br><span class="line"><span class="string">    --num_train_epochs=3 \</span></span><br><span class="line"><span class="string">    --max_steps=-1 \</span></span><br><span class="line"><span class="string">    --push_to_hub \</span></span><br><span class="line"><span class="string">    --gradient_checkpointing \</span></span><br><span class="line"><span class="string">    --use_peft \</span></span><br><span class="line"><span class="string">    --lora_r=64 \</span></span><br><span class="line"><span class="string">    --lora_alpha=16</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> nullcontext</span><br><span class="line"></span><br><span class="line">TRL_USE_RICH = os.environ.get(<span class="string">&quot;TRL_USE_RICH&quot;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> trl.commands.cli_utils <span class="keyword">import</span> init_zero_verbose, SFTScriptArguments, TrlParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> TRL_USE_RICH:</span><br><span class="line">    init_zero_verbose()</span><br><span class="line">    FORMAT = <span class="string">&quot;%(message)s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> rich.console <span class="keyword">import</span> Console</span><br><span class="line">    <span class="keyword">from</span> rich.logging <span class="keyword">import</span> RichHandler</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm.rich <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这里从trl-main/trl/trainer下可以找到这些dataclass的配置情况&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> (</span><br><span class="line">    ModelConfig,</span><br><span class="line">    RichProgressCallback,</span><br><span class="line">    SFTConfig,</span><br><span class="line">    SFTTrainer,</span><br><span class="line">    get_peft_config,</span><br><span class="line">    get_quantization_config,</span><br><span class="line">    get_kbit_device_map,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tqdm.pandas()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> TRL_USE_RICH:</span><br><span class="line">    logging.basicConfig(<span class="built_in">format</span>=FORMAT, datefmt=<span class="string">&quot;[%X]&quot;</span>, handlers=[RichHandler()], level=logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;解析配置文件和命令行参数，并将结果传入给分别存储脚本参数，训练参数和模型配置的变量&#x27;&#x27;&#x27;</span></span><br><span class="line">    parser = TrlParser((SFTScriptArguments, SFTConfig, ModelConfig))</span><br><span class="line">    args, training_args, model_config = parser.parse_args_and_config()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Force use our print callback</span></span><br><span class="line">    <span class="keyword">if</span> TRL_USE_RICH:</span><br><span class="line">        training_args.disable_tqdm = <span class="literal">True</span></span><br><span class="line">        console = Console()</span><br><span class="line"></span><br><span class="line">    <span class="comment">################</span></span><br><span class="line">    <span class="comment"># Model &amp; Tokenizer</span></span><br><span class="line">    <span class="comment">################</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;这里对于模型类型的选择要么是自动设置要么是用户手动设置&#x27;&#x27;&#x27;</span></span><br><span class="line">    torch_dtype = (</span><br><span class="line">        model_config.torch_dtype</span><br><span class="line">        <span class="keyword">if</span> model_config.torch_dtype <span class="keyword">in</span> [<span class="string">&quot;auto&quot;</span>, <span class="literal">None</span>]</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">getattr</span>(torch, model_config.torch_dtype)</span><br><span class="line">    )</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;调用量化模型参数的函数&#x27;&#x27;&#x27;</span></span><br><span class="line">    quantization_config = get_quantization_config(model_config)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;创建一个字典，下列参数分别包括模型版本号，是否信任远程代码，注意力机制的设置，数据类型，是否使用缓存，模型层的设备映射，量化配置参数&#x27;&#x27;&#x27;</span></span><br><span class="line">    model_kwargs = <span class="built_in">dict</span>(</span><br><span class="line">        revision=model_config.model_revision,</span><br><span class="line">        trust_remote_code=model_config.trust_remote_code,</span><br><span class="line">        attn_implementation=model_config.attn_implementation,</span><br><span class="line">        torch_dtype=torch_dtype,</span><br><span class="line">        use_cache=<span class="literal">False</span> <span class="keyword">if</span> training_args.gradient_checkpointing <span class="keyword">else</span> <span class="literal">True</span>,</span><br><span class="line">        device_map=get_kbit_device_map() <span class="keyword">if</span> quantization_config <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">        quantization_config=quantization_config,</span><br><span class="line">    )</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    这里的from_pretrained用法就是huggingface中transformers的分词器用法，并且将分词器的pad_token（填充标记）设置为eos_token（结束-of-sequence标记）。</span></span><br><span class="line"><span class="string">    这意味着在需要对序列进行填充时，将使用EOS标记作为填充标记</span></span><br><span class="line"><span class="string">    不过这里为什么用EOS标记还是存疑，需要笔者继续查询相关资料以后再做解释？</span></span><br><span class="line"><span class="string">    个人认为应该只是对非最长的句子进行补齐时才用到这个pad_token</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_config.model_name_or_path, use_fast=<span class="literal">True</span>)</span><br><span class="line">    tokenizer.pad_token = tokenizer.eos_token</span><br><span class="line"></span><br><span class="line">    <span class="comment">################</span></span><br><span class="line">    <span class="comment"># Dataset</span></span><br><span class="line">    <span class="comment">################</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;用load_dataset函数加载全部的数据集，并在下面对其进行分割&#x27;&#x27;&#x27;</span></span><br><span class="line">    raw_datasets = load_dataset(args.dataset_name)</span><br><span class="line"></span><br><span class="line">    train_dataset = raw_datasets[args.dataset_train_split]</span><br><span class="line">    eval_dataset = raw_datasets[args.dataset_test_split]</span><br><span class="line"></span><br><span class="line">    <span class="comment">################</span></span><br><span class="line">    <span class="comment"># Optional rich context managers</span></span><br><span class="line">    <span class="comment">###############</span></span><br><span class="line">    init_context = nullcontext() <span class="keyword">if</span> <span class="keyword">not</span> TRL_USE_RICH <span class="keyword">else</span> console.status(<span class="string">&quot;[bold green]Initializing the SFTTrainer...&quot;</span>)</span><br><span class="line">    save_context = (</span><br><span class="line">        nullcontext()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> TRL_USE_RICH</span><br><span class="line">        <span class="keyword">else</span> console.status(<span class="string">f&quot;[bold green]Training completed! Saving the model to <span class="subst">&#123;training_args.output_dir&#125;</span>&quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">################</span></span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    <span class="comment">################</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    打开文件管理器，对SFTTrainer的类中进行参数填充，分别为要微调的预训练模型，模型初始化的关键字参数，训练参数...</span></span><br><span class="line"><span class="string">    那这里为什么要用with呢，因为这样可以进入上下文管理器执行下面的代码，即使在代码中如果init_context是空的上下文管理器，</span></span><br><span class="line"><span class="string">    也依然可以使得代码结构清晰</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> init_context:</span><br><span class="line">        trainer = SFTTrainer(</span><br><span class="line">            model=model_config.model_name_or_path,</span><br><span class="line">            model_init_kwargs=model_kwargs,</span><br><span class="line">            args=training_args,</span><br><span class="line">            train_dataset=train_dataset,</span><br><span class="line">            eval_dataset=eval_dataset,</span><br><span class="line">            tokenizer=tokenizer,</span><br><span class="line">            peft_config=get_peft_config(model_config),</span><br><span class="line">            callbacks=[RichProgressCallback] <span class="keyword">if</span> TRL_USE_RICH <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">        )</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;调用train方法开始训练，关于这部分代码的解析放在后面进行&#x27;&#x27;&#x27;</span></span><br><span class="line">    trainer.train()   </span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> save_context:</span><br><span class="line">        trainer.save_model(training_args.output_dir)</span><br></pre></td></tr></table></figure>
<h3 id="sft-trainer-py"><a href="#sft-trainer-py" class="headerlink" title="sft_trainer.py"></a>sft_trainer.py</h3><p>这里承接之前的sft.py，这里进一步讲解类内部的构造和训练代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SFTTrainer</span>(<span class="title class_ inherited__">Trainer</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Class definition of the Supervised Finetuning Trainer (SFT Trainer).</span></span><br><span class="line"><span class="string">    This class is a wrapper around the `transformers.Trainer` class and inherits all of its attributes and methods.</span></span><br><span class="line"><span class="string">    The trainer takes care of properly initializing the PeftModel in case a user passes a `PeftConfig` object.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]):</span></span><br><span class="line"><span class="string">            The model to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the model name to</span></span><br><span class="line"><span class="string">            load from cache or download. The model can be also converted to a `PeftModel` if a `PeftConfig` object is</span></span><br><span class="line"><span class="string">            passed to the `peft_config` argument.</span></span><br><span class="line"><span class="string">        args (Optional[`transformers.TrainingArguments`]):</span></span><br><span class="line"><span class="string">            The arguments to tweak for training. Please refer to the official documentation of `transformers.TrainingArguments`</span></span><br><span class="line"><span class="string">            for more information.</span></span><br><span class="line"><span class="string">        data_collator (Optional[`transformers.DataCollator`]):</span></span><br><span class="line"><span class="string">            The data collator to use for training.</span></span><br><span class="line"><span class="string">        train_dataset (Optional[`datasets.Dataset`]):</span></span><br><span class="line"><span class="string">            The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.</span></span><br><span class="line"><span class="string">        eval_dataset (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]):</span></span><br><span class="line"><span class="string">            The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.</span></span><br><span class="line"><span class="string">        tokenizer (Optional[`transformers.PreTrainedTokenizer`]):</span></span><br><span class="line"><span class="string">            The tokenizer to use for training. If not specified, the tokenizer associated to the model will be used.</span></span><br><span class="line"><span class="string">        model_init (`Callable[[], transformers.PreTrainedModel]`):</span></span><br><span class="line"><span class="string">            The model initializer to use for training. If None is specified, the default model initializer will be used.</span></span><br><span class="line"><span class="string">        compute_metrics (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to None):</span></span><br><span class="line"><span class="string">            The function used to compute metrics during evaluation. It should return a dictionary mapping metric names to metric values.</span></span><br><span class="line"><span class="string">            If not specified, only the loss will be computed during evaluation.</span></span><br><span class="line"><span class="string">        callbacks (`List[transformers.TrainerCallback]`):</span></span><br><span class="line"><span class="string">            The callbacks to use for training.</span></span><br><span class="line"><span class="string">        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):</span></span><br><span class="line"><span class="string">            The optimizer and scheduler to use for training.</span></span><br><span class="line"><span class="string">        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):</span></span><br><span class="line"><span class="string">            The function to use to preprocess the logits before computing the metrics.</span></span><br><span class="line"><span class="string">        peft_config (`Optional[PeftConfig]`):</span></span><br><span class="line"><span class="string">            The PeftConfig object to use to initialize the PeftModel.</span></span><br><span class="line"><span class="string">        formatting_func (`Optional[Callable]`):</span></span><br><span class="line"><span class="string">            The formatting function to be used for creating the `ConstantLengthDataset`.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    _tag_names = [<span class="string">&quot;trl&quot;</span>, <span class="string">&quot;sft&quot;</span>]</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    这行代码是一个装饰器，它装饰了SFTTrainer类的构造函数。装饰器_deprecate_arguments的作用是警告用户某些参数在某个版本之后将被弃用。</span></span><br><span class="line"><span class="string">    定义了一个自定义的消息，当使用弃用参数时，将显示这个消息。这个消息告诉用户应该使用SFTConfig类来设置这些参数，而不是直接在SFTTrainer类的构造函数中传递它们</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="meta">    @_deprecate_arguments(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">        version=<span class="string">&quot;1.0.0&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">        deprecated_args=[</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;dataset_text_field&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;packing&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;max_seq_length&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;dataset_num_proc&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;dataset_batch_size&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;neftune_noise_alpha&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;model_init_kwargs&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;dataset_kwargs&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;eval_packing&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;num_of_sequences&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">            <span class="string">&quot;chars_per_token&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">        ],</span></span></span><br><span class="line"><span class="params"><span class="meta">        custom_message=<span class="string">&quot;Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    </span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        model: <span class="type">Optional</span>[<span class="type">Union</span>[PreTrainedModel, nn.Module, <span class="built_in">str</span>]] = <span class="literal">None</span>,  <span class="comment"># 是可选的，可以是PreTrainedModel、nn.Module或一个字符串</span></span></span><br><span class="line"><span class="params">        args: <span class="type">Optional</span>[SFTConfig] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        data_collator: <span class="type">Optional</span>[DataCollator] = <span class="literal">None</span>,  <span class="comment"># type: ignore</span></span></span><br><span class="line"><span class="params">        train_dataset: <span class="type">Optional</span>[Dataset] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        eval_dataset: <span class="type">Optional</span>[<span class="type">Union</span>[Dataset, <span class="type">Dict</span>[<span class="built_in">str</span>, Dataset]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        tokenizer: <span class="type">Optional</span>[PreTrainedTokenizerBase] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        model_init: <span class="type">Optional</span>[<span class="type">Callable</span>[[], PreTrainedModel]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        compute_metrics: <span class="type">Optional</span>[<span class="type">Callable</span>[[EvalPrediction], <span class="type">Dict</span>]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        callbacks: <span class="type">Optional</span>[<span class="type">List</span>[TrainerCallback]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        optimizers: <span class="type">Tuple</span>[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (<span class="params"><span class="literal">None</span>, <span class="literal">None</span></span>),</span></span><br><span class="line"><span class="params">        preprocess_logits_for_metrics: <span class="type">Optional</span>[<span class="type">Callable</span>[[torch.Tensor, torch.Tensor], torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        peft_config: <span class="type">Optional</span>[<span class="string">&quot;PeftConfig&quot;</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dataset_text_field: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        packing: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        formatting_func: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        max_seq_length: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        infinite: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        num_of_sequences: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1024</span>,</span></span><br><span class="line"><span class="params">        chars_per_token: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="number">3.6</span>,</span></span><br><span class="line"><span class="params">        dataset_num_proc: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dataset_batch_size: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">        neftune_noise_alpha: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        model_init_kwargs: <span class="type">Optional</span>[<span class="type">Dict</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dataset_kwargs: <span class="type">Optional</span>[<span class="type">Dict</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        eval_packing: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="keyword">if</span> model_init_kwargs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.model_init_kwargs = model_init_kwargs</span><br><span class="line">        <span class="keyword">if</span> args.model_init_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_init_kwargs = &#123;&#125;</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(model, <span class="built_in">str</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You passed model_init_kwargs to the SFTConfig, but your model is already instantiated.&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model_init_kwargs = args.model_init_kwargs</span><br><span class="line">            model_init_kwargs[<span class="string">&quot;torch_dtype&quot;</span>] = (</span><br><span class="line">                model_init_kwargs[<span class="string">&quot;torch_dtype&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> model_init_kwargs[<span class="string">&quot;torch_dtype&quot;</span>] <span class="keyword">in</span> [<span class="string">&quot;auto&quot;</span>, <span class="literal">None</span>]</span><br><span class="line">                <span class="keyword">else</span> <span class="built_in">getattr</span>(torch, model_init_kwargs[<span class="string">&quot;torch_dtype&quot;</span>])</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> infinite <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;The `infinite` argument is deprecated and will be removed in a future version of TRL. Use `TrainingArguments.max_steps` or `TrainingArguments.num_train_epochs` instead to control training length.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(model, <span class="built_in">str</span>):</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a model_id to the SFTTrainer. This will automatically create an &quot;</span></span><br><span class="line">                <span class="string">&quot;`AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> packing:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.packing = packing</span><br><span class="line">        <span class="keyword">if</span> eval_packing <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `eval_packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.eval_packing = eval_packing</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.packing <span class="keyword">and</span> data_collator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(data_collator, DataCollatorForCompletionOnlyLM):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;You passed a `DataCollatorForCompletionOnlyLM` to the SFTTrainer. This is not compatible with the `packing` argument.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_peft_available() <span class="keyword">and</span> peft_config <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(peft_config, PeftConfig):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">&quot;If you want to use the PeftModel, you need to pass a PeftConfig object to the SFTTrainer.&quot;</span></span><br><span class="line">                    <span class="string">f&quot; and you passed a <span class="subst">&#123;<span class="built_in">type</span>(peft_config)&#125;</span>.&quot;</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(model, PeftModel):</span><br><span class="line">                _support_gc_kwargs = <span class="built_in">hasattr</span>(</span><br><span class="line">                    args, <span class="string">&quot;gradient_checkpointing_kwargs&quot;</span></span><br><span class="line">                ) <span class="keyword">and</span> <span class="string">&quot;gradient_checkpointing_kwargs&quot;</span> <span class="keyword">in</span> <span class="built_in">list</span>(</span><br><span class="line">                    inspect.signature(prepare_model_for_kbit_training).parameters</span><br><span class="line">                )</span><br><span class="line">                gradient_checkpointing_kwargs = <span class="built_in">getattr</span>(args, <span class="string">&quot;gradient_checkpointing_kwargs&quot;</span>, <span class="literal">None</span>) <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">                is_sharded_qlora = <span class="literal">False</span></span><br><span class="line">                <span class="comment"># Below is to support QLoRA + FSDP / DS-Zero3 - one should never call</span></span><br><span class="line">                <span class="comment"># peft_module_casting_to_bf16 or prepare_model_for_kbit_training when doing</span></span><br><span class="line">                <span class="comment"># QLoRA + FSDP / DS-Zero3</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">getattr</span>(model, <span class="string">&quot;is_loaded_in_4bit&quot;</span>, <span class="literal">False</span>):</span><br><span class="line">                    <span class="keyword">for</span> _, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">                        <span class="keyword">if</span> param.__class__.__name__ == <span class="string">&quot;Params4bit&quot;</span>:</span><br><span class="line">                            is_sharded_qlora = param.data.device.<span class="built_in">type</span> == <span class="string">&quot;cpu&quot;</span></span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">getattr</span>(model, <span class="string">&quot;is_loaded_in_8bit&quot;</span>, <span class="literal">False</span>) <span class="keyword">or</span> (</span><br><span class="line">                    <span class="built_in">getattr</span>(model, <span class="string">&quot;is_loaded_in_4bit&quot;</span>, <span class="literal">False</span>) <span class="keyword">and</span> <span class="keyword">not</span> is_sharded_qlora</span><br><span class="line">                ):</span><br><span class="line">                    prepare_model_kwargs = &#123;</span><br><span class="line">                        <span class="string">&quot;use_gradient_checkpointing&quot;</span>: <span class="built_in">getattr</span>(args, <span class="string">&quot;gradient_checkpointing&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> _support_gc_kwargs:</span><br><span class="line">                        prepare_model_kwargs[<span class="string">&quot;gradient_checkpointing_kwargs&quot;</span>] = gradient_checkpointing_kwargs</span><br><span class="line"></span><br><span class="line">                    model = prepare_model_for_kbit_training(model, **prepare_model_kwargs)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> args <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        args = dataclasses.replace(args, gradient_checkpointing=<span class="literal">False</span>)</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">getattr</span>(args, <span class="string">&quot;gradient_checkpointing&quot;</span>, <span class="literal">False</span>) <span class="keyword">and</span> (</span><br><span class="line">                    <span class="string">&quot;use_reentrant&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> gradient_checkpointing_kwargs</span><br><span class="line">                    <span class="keyword">or</span> gradient_checkpointing_kwargs[<span class="string">&quot;use_reentrant&quot;</span>]</span><br><span class="line">                ):</span><br><span class="line">                    <span class="comment"># For backward compatibility with older versions of transformers</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&quot;enable_input_require_grads&quot;</span>):</span><br><span class="line">                        model.enable_input_require_grads()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">def</span> <span class="title function_">make_inputs_require_grad</span>(<span class="params">module, <span class="built_in">input</span>, output</span>):</span><br><span class="line">                            output.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">                        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)</span><br><span class="line"></span><br><span class="line">                model = get_peft_model(model, peft_config)</span><br><span class="line">                <span class="keyword">if</span> (</span><br><span class="line">                    args <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                    <span class="keyword">and</span> args.bf16</span><br><span class="line">                    <span class="keyword">and</span> <span class="built_in">getattr</span>(model, <span class="string">&quot;is_loaded_in_4bit&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">                    <span class="keyword">and</span> <span class="keyword">not</span> is_sharded_qlora</span><br><span class="line">                ):</span><br><span class="line">                    peft_module_casting_to_bf16(model)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tokenizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">getattr</span>(tokenizer, <span class="string">&quot;pad_token&quot;</span>, <span class="literal">None</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                tokenizer.pad_token = tokenizer.eos_token</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> max_seq_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.max_seq_length = max_seq_length</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.max_seq_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># to overcome some issues with broken tokenizers</span></span><br><span class="line">            max_seq_length = <span class="built_in">min</span>(tokenizer.model_max_length, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">f&quot;You didn&#x27;t pass a `max_seq_length` argument to the SFTTrainer, this will default to <span class="subst">&#123;max_seq_length&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dataset_num_proc <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.dataset_num_proc = dataset_num_proc</span><br><span class="line">        self.dataset_num_proc = args.dataset_num_proc</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dataset_batch_size != args.dataset_batch_size:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `dataset_batch_size` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.dataset_batch_size = dataset_batch_size</span><br><span class="line">        self.dataset_batch_size = args.dataset_batch_size</span><br><span class="line"></span><br><span class="line">        self._trainer_supports_neftune = <span class="built_in">hasattr</span>(args, <span class="string">&quot;neftune_noise_alpha&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> neftune_noise_alpha <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> self._trainer_supports_neftune:</span><br><span class="line">            args.neftune_noise_alpha = neftune_noise_alpha</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># self.neftune_noise_alpha is done at Trainer level</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> self._trainer_supports_neftune:</span><br><span class="line">            self.neftune_noise_alpha = neftune_noise_alpha</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dataset_text_field <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.dataset_text_field = dataset_text_field</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> formatting_func <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> args.dataset_text_field <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># check if dataset has ChatML format or instruction format and is supported</span></span><br><span class="line">            <span class="comment"># if not stays #None</span></span><br><span class="line">            formatting_func = get_formatting_func_from_dataset(train_dataset, tokenizer)</span><br><span class="line">            <span class="comment"># if a template is detected, we don&#x27;t need to add special tokens again</span></span><br><span class="line">            <span class="keyword">if</span> formatting_func <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> dataset_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    dataset_kwargs = &#123;<span class="string">&quot;add_special_tokens&quot;</span>: <span class="literal">False</span>&#125;</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dataset_kwargs[<span class="string">&quot;add_special_tokens&quot;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args.packing:</span><br><span class="line">            <span class="keyword">if</span> args.dataset_text_field <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> formatting_func <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">&quot;You passed `packing=False` to the SFTTrainer/SFTConfig, but you didn&#x27;t pass a `dataset_text_field` or `formatting_func` argument.&quot;</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> data_collator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_of_sequences != args.num_of_sequences:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `num_of_sequences` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.num_of_sequences = num_of_sequences</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> chars_per_token != args.chars_per_token:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a `chars_per_token` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            args.chars_per_token = chars_per_token</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pre-process the datasets only once per node. The remaining processes will use the cache.</span></span><br><span class="line">        <span class="keyword">with</span> PartialState().local_main_process_first():</span><br><span class="line">            <span class="keyword">if</span> dataset_kwargs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                warnings.warn(</span><br><span class="line">                    <span class="string">&quot;You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;</span></span><br><span class="line">                )</span><br><span class="line">                args.dataset_kwargs = dataset_kwargs</span><br><span class="line">            <span class="keyword">if</span> args.dataset_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                args.dataset_kwargs = &#123;&#125;</span><br><span class="line">            <span class="keyword">if</span> train_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                train_dataset = self._prepare_dataset(</span><br><span class="line">                    train_dataset,</span><br><span class="line">                    tokenizer,</span><br><span class="line">                    args.packing,</span><br><span class="line">                    args.dataset_text_field,</span><br><span class="line">                    args.max_seq_length,</span><br><span class="line">                    formatting_func,</span><br><span class="line">                    args.num_of_sequences,</span><br><span class="line">                    args.chars_per_token,</span><br><span class="line">                    remove_unused_columns=args.remove_unused_columns <span class="keyword">if</span> args <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">True</span>,</span><br><span class="line">                    **args.dataset_kwargs,</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">if</span> eval_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                _multiple = <span class="built_in">isinstance</span>(eval_dataset, <span class="built_in">dict</span>)</span><br><span class="line">                _eval_datasets = eval_dataset <span class="keyword">if</span> _multiple <span class="keyword">else</span> &#123;<span class="string">&quot;singleton&quot;</span>: eval_dataset&#125;</span><br><span class="line"></span><br><span class="line">                eval_packing = args.packing <span class="keyword">if</span> args.eval_packing <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> args.eval_packing</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> _eval_dataset_name, _eval_dataset <span class="keyword">in</span> _eval_datasets.items():</span><br><span class="line">                    _eval_datasets[_eval_dataset_name] = self._prepare_dataset(</span><br><span class="line">                        _eval_dataset,</span><br><span class="line">                        tokenizer,</span><br><span class="line">                        eval_packing,</span><br><span class="line">                        args.dataset_text_field,</span><br><span class="line">                        args.max_seq_length,</span><br><span class="line">                        formatting_func,</span><br><span class="line">                        args.num_of_sequences,</span><br><span class="line">                        args.chars_per_token,</span><br><span class="line">                        remove_unused_columns=args.remove_unused_columns <span class="keyword">if</span> args <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">True</span>,</span><br><span class="line">                        **args.dataset_kwargs,</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> _multiple:</span><br><span class="line">                    eval_dataset = _eval_datasets[<span class="string">&quot;singleton&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tokenizer.padding_side <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> tokenizer.padding_side != <span class="string">&quot;right&quot;</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to &quot;</span></span><br><span class="line">                <span class="string">&quot;overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = &#x27;right&#x27;` to your code.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__(</span><br><span class="line">            model=model,</span><br><span class="line">            args=args,</span><br><span class="line">            data_collator=data_collator,</span><br><span class="line">            train_dataset=train_dataset,</span><br><span class="line">            eval_dataset=eval_dataset,</span><br><span class="line">            tokenizer=tokenizer,</span><br><span class="line">            model_init=model_init,</span><br><span class="line">            compute_metrics=compute_metrics,</span><br><span class="line">            callbacks=callbacks,</span><br><span class="line">            optimizers=optimizers,</span><br><span class="line">            preprocess_logits_for_metrics=preprocess_logits_for_metrics,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add tags for models that have been loaded with the correct transformers version</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(self.model, <span class="string">&quot;add_model_tags&quot;</span>):</span><br><span class="line">            self.model.add_model_tags(self._tag_names)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.args.max_steps &gt; <span class="number">0</span> <span class="keyword">and</span> args.packing:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            self.train_dataset.infinite = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> self.args.max_steps == -<span class="number">1</span> <span class="keyword">and</span> args.packing:</span><br><span class="line">            self.train_dataset.infinite = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">any</span>(<span class="built_in">isinstance</span>(callback, RichProgressCallback) <span class="keyword">for</span> callback <span class="keyword">in</span> self.callback_handler.callbacks):</span><br><span class="line">            <span class="keyword">for</span> callback <span class="keyword">in</span> self.callback_handler.callbacks:</span><br><span class="line">                <span class="comment"># Remove the PrinterCallback to avoid duplicated prints in case we passed a `RichProgressCallback`</span></span><br><span class="line">                <span class="keyword">if</span> callback.__class__.__name__ == <span class="string">&quot;PrinterCallback&quot;</span>:</span><br><span class="line">                    self.callback_handler.pop_callback(callback)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @wraps(<span class="params">Trainer.train</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># Activate neftune right before training.</span></span><br><span class="line">        <span class="keyword">if</span> self.neftune_noise_alpha <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="keyword">not</span> self._trainer_supports_neftune:</span><br><span class="line">            self.model = self._trl_activate_neftune(self.model)</span><br><span class="line"></span><br><span class="line">        output = <span class="built_in">super</span>().train(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After training we make sure to retrieve back the original forward pass method</span></span><br><span class="line">        <span class="comment"># for the embedding layer by removing the forward post hook.</span></span><br><span class="line">        <span class="keyword">if</span> self.neftune_noise_alpha <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="keyword">not</span> self._trainer_supports_neftune:</span><br><span class="line">            unwrapped_model = unwrap_model(self.model)</span><br><span class="line">            <span class="keyword">if</span> is_peft_available() <span class="keyword">and</span> <span class="built_in">isinstance</span>(unwrapped_model, PeftModel):</span><br><span class="line">                embeddings = unwrapped_model.base_model.model.get_input_embeddings()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                embeddings = unwrapped_model.get_input_embeddings()</span><br><span class="line"></span><br><span class="line">            self.neftune_hook_handle.remove()</span><br><span class="line">            <span class="keyword">del</span> embeddings.neftune_noise_alpha</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<!-- ### ppo.py

```python
"""
下面是运行该脚本的方法，使用Weights & Biases (wandb) 进行日志记录
python examples/scripts/ppo.py \
    --log_with=wandb
"""
'''
几个调用的包：dataclasses用于定义配置类，accelerate用于加速训练，datasets用于加载数据集，
peft用于低秩适配，tqdm用于显示进度条，transformers用于加载模型和分词器，trl用于强化学习训练
'''
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, HfArgumentParser, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
from trl.import_utils import is_npu_available, is_xpu_available


tqdm.pandas()  # 使得tqdm进度条可以在Pandas数据处理中使用

'''这里定义了一个ScriptArguments数据类，用于存储脚本运行时的参数'''
@dataclass
class ScriptArguments:
    '''field() 函数是 dataclasses 模块的一个函数，它用于指定数据类字段的额外信息，比如默认值、字段类型注解、字段元数据等'''
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq"})
    trust_remote_code: bool = field(default=False, metadata={"help": "Enable `trust_remote_code`"})

    # LoraConfig
    '''Optional[float] 用于指示 lora_alpha 字段可以是 float 类型或者 None, 它在创建数据类的实例时可以有值，也可以没有值（即为 None）'''
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    lora_alpha: Optional[float] = field(default=16, metadata={"help": "the lora alpha parameter"})
    lora_r: Optional[int] = field(default=16, metadata={"help": "the lora r parameter"})


'''使用了 transformers 库中的 HfArgumentParser 类来解析命令行参数，并创建传入的两个数据类（ScriptArguments, PPOConfig）的实例'''
parser = HfArgumentParser((ScriptArguments, PPOConfig))  # 这里PPOConfig文件在./trl/trainer/ppo_config.py中，之后再来详细看看
'''调用了 HfArgumentParser 实例的 parse_args_into_dataclasses 方法，该方法解析命令行参数并返回一个元组，其中包含了每个传递的数据类类型的实例。返回了一个包含 ScriptArguments 实例和 PPOConfig 实例的元组，并将它们分别赋值给 args 和 ppo_config 变量'''
args, ppo_config = parser.parse_args_into_dataclasses()


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
'''前两个标记分别为获取每个标记的情感分数, 指示不对输出应用任何函数'''
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

'''以下部分还未阅读，先去看sft的用法'''
# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(ppo_config, ppo_config.query_dataset)


def collator(data):
    return {key: [d[key] for d in data] for key in data[0]}


# set seed before initializing value head for deterministic eval
set_seed(ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(ppo_config.model_name, trust_remote_code=args.trust_remote_code)
    device_map = None
    peft_config = None
else:
    peft_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    ppo_config.model_name,
    trust_remote_code=args.trust_remote_code,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    if is_xpu_available():
        device = "xpu:0"
    elif is_npu_available():
        device = "npu:0"
    else:
        device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 32,
}

for _epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors, ref_response_tensors = ppo_trainer.generate(
        query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs
    )
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["ref_response"] = tokenizer.batch_decode(ref_response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    ref_texts = [q + r for q, r in zip(batch["query"], batch["ref_response"])]
    ref_pipe_outputs = sentiment_pipe(ref_texts, **sent_kwargs)
    ref_rewards = [torch.tensor(output[1]["score"]) for output in ref_pipe_outputs]
    batch["ref_rewards"] = ref_rewards

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=["query", "response", "ref_response", "ref_rewards"])

``` -->

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-TRL%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大模型初步-transformers用法学习" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-transformers%E7%94%A8%E6%B3%95%E5%AD%A6%E4%B9%A0/">大模型初步-transformers用法学习</a>
    </h1>
  

        
        <a href="/2024/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-transformers%E7%94%A8%E6%B3%95%E5%AD%A6%E4%B9%A0/" class="archive-article-date">
  	<time datetime="2024-04-30T10:16:54.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-04-30</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h3><p>在github网站上找hugging face就可以找到了全英文的教程<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/README_zh-hans.md">github网址</a>，如果英语不好的可以去看中文<br>当然我觉得如果是初学者还是从基础的开始，以下是两个适合入门的学习网址:<br><a target="_blank" rel="noopener" href="https://transformers.run">https://transformers.run</a><br><a target="_blank" rel="noopener" href="https://github.com/liuzard/transformers_zh_docs/tree/master">https://github.com/liuzard/transformers_zh_docs/tree/master</a><br>接下来就是参考这两篇文档提取出来的核心知识点。</p>
<h3 id="通过pipeline进行推理"><a href="#通过pipeline进行推理" class="headerlink" title="通过pipeline进行推理"></a>通过pipeline进行推理</h3><p>这部分内容用得比较少，先跳过。</p>
<h3 id="使用AutoClass编写可移植代码"><a href="#使用AutoClass编写可移植代码" class="headerlink" title="使用AutoClass编写可移植代码"></a>使用AutoClass编写可移植代码</h3><blockquote>
<p>架构指的是模型的框架，而检查点是给定架构的权重。例如，BERT是一个架构，而 bert-base-uncased 是一个检查点。”模型” 是一个通用术语，可以指代架构或检查点。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载AutoTokenizer，AutoImageProcessor，AutoFeatureExtractor，AutoProcessor，AutoModel</span></span><br><span class="line"><span class="string">分别为预训练的分词器，预训练的图像处理器，预训练的特征提取器，预训练的处理器，预训练的模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><ol>
<li>自然语言处理</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">encoded_input = tokenizer(<span class="string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoded_input)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>分词处理器返回一个包含三个重要的字典项：<br> input_ids 是句子中每个token对应的索引<br> attention_mask 指示一个token是否应该被注意(attention or mask)<br> token_type_ids 是当有多个序列时，标识一个token属于哪个序列</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># outputs:</span></span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">101</span>, <span class="number">2079</span>, <span class="number">2025</span>, <span class="number">19960</span>, <span class="number">10362</span>, <span class="number">1999</span>, <span class="number">1996</span>, <span class="number">3821</span>, <span class="number">1997</span>, <span class="number">16657</span>, <span class="number">1010</span>, <span class="number">2005</span>, <span class="number">2027</span>, <span class="number">2024</span>, <span class="number">11259</span>, <span class="number">1998</span>, <span class="number">4248</span>, <span class="number">2000</span>, <span class="number">4963</span>, <span class="number">1012</span>, <span class="number">102</span>],</span><br><span class="line"> <span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> <span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;这里调用了input_ids和decode函数&#x27;&#x27;&#x27;</span></span><br><span class="line">tokenizer.decode(encoded_input[<span class="string">&quot;input_ids&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># outputs:</span></span><br><span class="line"><span class="string">&#x27;[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]&#x27;</span></span><br></pre></td></tr></table></figure>
<p>注意：一次性输入多个句子token_type_ids依然都是0，这里指的是对于多个序列<br>例如：<code>encoded_input = tokenizer(a, b)</code></p>
<p>tokenizer函数的其他参数：<br>填充padding —- 变成相同维度，取最长维度的序列<br>阶段truncation —- 将序列截断为模型接受的最大长度<br>构建张量 —- pt或者tf<br>完整如下：<br><code>encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=&quot;pt&quot;)</code></p>
<ol>
<li>音频</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, Audio</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;PolyAI/minds14&quot;</span>, name=<span class="string">&quot;en-US&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">dataset[<span class="number">0</span>][<span class="string">&quot;audio&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">&#123;<span class="string">&#x27;array&#x27;</span>: array([ <span class="number">0.</span>        ,  <span class="number">0.00024414</span>, -<span class="number">0.00024414</span>, ..., -<span class="number">0.00024414</span>,</span><br><span class="line">         <span class="number">0.</span>        ,  <span class="number">0.</span>        ], dtype=float32),</span><br><span class="line"> <span class="string">&#x27;path&#x27;</span>: <span class="string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;sampling_rate&#x27;</span>: <span class="number">8000</span>&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>array 是作为1D数组加载 - 并且可能重采样 - 的语音信号。<br> path 指向音频文件的位置。<br> sampling_rate 表示每秒测量语音信号的数据点数</p>
</blockquote>
<h3 id="调优预训练模型"><a href="#调优预训练模型" class="headerlink" title="调优预训练模型"></a>调优预训练模型</h3><ol>
<li>数据集准备</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;加载一个数据集，这个数据集是在可以下载的（在jupyter notebook上测试过）&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;yelp_review_full&quot;</span>)</span><br><span class="line">dataset[<span class="string">&quot;train&quot;</span>][<span class="number">100</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">&#123;<span class="string">&#x27;label&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\n ...(data ignored)</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对于数据集的map操作函数很重要</span></span><br><span class="line"><span class="string">map函数是用于对数据集的每个样本应用一个函数，tokenize_function是自定义的一个函数以处理数据集中的样本，这里用的是bert的tokenizer进行分词处理的操作（具体可见上文的内容）</span></span><br><span class="line"><span class="string">batched表示可以批量处理样本，提高处理效率，这里设置为True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">tokenized_datasets = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这里创建一个小的子集，打乱数据集&#x27;&#x27;&#x27;</span></span><br><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p><code>以上的内容要熟练使用！</code></p>
<ol>
<li>pytorch trainer训练</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;加载模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;训练超参数&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line">training_args = TrainingArguments(output_dir=<span class="string">&quot;test_trainer&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;评估模型性能&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line">metric = evaluate.load(<span class="string">&quot;accuracy&quot;</span>)  <span class="comment"># 这里metric是一个度量标准</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_pred</span>):</span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)  <span class="comment"># 这里不需要用softmax</span></span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;微调过程中监视评估指标&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments, Trainer</span><br><span class="line">training_args = TrainingArguments(output_dir=<span class="string">&quot;test_trainer&quot;</span>, evaluation_strategy=<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;最后创建训练对象并微调模型&#x27;&#x27;&#x27;</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=small_train_dataset,</span><br><span class="line">    eval_dataset=small_eval_dataset,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<p>这部分内容是高级调用，实际情况下还需要魔改代码才能完成。</p>
<ol>
<li>原生pytorch训练</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;执行代码释放内存&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line"><span class="keyword">del</span> trainer</span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意如果以上代码是用autotokenzier处理过，似乎就不需要处理删除text等操作了，因为本来就没有这一列</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;删除text列，这个操作通常在文本数据被转换为tokens之后进行，因为转换后的tokens会被存储在新列中，而原始的&quot;text&quot;列就不再需要了&#x27;&#x27;&#x27;</span></span><br><span class="line">tokenized_datasets = tokenized_datasets.remove_columns([<span class="string">&quot;text&quot;</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;将label列重命名为labels，因为模型期望参数名称为labels&#x27;&#x27;&#x27;</span></span><br><span class="line">tokenized_datasets = tokenized_datasets.rename_column(<span class="string">&quot;label&quot;</span>, <span class="string">&quot;labels&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;设置数据集的格式，以返回PyTorch张量而不是列表&#x27;&#x27;&#x27;</span></span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;创建数据加载的迭代器&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>)</span><br><span class="line">eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;加载模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;创建优化器&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;创建学习率调度器，总共跑3轮，根据步骤数来调整学习率，线性调度器，并且指定学习率热身步骤的数量为0(没有热身阶段)&#x27;&#x27;&#x27;</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    name=<span class="string">&quot;linear&quot;</span>, optimizer=optimizer, num_warmup_steps=<span class="number">0</span>, num_training_steps=num_training_steps</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;指定设备&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;调用tqdm打印进度条&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;设置为训练模式在某些模型中，训练模式和评估模式的行为可能不同（例如，BatchNorm和Dropout层的操作不同）&#x27;&#x27;&#x27;</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;将批次中的所有数据移动到之前设定的设备上。这通常是通过.to(device)方法完成的，其中device是在代码其他部分中定义的&#x27;&#x27;&#x27;</span></span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss  <span class="comment"># 这里访问属性</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;step和清零梯度操作不要漏掉，然后就是进度条更新操作&#x27;&#x27;&#x27;</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;先设置为评估模式来评估，得到logits的属性&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line">metric = evaluate.load(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">    batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predictions = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    metric.add_batch(predictions=predictions, references=batch[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line">metric.compute()</span><br></pre></td></tr></table></figure>
<h4 id="补充内容"><a href="#补充内容" class="headerlink" title="补充内容"></a>补充内容</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;这里笔者最开始不是很了解dataloader是什么样的东西，就把batch打印了下来&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="built_in">print</span>(batch)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;将批次中的所有数据移动到之前设定的设备上。这通常是通过.to(device)方法完成的，其中device是在代码其他部分中定义的&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(batch)</span><br><span class="line">        <span class="comment"># batch = &#123;k: v.to(device) for k, v in batch.items()&#125;</span></span><br><span class="line">&#123;<span class="string">&#x27;labels&#x27;</span>: tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]), <span class="string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="number">101</span>, <span class="number">23158</span>,  <span class="number">1204</span>,  ...,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">101</span>,  <span class="number">1135</span>,   <span class="number">112</span>,  ...,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">101</span>,   <span class="number">146</span>,  <span class="number">1328</span>,  ...,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [  <span class="number">101</span>,  <span class="number">1753</span>,  <span class="number">3869</span>,  ...,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">101</span>, <span class="number">26505</span>,  <span class="number">1660</span>,  ...,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">101</span>,  <span class="number">4081</span>,  <span class="number">1159</span>,  ...,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>]]), <span class="string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]), <span class="string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])&#125;</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;加上.to(device)没有测试过，但如果不加上这个东西说明实际上也没变化，因为本身dataloader在将batch循环的时候就是一个字典，这里的用处就是移到指定的计算单元位置去&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="使用Accelerate进行分布式训练"><a href="#使用Accelerate进行分布式训练" class="headerlink" title="使用Accelerate进行分布式训练"></a>使用Accelerate进行分布式训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;相比于之前的代码增加减少的部分&#x27;&#x27;&#x27;</span></span><br><span class="line">+ <span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line">  <span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler</span><br><span class="line"></span><br><span class="line">+ accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line">  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line">  optimizer = AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br><span class="line"></span><br><span class="line">- device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">- model.to(device)</span><br><span class="line"></span><br><span class="line">+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(</span><br><span class="line">+     train_dataloader, eval_dataloader, model, optimizer</span><br><span class="line">+ )</span><br><span class="line"></span><br><span class="line">  num_epochs = <span class="number">3</span></span><br><span class="line">  num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">  lr_scheduler = get_scheduler(</span><br><span class="line">      <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">      optimizer=optimizer,</span><br><span class="line">      num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">      num_training_steps=num_training_steps</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">  model.train()</span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">      <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">-         batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">          outputs = model(**batch)</span><br><span class="line">          loss = outputs.loss</span><br><span class="line">-         loss.backward()</span><br><span class="line">+         accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">          optimizer.step()</span><br><span class="line">          lr_scheduler.step()</span><br><span class="line">          optimizer.zero_grad()</span><br><span class="line">          progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="使用PEFT加载adapters"><a href="#使用PEFT加载adapters" class="headerlink" title="使用PEFT加载adapters"></a>使用PEFT加载adapters</h3><p>支持peft的模型有低秩adapters，IA3，AdaLoRA</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;指定peft模型id，然后传递给AutoModelForCausalLM类&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line">peft_model_id = <span class="string">&quot;ybelkada/opt-350m-lora&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(peft_model_id)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;支持8位或者4位精度数据类型，并且用device_map将模型分配&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line">peft_model_id = <span class="string">&quot;ybelkada/opt-350m-lora&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map=<span class="string">&quot;auto&quot;</span>, load_in_8bit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;添加新的adapters&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftConfig  <span class="comment"># PeftConfig更加通用，包含了lora_config的内容</span></span><br><span class="line">model_id = <span class="string">&quot;facebook/opt-350m&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_id)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">target_modules参数指定了LoRA适配器将应用于哪些模块，这里应用于查询投影(q_proj)和键投影(k_proj)</span></span><br><span class="line"><span class="string">init_lora_weights 参数设置为 False，这意味着在添加适配器时不会初始化 LoRA 权重</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>],</span><br><span class="line">    init_lora_weights=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;add_adapter用于向模型添加一个适配器，这里使用之前创建的 lora_config 作为配置，并将适配器命名为 “adapter_1”&#x27;&#x27;&#x27;</span></span><br><span class="line">model.add_adapter(lora_config, adapter_name=<span class="string">&quot;adapter_1&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;使用相同的配置附加新的adapters&#x27;&#x27;&#x27;</span></span><br><span class="line">model.add_adapter(lora_config, adapter_name=<span class="string">&quot;adapter_2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">先激活了名为 adapter_1 的适配器</span></span><br><span class="line"><span class="string">model.generate(**inputs) 调用了模型的生成方法，**inputs 是传递给生成方法的参数，通常包括输入文本的编码、生成文本的最大长度等</span></span><br><span class="line"><span class="string">将生成的输出解码为文本，并打印出来。tokenizer.decode 方法将模型的输出（token IDs）转换为可读的文本</span></span><br><span class="line"><span class="string">skip_special_tokens=True 参数确保在解码过程中跳过任何特殊标记（例如 BOS、EOS、PAD 等）</span></span><br><span class="line"><span class="string">adapter_2的操作和这里一样</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 使用adapters_1</span></span><br><span class="line">model.set_adapter(<span class="string">&quot;adapter_1&quot;</span>)</span><br><span class="line">output_disabled = model.generate(**inputs)   <span class="comment"># 这里可能需要勘误？？？ 原代码传进去的变量名是output</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(output_disabled[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用adapters_2</span></span><br><span class="line">model.set_adapter(<span class="string">&quot;adapter_2&quot;</span>)</span><br><span class="line">output_enabled = model.generate(**inputs)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(output_enabled[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;激活adapter全流程&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftConfig</span><br><span class="line">model_id = <span class="string">&quot;facebook/opt-350m&quot;</span></span><br><span class="line">adapter_model_id = <span class="string">&quot;ybelkada/opt-350m-lora&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class="line">text = <span class="string">&quot;Hello&quot;</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_id)</span><br><span class="line">peft_config = PeftConfig.from_pretrained(adapter_model_id)</span><br><span class="line"><span class="comment"># 用随机权重初始化</span></span><br><span class="line">peft_config.init_lora_weights = <span class="literal">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这里enable_adapters和上面set_adapter应该是一个意思，后面查询说明文档做解释&#x27;&#x27;&#x27;</span></span><br><span class="line">model.add_adapter(peft_config)  </span><br><span class="line">model.enable_adapters()</span><br><span class="line">output = model.generate(**inputs)</span><br><span class="line"><span class="comment"># 禁用adapters模块</span></span><br><span class="line">model.disable_adapters()</span><br><span class="line">output = model.generate(**inputs)</span><br></pre></td></tr></table></figure>
<p>总结：adapter的意义是使得每个适配器都可以为模型提供不同的行为，使其适应不同的任务或数据集。通过这种方式，可以在不改变模型大部分参数的情况下，为模型引入额外的灵活性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;训练peft adapters&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig</span><br><span class="line">peft_config = LoraConfig(</span><br><span class="line">    lora_alpha=<span class="number">16</span>,</span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">    r=<span class="number">64</span>,</span><br><span class="line">    bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">    task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;将adapters添加到模型中&#x27;&#x27;&#x27;</span></span><br><span class="line">model.add_adapter(peft_config)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;训练模型&#x27;&#x27;&#x27;</span></span><br><span class="line">trainer = Trainer(model=model, ...)</span><br><span class="line">trainer.train()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;保存训练的adapters模型并加载&#x27;&#x27;&#x27;</span></span><br><span class="line">model.save_pretrained(save_dir)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(save_dir)</span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2024/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%AD%A5-transformers%E7%94%A8%E6%B3%95%E5%AD%A6%E4%B9%A0/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2024 John Doe
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">随笔</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接1</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接2</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接3</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接4</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接5</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接6</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧&lt;br&gt;&lt;br&gt;只做了一点微小的工作&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>PPO代码解读2 | Hexo</title><meta name="author" content="Kevin"><meta name="copyright" content="Kevin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="继续承接PPO代码解读1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798">
<meta property="og:type" content="article">
<meta property="og:title" content="PPO代码解读2">
<meta property="og:url" content="https://kevin236-max.github.io/2024/07/08/PPO%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="继续承接PPO代码解读1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://kevin236-max.github.io/img/avatar.jpg">
<meta property="article:published_time" content="2024-07-08T08:30:55.000Z">
<meta property="article:modified_time" content="2024-07-09T08:45:03.186Z">
<meta property="article:author" content="Kevin">
<meta property="article:tag" content="微调">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kevin236-max.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://kevin236-max.github.io/2024/07/08/PPO%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PPO代码解读2',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-07-09 16:45:03'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">44</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">9</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/background.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Hexo"><img class="site-icon" src="/img/avatar.jpg"/><span class="site-name">Hexo</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PPO代码解读2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-07-08T08:30:55.000Z" title="Created 2024-07-08 16:30:55">2024-07-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-07-09T08:45:03.186Z" title="Updated 2024-07-09 16:45:03">2024-07-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/">大模型基础</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">2.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>14mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PPO代码解读2"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>继续承接<a href="https://kevin236-max.github.io/2024/07/08/PPO%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/">PPO代码解读1</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">def evaluate(args: Args, reward_model, policy, tokenizer, dataloader, generation_config, sampling=True):</span></span><br><span class="line"><span class="string">    eval_storage = EvalStorage()</span></span><br><span class="line"><span class="string">    with torch.no_grad():</span></span><br><span class="line"><span class="string">        for data in tqdm(dataloader):</span></span><br><span class="line"><span class="string">            queries = data[&quot;query_token&quot;]</span></span><br><span class="line"><span class="string">            reference_response_token = data[&quot;reference_response_token&quot;]</span></span><br><span class="line"><span class="string">            context_length = queries.shape[1]</span></span><br><span class="line"><span class="string">            query_reference_responses = torch.cat((data[&quot;query_token&quot;], data[&quot;reference_response_token&quot;]), dim=1)</span></span><br><span class="line"><span class="string">            _, reference_score, _ = get_reward(reward_model, query_reference_responses, tokenizer, queries.shape[1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            query_responses = generate(</span></span><br><span class="line"><span class="string">                policy,</span></span><br><span class="line"><span class="string">                queries,</span></span><br><span class="line"><span class="string">                tokenizer,</span></span><br><span class="line"><span class="string">                generation_config,</span></span><br><span class="line"><span class="string">            )</span></span><br><span class="line"><span class="string">            responses = query_responses[:, context_length:]</span></span><br><span class="line"><span class="string">            postprocessed_responses = truncate_response(args, tokenizer, responses)</span></span><br><span class="line"><span class="string">            postprocessed_query_responses = torch.cat((queries, postprocessed_responses), 1)</span></span><br><span class="line"><span class="string">            _, score, _ = get_reward(reward_model, postprocessed_query_responses, tokenizer, queries.shape[1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            eval_storage.query_token.extend(queries)</span></span><br><span class="line"><span class="string">            eval_storage.reference_response_token.extend(reference_response_token)</span></span><br><span class="line"><span class="string">            eval_storage.reference_score.append(reference_score)</span></span><br><span class="line"><span class="string">            eval_storage.postprocessed_response_token.extend(postprocessed_responses)</span></span><br><span class="line"><span class="string">            eval_storage.score.append(score)</span></span><br><span class="line"><span class="string">            if sampling:</span></span><br><span class="line"><span class="string">                break</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    eval_storage.query = tokenizer.batch_decode(eval_storage.query_token, skip_special_tokens=True)</span></span><br><span class="line"><span class="string">    eval_storage.reference_response = tokenizer.batch_decode(eval_storage.reference_response_token)</span></span><br><span class="line"><span class="string">    eval_storage.postprocessed_response = tokenizer.batch_decode(</span></span><br><span class="line"><span class="string">        eval_storage.postprocessed_response_token, skip_special_tokens=True</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    eval_score = torch.cat(eval_storage.score).float().cpu().numpy().tolist()</span></span><br><span class="line"><span class="string">    eval_reference_score = torch.cat(eval_storage.reference_score).float().cpu().numpy().tolist()</span></span><br><span class="line"><span class="string">    eval_df = pd.DataFrame(</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            &quot;query&quot;: gather_object(eval_storage.query),</span></span><br><span class="line"><span class="string">            &quot;postprocessed_response&quot;: gather_object(eval_storage.postprocessed_response),</span></span><br><span class="line"><span class="string">            &quot;reference_responses&quot;: gather_object(eval_storage.reference_response),</span></span><br><span class="line"><span class="string">            &quot;scores&quot;: gather_object(eval_score),</span></span><br><span class="line"><span class="string">            &quot;reference_scores&quot;: gather_object(eval_reference_score),</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    return eval_storage, eval_df</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def generate(lm_backbone, queries, tokenizer, generation_config):</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;generate in a way that does not affect padding tokens&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        context_length = queries.shape[1]</span></span><br><span class="line"><span class="string">        attention_mask = queries != tokenizer.pad_token_id  attention_mask是布尔张量</span></span><br><span class="line"><span class="string">        input_ids = torch.masked_fill(queries, ~attention_mask, 0)  # ~是对布尔矩阵取反</span></span><br><span class="line"><span class="string">        output = lm_backbone.generate(   generate是transformers库中的一个用法</span></span><br><span class="line"><span class="string">            input_ids=input_ids,  # 输入序列</span></span><br><span class="line"><span class="string">            attention_mask=attention_mask,  # 注意力掩码</span></span><br><span class="line"><span class="string">            # position_ids=attention_mask.cumsum(1) - attention_mask.long(), # generation collapsed if this was turned on. TODO: why does generation collapse with this?</span></span><br><span class="line"><span class="string">            generation_config=generation_config,  # 包含生成文本时所需参数的配置对象，例如最大长度、温度、top-k 采样、top-p 采样</span></span><br><span class="line"><span class="string">            return_dict_in_generate=True,</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string">        return torch.cat((queries, output.sequences[:, context_length:]), dim=1)  # 返回的是问题回答的组合矩阵</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> update <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, args.ppo.num_updates + <span class="number">1</span>):</span><br><span class="line">    global_step += <span class="number">1</span> * args.batch_size</span><br><span class="line">    frac = <span class="number">1.0</span> - (update - <span class="number">1.0</span>) / args.ppo.num_updates</span><br><span class="line">    lrnow = frac * args.lr  <span class="comment"># 动态调整学习率</span></span><br><span class="line">    optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>] = lrnow  <span class="comment"># 优化器学习率也更新</span></span><br><span class="line">    data = <span class="built_in">next</span>(iter_dataloader)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不计算关于张量的梯度</span></span><br><span class="line">        eval_storage, eval_df = evaluate(  <span class="comment"># 采样评估</span></span><br><span class="line">            args,</span><br><span class="line">            reward_model,</span><br><span class="line">            accelerator.unwrap_model(model).policy,</span><br><span class="line">            tokenizer,</span><br><span class="line">            validation_dataloader,</span><br><span class="line">            validation_generation_config,</span><br><span class="line">        )</span><br><span class="line">        validation_score = eval_storage.score[<span class="number">0</span>]  <span class="comment"># 获取第一个分数</span></span><br><span class="line">        <span class="keyword">if</span> args.print_sample_output_freq &gt; <span class="number">0</span> <span class="keyword">and</span> update &gt; <span class="number">1</span> <span class="keyword">and</span> (update - <span class="number">1</span>) % args.print_sample_output_freq == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> accelerator.is_main_process:  <span class="comment"># 保存评估结果</span></span><br><span class="line">                eval_df.to_csv(<span class="string">f&quot;runs/<span class="subst">&#123;run_name&#125;</span>/table_<span class="subst">&#123;global_step&#125;</span>.csv&quot;</span>)  <span class="comment"># 如果是主进程，将评估结果的DataFrame eval_df 保存到CSV文件</span></span><br><span class="line">                <span class="keyword">if</span> args.track:  <span class="comment"># 如果启用了W&amp;B跟踪（args.track），则将样本输出记录到Weights &amp; Biases平台</span></span><br><span class="line">                    wandb.log(&#123;<span class="string">&quot;samples/query_responses&quot;</span>: wandb.Table(dataframe=eval_df)&#125;, step=update)</span><br><span class="line">                <span class="keyword">else</span>:  <span class="comment"># 如果没有启用W&amp;B跟踪，尝试使用 print_rich_table 函数打印样本输出的表格</span></span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        print_rich_table(<span class="string">f&quot;Sample Output at Step <span class="subst">&#123;update&#125;</span>&quot;</span>, eval_df[:<span class="number">1</span>], console)</span><br><span class="line">                    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                        <span class="built_in">print</span>(e)</span><br><span class="line">            <span class="keyword">if</span> args.run_eval:</span><br><span class="line">                eval_storage, eval_df = evaluate(  <span class="comment"># 全面评估</span></span><br><span class="line">                    args,</span><br><span class="line">                    reward_model,</span><br><span class="line">                    accelerator.unwrap_model(model).policy,</span><br><span class="line">                    tokenizer,</span><br><span class="line">                    validation_dataloader,</span><br><span class="line">                    validation_generation_config,</span><br><span class="line">                    sampling=<span class="literal">False</span>,</span><br><span class="line">                )</span><br><span class="line">                <span class="keyword">if</span> accelerator.is_main_process:  <span class="comment"># 保存评估结果</span></span><br><span class="line">                    eval_df.to_csv(<span class="string">f&quot;runs/<span class="subst">&#123;run_name&#125;</span>/table.csv&quot;</span>)</span><br><span class="line">                    <span class="keyword">if</span> args.track:</span><br><span class="line">                        wandb.log(&#123;<span class="string">&quot;eval/query_responses&quot;</span>: wandb.Table(dataframe=eval_df)&#125;, step=update)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># save model</span></span><br><span class="line">            <span class="keyword">if</span> args.output_dir <span class="keyword">and</span> args.num_train_epochs &gt; <span class="number">0</span>:</span><br><span class="line">                os.makedirs(os.path.dirname(args.output_dir), exist_ok=<span class="literal">True</span>)</span><br><span class="line">                time_tensor = torch.tensor([<span class="built_in">int</span>(time.time())], device=device)</span><br><span class="line">                time_int = accelerator.gather(time_tensor)[<span class="number">0</span>].item()  <span class="comment"># avoid different timestamps across processes</span></span><br><span class="line">                repo_name = <span class="string">f&quot;<span class="subst">&#123;args.base_model.replace(<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)&#125;</span>__<span class="subst">&#123;args.exp_name&#125;</span>__tldr&quot;</span></span><br><span class="line">                repo_id = <span class="string">f&quot;<span class="subst">&#123;args.hf_entity&#125;</span>/<span class="subst">&#123;repo_name&#125;</span>&quot;</span> <span class="keyword">if</span> args.hf_entity <span class="keyword">else</span> repo_name</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">                    tokenizer.save_pretrained(args.output_dir, repo_id=repo_id)</span><br><span class="line">                    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">                        tokenizer.push_to_hub(repo_id, revision=<span class="string">f&quot;seed<span class="subst">&#123;args.seed&#125;</span>_<span class="subst">&#123;<span class="built_in">str</span>(time_int)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">                unwrapped: PreTrainedModel = accelerator.unwrap_model(model).policy</span><br><span class="line">                accelerator.wait_for_everyone()</span><br><span class="line">                <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">                    unwrapped.save_pretrained(</span><br><span class="line">                        args.output_dir,</span><br><span class="line">                        is_main_process=accelerator.is_main_process,</span><br><span class="line">                        save_function=accelerator.save,</span><br><span class="line">                        state_dict=accelerator.get_state_dict(unwrapped),</span><br><span class="line">                        safe_serialization=<span class="literal">False</span>,</span><br><span class="line">                        repo_id=repo_id,</span><br><span class="line">                    )</span><br><span class="line">                    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">                        unwrapped.push_to_hub(repo_id, revision=<span class="string">f&quot;seed<span class="subst">&#123;args.seed&#125;</span>_<span class="subst">&#123;<span class="built_in">str</span>(time_int)&#125;</span>&quot;</span>, safe_serialization=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">del</span> eval_storage, eval_df</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        queries = data[<span class="string">&quot;query_token&quot;</span>].to(device)</span><br><span class="line">        context_length = queries.shape[<span class="number">1</span>]</span><br><span class="line">        query_responses = []</span><br><span class="line">        responses = []</span><br><span class="line">        postprocessed_responses = []</span><br><span class="line">        logprobs = []</span><br><span class="line">        ref_logprobs = []</span><br><span class="line">        values = []</span><br><span class="line">        scores = []</span><br><span class="line">        sequence_lengths = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, queries.shape[<span class="number">0</span>], args.local_rollout_forward_batch_size):  <span class="comment"># 每32个计算一次</span></span><br><span class="line">            query = queries[i : i + args.local_rollout_forward_batch_size]</span><br><span class="line">            query_response = generate(</span><br><span class="line">                accelerator.unwrap_model(model).policy,  <span class="comment"># unwrap_model 方法用于获取包装模型的原始实例，在解读1中，model实例有两个成员，分别是policy和critic</span></span><br><span class="line">                query,</span><br><span class="line">                tokenizer,</span><br><span class="line">                generation_config,</span><br><span class="line">            )</span><br><span class="line">            response = query_response[:, context_length:]  <span class="comment"># 注意这里截取的大小</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;这里对于logits的切片操作存疑&#x27;&#x27;&#x27;</span></span><br><span class="line">            output = forward(accelerator.unwrap_model(model).policy, query_response, tokenizer)</span><br><span class="line">            logits = output.logits[:, context_length - <span class="number">1</span> : -<span class="number">1</span>]  <span class="comment"># logits 是一个二维张量[batch_size, vocab_size]，其中每一行包含了对应于每个token的原始分数</span></span><br><span class="line">            logits /= args.task.temperature + <span class="number">1e-7</span>  <span class="comment"># 应用温度参数进行softmax操作前的缩放，温度较高可能导致概率分布更平坦，降低则使分布更尖锐</span></span><br><span class="line">            all_logprob = F.log_softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            logprob = torch.gather(all_logprob, <span class="number">2</span>, response.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)  <span class="comment"># 根据实际生成的响应response提取对应对数概率得到logprob</span></span><br><span class="line">            <span class="keyword">del</span> output, logits, all_logprob</span><br><span class="line">            torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">            ref_output = forward(accelerator.unwrap_model(policy), query_response, tokenizer, ref=<span class="literal">True</span>)</span><br><span class="line">            ref_logits = ref_output.logits[:, context_length - <span class="number">1</span> : -<span class="number">1</span>]</span><br><span class="line">            ref_logits /= args.task.temperature + <span class="number">1e-7</span></span><br><span class="line">            ref_all_logprob = F.log_softmax(ref_logits, dim=-<span class="number">1</span>)</span><br><span class="line">            ref_logprob = torch.gather(ref_all_logprob, <span class="number">2</span>, response.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">del</span> ref_output, ref_logits, ref_all_logprob</span><br><span class="line">            torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Response Processing 1. truncate response after the first occurrence of `truncate_token_id`</span></span><br><span class="line">            postprocessed_response = truncate_response(args, tokenizer, response)  <span class="comment"># 确保生成的文本响应不会超过预设的最大长度</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Response Processing 2. run reward model on the truncated responses</span></span><br><span class="line">            postprocessed_query_response = torch.cat((query, postprocessed_response), <span class="number">1</span>)  <span class="comment"># 将两个张量沿着第一维拼接</span></span><br><span class="line">            sequence_length = first_true_indices(postprocessed_response == tokenizer.pad_token_id) - <span class="number">1</span></span><br><span class="line">            full_value, _, _ = get_reward(</span><br><span class="line">                accelerator.unwrap_model(model).critic, query_response, tokenizer, context_length</span><br><span class="line">            )</span><br><span class="line">            value = full_value[:, context_length - <span class="number">1</span> : -<span class="number">1</span>].squeeze(-<span class="number">1</span>)</span><br><span class="line">            _, score, _ = get_reward(reward_model, postprocessed_query_response, tokenizer, context_length)</span><br><span class="line"></span><br><span class="line">            query_responses.append(query_response)</span><br><span class="line">            responses.append(response)</span><br><span class="line">            postprocessed_responses.append(postprocessed_response)</span><br><span class="line">            logprobs.append(logprob)</span><br><span class="line">            ref_logprobs.append(ref_logprob)</span><br><span class="line">            values.append(value)</span><br><span class="line">            sequence_lengths.append(sequence_length)</span><br><span class="line">            scores.append(score)</span><br><span class="line">        query_responses = torch.cat(query_responses, <span class="number">0</span>)</span><br><span class="line">        responses = torch.cat(responses, <span class="number">0</span>)</span><br><span class="line">        postprocessed_responses = torch.cat(postprocessed_responses, <span class="number">0</span>)</span><br><span class="line">        logprobs = torch.cat(logprobs, <span class="number">0</span>)</span><br><span class="line">        ref_logprobs = torch.cat(ref_logprobs, <span class="number">0</span>)</span><br><span class="line">        values = torch.cat(values, <span class="number">0</span>)</span><br><span class="line">        sequence_lengths = torch.cat(sequence_lengths, <span class="number">0</span>)</span><br><span class="line">        scores = torch.cat(scores, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">del</span> (logprob, ref_logprob, full_value, value, score)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Response Processing 3. filter response. Ensure that the sample contains truncate_token_id</span></span><br><span class="line">        <span class="comment"># responses not passing that filter will receive a low (fixed) score</span></span><br><span class="line">        <span class="comment"># only query humans on responses that pass that filter</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        postprocessed_responses = torch.tensor([[1,2,3],[4,0,0]])</span></span><br><span class="line"><span class="string">        contain_pad_token = torch.any(postprocessed_responses == 0, dim=-1)</span></span><br><span class="line"><span class="string">        print(contain_pad_token)</span></span><br><span class="line"><span class="string">        scores = torch.tensor([3,4])</span></span><br><span class="line"><span class="string">        scores = torch.where(contain_pad_token, scores, torch.full_like(scores, -1))</span></span><br><span class="line"><span class="string">        print(scores)</span></span><br><span class="line"><span class="string">        tensor([False,  True]) 和 tensor([-1,  4])        </span></span><br><span class="line"><span class="string">        torch.where(cond, x, y) 函数根据条件 cond 选择 x 或 y。在这里，cond 是 contain_pad_token，对于没有pad_token的序列给予惩罚奖励-1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        contain_pad_token = torch.<span class="built_in">any</span>(postprocessed_responses == tokenizer.pad_token_id, dim=-<span class="number">1</span>)  <span class="comment"># 沿着最后一维</span></span><br><span class="line">        scores = torch.where(contain_pad_token, scores, torch.full_like(scores, args.task.penalty_reward_value))</span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;scores=&#125;</span>, <span class="subst">&#123;(contain_pad_token.<span class="built_in">sum</span>() / <span class="built_in">len</span>(contain_pad_token))=&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. compute rewards</span></span><br><span class="line">        kl = logprobs - ref_logprobs</span><br><span class="line">        non_score_reward = -kl_ctl.value * kl</span><br><span class="line">        rewards = non_score_reward.clone()  <span class="comment"># 注意clone可以保留原来的值，即使原来的值改变也不影响clone的值</span></span><br><span class="line">        actual_start = torch.arange(rewards.size(<span class="number">0</span>), device=rewards.device)</span><br><span class="line">        actual_end = sequence_lengths</span><br><span class="line">        rewards[[actual_start, actual_end]] += scores</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. whiten rewards</span></span><br><span class="line">        <span class="keyword">if</span> args.ppo.whiten_rewards:</span><br><span class="line">            rewards = whiten(rewards, shift_mean=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. compute advantages and returns</span></span><br><span class="line">        lastgaelam = <span class="number">0</span></span><br><span class="line">        advantages_reversed = []</span><br><span class="line">        gen_length = args.task.response_length</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(gen_length)):</span><br><span class="line">            nextvalues = values[:, t + <span class="number">1</span>] <span class="keyword">if</span> t &lt; gen_length - <span class="number">1</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">            delta = rewards[:, t] + args.ppo.gamma * nextvalues - values[:, t]</span><br><span class="line">            lastgaelam = delta + args.ppo.gamma * args.ppo.lam * lastgaelam</span><br><span class="line">            advantages_reversed.append(lastgaelam)</span><br><span class="line">        advantages = torch.stack(advantages_reversed[::-<span class="number">1</span>], axis=<span class="number">1</span>)</span><br><span class="line">        returns = advantages + values</span><br><span class="line">        advantages = whiten(advantages)</span><br><span class="line">        return_mean, return_var = returns.mean(), returns.var()</span><br><span class="line">        value_mean, value_var = values.mean(), values.var()</span><br><span class="line">        writer.add_histogram(<span class="string">&quot;rewards&quot;</span>, rewards[<span class="number">0</span>].<span class="built_in">float</span>(), global_step)</span><br><span class="line">        writer.add_histogram(<span class="string">&quot;advantages&quot;</span>, advantages[<span class="number">0</span>].<span class="built_in">float</span>(), global_step)</span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">&quot;rewards====&quot;</span>, rewards[<span class="number">0</span>])</span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">&quot;advantages====&quot;</span>, advantages[<span class="number">0</span>])</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Do multiple epochs of PPO training, with a fresh random shuffle in each epoch</span></span><br><span class="line">    <span class="keyword">for</span> ppo_epoch_idx <span class="keyword">in</span> <span class="built_in">range</span>(args.ppo.noptepochs):</span><br><span class="line">        b_inds = np.random.permutation(args.local_batch_size)</span><br><span class="line">        minibatch_idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> mini_batch_start <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, args.local_batch_size, args.local_mini_batch_size):</span><br><span class="line">            mini_batch_end = mini_batch_start + args.local_mini_batch_size</span><br><span class="line">            mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]</span><br><span class="line">            gradient_accumulation_idx = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> micro_batch_start <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, args.local_mini_batch_size, args.local_micro_batch_size):</span><br><span class="line">                <span class="keyword">with</span> accelerator.accumulate(policy):</span><br><span class="line">                    micro_batch_end = micro_batch_start + args.local_micro_batch_size</span><br><span class="line">                    micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]</span><br><span class="line">                    mb_return = returns[micro_batch_inds]</span><br><span class="line">                    mb_advantage = advantages[micro_batch_inds]</span><br><span class="line">                    mb_values = values[micro_batch_inds]</span><br><span class="line">                    mb_responses = responses[micro_batch_inds]</span><br><span class="line">                    mb_query_responses = query_responses[micro_batch_inds]</span><br><span class="line">                    mb_logprobs = logprobs[micro_batch_inds]</span><br><span class="line"></span><br><span class="line">                    output, vpred_temp = forward(model, mb_query_responses, tokenizer)</span><br><span class="line">                    logits = output.logits[:, context_length - <span class="number">1</span> : -<span class="number">1</span>]</span><br><span class="line">                    logits /= args.task.temperature + <span class="number">1e-7</span></span><br><span class="line">                    new_all_logprobs = F.log_softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">                    new_logprobs = torch.gather(new_all_logprobs, <span class="number">2</span>, mb_responses.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">                    vpred = vpred_temp[:, context_length - <span class="number">1</span> : -<span class="number">1</span>].squeeze(-<span class="number">1</span>)</span><br><span class="line">                    vpredclipped = torch.clamp(</span><br><span class="line">                        vpred,</span><br><span class="line">                        mb_values - args.ppo.cliprange_value,</span><br><span class="line">                        mb_values + args.ppo.cliprange_value,</span><br><span class="line">                    )</span><br><span class="line">                    vf_losses1 = torch.square(vpred - mb_return)</span><br><span class="line">                    vf_losses2 = torch.square(vpredclipped - mb_return)</span><br><span class="line">                    vf_loss = <span class="number">0.5</span> * torch.<span class="built_in">max</span>(vf_losses1, vf_losses2).mean()</span><br><span class="line">                    vf_clipfrac = (vf_losses2 &gt; vf_losses1).<span class="built_in">float</span>().mean()</span><br><span class="line">                    logprobs_diff = new_logprobs - mb_logprobs</span><br><span class="line">                    ratio = torch.exp(logprobs_diff)</span><br><span class="line">                    pg_losses = -mb_advantage * ratio</span><br><span class="line">                    pg_losses2 = -mb_advantage * torch.clamp(ratio, <span class="number">1.0</span> - args.ppo.cliprange, <span class="number">1.0</span> + args.ppo.cliprange)</span><br><span class="line">                    pg_loss = torch.<span class="built_in">max</span>(pg_losses, pg_losses2).mean()</span><br><span class="line">                    loss = pg_loss + args.ppo.vf_coef * vf_loss</span><br><span class="line">                    accelerator.backward(loss)</span><br><span class="line">                    optimizer.step()</span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line">                    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                        pg_clipfrac = (pg_losses2 &gt; pg_losses).<span class="built_in">float</span>().mean()</span><br><span class="line">                        prob_dist = torch.nn.functional.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">                        entropy = torch.logsumexp(logits, dim=-<span class="number">1</span>) - torch.<span class="built_in">sum</span>(prob_dist * logits, dim=-<span class="number">1</span>)</span><br><span class="line">                        approxkl = <span class="number">0.5</span> * (logprobs_diff**<span class="number">2</span>).mean()</span><br><span class="line">                        approxkl_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = approxkl</span><br><span class="line">                        pg_clipfrac_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = pg_clipfrac</span><br><span class="line">                        pg_loss_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = pg_loss</span><br><span class="line">                        vf_loss_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = vf_loss</span><br><span class="line">                        vf_clipfrac_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = vf_clipfrac</span><br><span class="line">                        entropy_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = entropy.mean()</span><br><span class="line">                        ratio_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = ratio.mean()</span><br><span class="line">                    <span class="comment"># if ppo_epoch_idx == 0 and micro_batch_start == 0:</span></span><br><span class="line">                    <span class="comment">#     torch.testing.assert_close(ratio, torch.zeros_like(ratio) + 1, atol=1e-4, rtol=1e-4)</span></span><br><span class="line">                    <span class="comment"># if ppo_epoch_idx == 0:</span></span><br><span class="line">                    <span class="comment">#     pprint(&#123;</span></span><br><span class="line">                    <span class="comment">#         # &quot;responses&quot;: responses,</span></span><br><span class="line">                    <span class="comment">#         # &quot;values&quot;: values,</span></span><br><span class="line">                    <span class="comment">#         &quot;rewards&quot;: rewards,</span></span><br><span class="line">                    <span class="comment">#         # &quot;scores&quot;: scores,</span></span><br><span class="line">                    <span class="comment">#         &quot;advantages&quot;: advantages,</span></span><br><span class="line">                    <span class="comment">#         # &quot;ratio&quot;: ratio,</span></span><br><span class="line">                    <span class="comment">#         # &quot;pg_losses&quot;: pg_losses,</span></span><br><span class="line">                    <span class="comment">#         # &quot;approxkl&quot;: approxkl,</span></span><br><span class="line">                    <span class="comment">#         # &quot;pg_loss&quot;: pg_loss,</span></span><br><span class="line">                    <span class="comment">#         # &quot;pg_clipfrac&quot;: pg_clipfrac,</span></span><br><span class="line">                    <span class="comment">#         # &quot;ratio&quot;: ratio.mean(),</span></span><br><span class="line">                    <span class="comment">#         # &quot;vf_loss&quot;: vf_loss,</span></span><br><span class="line">                    <span class="comment">#         # &quot;vf_clipfrac&quot;: vf_clipfrac,</span></span><br><span class="line">                    <span class="comment">#         # &quot;entropy&quot;: masked_mean(entropy, ~padding_mask[micro_batch_inds]),</span></span><br><span class="line">                    <span class="comment">#     &#125;)</span></span><br><span class="line">                    <span class="comment">#     breakpoint()</span></span><br><span class="line">                gradient_accumulation_idx += <span class="number">1</span></span><br><span class="line">            minibatch_idx += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">                console.<span class="built_in">print</span>(</span><br><span class="line">                    <span class="string">f&quot;ppo_epoch_idx&quot;</span>,</span><br><span class="line">                    ppo_epoch_idx,</span><br><span class="line">                    <span class="string">&quot;approxkl&quot;</span>,</span><br><span class="line">                    approxkl_stats[: ppo_epoch_idx + <span class="number">1</span>].mean().item(),</span><br><span class="line">                    <span class="string">&quot;pg_loss&quot;</span>,</span><br><span class="line">                    pg_loss_stats[: ppo_epoch_idx + <span class="number">1</span>].mean().item(),</span><br><span class="line">                    <span class="string">&quot;pg_clipfrac&quot;</span>,</span><br><span class="line">                    pg_clipfrac_stats[: ppo_epoch_idx + <span class="number">1</span>].mean().item(),</span><br><span class="line">                    <span class="string">&quot;ratio&quot;</span>,</span><br><span class="line">                    ratio_stats[: ppo_epoch_idx + <span class="number">1</span>].mean().item(),</span><br><span class="line">                )</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args.deepspeed:  <span class="comment"># for some reason there is a OOM with the `writer.add_histogram`</span></span><br><span class="line">            writer.add_histogram(<span class="string">&quot;ppo/val/ratio_hist&quot;</span>, ratio, update)</span><br><span class="line">        mean_kl = kl.<span class="built_in">sum</span>(<span class="number">1</span>).mean()</span><br><span class="line">        mean_entropy = (-logprobs).<span class="built_in">sum</span>(<span class="number">1</span>).mean()</span><br><span class="line">        mean_non_score_reward = non_score_reward.<span class="built_in">sum</span>(<span class="number">1</span>).mean()</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;objective/kl_coef&quot;</span>, kl_ctl.value, update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;objective/kl&quot;</span>, accelerator.gather(mean_kl).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;objective/entropy&quot;</span>, accelerator.gather(mean_entropy).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;objective/non_score_reward&quot;</span>, accelerator.gather(mean_non_score_reward).mean().item(), update)</span><br><span class="line">        writer.add_scalar(</span><br><span class="line">            <span class="string">&quot;objective/score_total&quot;</span>, accelerator.gather(mean_non_score_reward + scores.mean()).mean().item(), update</span><br><span class="line">        )</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;objective/scores&quot;</span>, accelerator.gather(scores.mean()).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;objective/validation_score&quot;</span>, accelerator.gather(validation_score.mean()).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/loss/policy&quot;</span>, accelerator.gather(pg_loss).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/loss/value&quot;</span>, accelerator.gather(vf_loss).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/loss/total&quot;</span>, accelerator.gather(loss).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/policy/entropy&quot;</span>, accelerator.gather(entropy.mean()).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/policy/approxkl&quot;</span>, accelerator.gather(approxkl).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/policy/clipfrac&quot;</span>, accelerator.gather(pg_clipfrac).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/policy/approxkl_avg&quot;</span>, accelerator.gather(approxkl_stats).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/policy/clipfrac_avg&quot;</span>, accelerator.gather(pg_clipfrac_stats).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/loss/policy_avg&quot;</span>, accelerator.gather(pg_loss_stats).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/loss/value_avg&quot;</span>, accelerator.gather(vf_loss_stats).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/clipfrac_avg&quot;</span>, accelerator.gather(vf_clipfrac_stats).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/policy/entropy_avg&quot;</span>, accelerator.gather(entropy_stats).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/returns/mean&quot;</span>, accelerator.gather(return_mean).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/returns/var&quot;</span>, accelerator.gather(return_var).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/vpred&quot;</span>, accelerator.gather(vpred.mean()).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/error&quot;</span>, accelerator.gather(vf_losses1.mean()).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/clipfrac&quot;</span>, accelerator.gather(vf_clipfrac).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/mean&quot;</span>, accelerator.gather(value_mean).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/var&quot;</span>, accelerator.gather(value_var).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/ratio&quot;</span>, accelerator.gather(ratio_stats).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/ratio_var&quot;</span>, accelerator.gather(ratio_stats).var().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/advantage&quot;</span>, accelerator.gather(advantages.mean()).mean().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/advantage_var&quot;</span>, accelerator.gather(advantages.mean()).var().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/val/num_eos_tokens&quot;</span>, (responses == tokenizer.eos_token_id).<span class="built_in">sum</span>().item(), update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/lr&quot;</span>, lrnow, update)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/episode&quot;</span>, global_step, update)</span><br><span class="line">        eps = <span class="built_in">int</span>(global_step / (time.time() - start_time))</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;ppo/eps&quot;</span>, eps, update)</span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">&quot;ppo/eps&quot;</span>, eps, update)</span><br><span class="line">        <span class="keyword">if</span> args.reward.use_adaptive_kl:</span><br><span class="line">            kl_ctl.update(mean_kl.item(), args.batch_size)</span><br><span class="line">        <span class="keyword">del</span> kl, mean_kl, mean_entropy, mean_non_score_reward, scores</span><br><span class="line">        torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://kevin236-max.github.io">Kevin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://kevin236-max.github.io/2024/07/08/PPO%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/">https://kevin236-max.github.io/2024/07/08/PPO%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%BE%AE%E8%B0%83/">微调</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/09/AgentGym%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB2/" title="AgentGym代码解读2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">AgentGym代码解读2</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/08/PPO%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/" title="PPO代码解读1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">PPO代码解读1</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/07/08/PPO%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/" title="PPO代码解读1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-08</div><div class="title">PPO代码解读1</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Kevin</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">44</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://kevin236-max.github.io/" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/12/%E5%8C%BB%E5%AD%A6%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/" title="医学多模态论文合集">医学多模态论文合集</a><time datetime="2024-11-12T09:37:06.000Z" title="Created 2024-11-12 17:37:06">2024-11-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/17/%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%BF%AE%E6%94%B9%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/" title="模型架构修改论文合集">模型架构修改论文合集</a><time datetime="2024-10-17T02:30:31.000Z" title="Created 2024-10-17 10:30:31">2024-10-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/14/flow-matching-rlhf%E9%A1%B9%E7%9B%AE%E5%8F%82%E8%80%83%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/" title="flow matching rlhf项目参考论文合集">flow matching rlhf项目参考论文合集</a><time datetime="2024-10-14T12:48:11.000Z" title="Created 2024-10-14 20:48:11">2024-10-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/13/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%85%A5%E9%97%A8/" title="多模态入门">多模态入门</a><time datetime="2024-10-13T04:22:09.000Z" title="Created 2024-10-13 12:22:09">2024-10-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/12/%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95%E8%B0%83%E7%A0%94/" title="图片生成算法调研">图片生成算法调研</a><time datetime="2024-10-12T12:43:17.000Z" title="Created 2024-10-12 20:43:17">2024-10-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Kevin</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>